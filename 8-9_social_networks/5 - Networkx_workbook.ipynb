{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/Minyall/sc207_materials/blob/master/images/gephi_network.png?raw=true\" align=\"right\" width=\"300\">\n",
    "\n",
    "\n",
    "# SC207 - Session 5\n",
    "# Social Network Analysis - NetworkX and Gephi\n",
    "\n",
    "\n",
    "\n",
    "- Social Network Analysis allows us to explore how different subjects relate to one another in complex ways.\n",
    "- Often we think of network analysis relation to social networks, how different indivduals are connected to one another, and how this forms larger communities, and influences the movement of information.\n",
    "- Network analysis can be used for a much wider variety of things - anything essentially that can be understood as existing in relation to something else.\n",
    "\n",
    "# Tools\n",
    "- Today we will be using two tools\n",
    "1. NetworkX - a Python library for shaping data into network form. NetworkX can also use a number of network analysis techniques but we will stick to data structuring so that we can export our data into...\n",
    "2. Gephi - a network visualisation and analysis tool with a click-button interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "Today we will just need...\n",
    "- Pandas to import and reshape our twitter data\n",
    "- NetworkX to do the final reshaping and export it to a file readable by Gephi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Testing on Toy Data\n",
    "- Often it can be useful to test out techniques on simpler datasets just to see how the process works before we try with something larger.\n",
    "- We're going to use a simple network of that shows which Marvel characters appear in which of the Marvel Movies.\n",
    "- Data originally compiled by the [Social Media Research Foundation](https://www.smrfoundation.org/nodexl/teaching-with-nodexl/teaching-resources/) from IMDB in 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 'marvel_movie_data.csv' here as df\n",
    "\n",
    "df = \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build this simple data into a network is relatively easy, and we can get NetworkX to do most of the heavy lifting.\n",
    "- We're going to make an undirected graph, that means there is just an edge between a movie and a character, there is no direction involved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Graph with an Edge List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marvel_G = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine nodes directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine edges directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may be useful to also have a little bit of information about the nodes. Normally a network graph, if represented as a spreadsheet, would be split into to items, the edge list, which we already have, and the node list, which provides information about the attributes of our nodes.\n",
    "\n",
    "Let's keep this simple by giving our nodes one attribute, `type` which will be either 'Movie' or 'Character'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Nodes List with Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can make a series into a new dataframe by using the .to_frame() method\n",
    "\n",
    "movie_nodes = \n",
    "# drop duplicates on the 'movie' column and reset index\n",
    "movie_nodes =\n",
    "\n",
    "# create a new 'type' column with the value 'movie'\n",
    "movie_nodes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same again for the characters \n",
    "\n",
    "char_nodes = \n",
    "\n",
    "# drop duplicates on the 'character' column and reset index\n",
    "char_nodes = \n",
    "\n",
    "# create a new 'type' column with the value 'character'\n",
    "\n",
    "char_nodes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we will stack these two dataframes on top of one another using pd.concat\n",
    "# BUT first we need to rename the movie column, and the character column to a common name so that the two dataframes line up properly.\n",
    "movie_nodes = \n",
    "char_nodes = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_nodes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_nodes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pd.concat \n",
    "- 'Concatenates' two Series, or two dataframes together into one. \n",
    "- Ideally they will share the same index or column names.\n",
    "- Think of it as two spreadsheets lined up against each other either top to bottom, or left to right.\n",
    "\n",
    "#### `axis='columns'`\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/Minyall/sc207_materials/blob/master/images/pandas%20column%20concat.png?raw=true\" width=500>\n",
    "\n",
    "\n",
    "#### `axis='index'`\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/Minyall/sc207_materials/blob/master/images/pandas%20row%20concat.png?raw=true\" width=500>\n",
    "\n",
    "\n",
    "*Images from http://www.datasciencemadesimple.com*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's concatenate them together top to bottom  (so on the 'index') and call it 'nodes'. make sure you reset the index after\n",
    "\n",
    "nodes =\n",
    "nodes['type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Node Attributes\n",
    "\n",
    "To attach atributes to NetworkX you need a dictionary where the key is the node id, in our case the movie or character name, and then the value attached to that key is another dictionary where the key is the name of the attribute, and the value is the attribute vaue.\n",
    "\n",
    "We want an attribute dictionary for each node like this...\n",
    "\n",
    "`{'type':'movie'}`\n",
    "\n",
    "\n",
    "Which is then embedded in another dictionary where the keys are our node names...\n",
    "\n",
    "```{'Iron Man 2':{'type': 'movie'},\n",
    " 'Dr. Strange':{'type': 'character'},\n",
    " 'Spiderman':{'type': 'movie'},\n",
    " ... }```\n",
    "\n",
    "It is worth knowing that in your attribute dictionary you can have as many attributes as you like\n",
    "\n",
    "Attribute dictionary with 2 attributes\n",
    "`{'type':'movie', 'release date': 2010}`\n",
    "\n",
    "The final dictionary keyed to the node name `{'Iron Man 2':{'type': 'movie', 'release date': 2010}}`\n",
    "\n",
    "#### Looks complicated?\n",
    "PANDAS TO THE RESCUE!\n",
    "\n",
    "As long as we have set up our node dataframe where each attribute is its own column we can use this chain of methods..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we set our index to be the value that represents our node in the NetworkX graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we transpose the data so columns become rows and rows become columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.. and finally we convert to a dictionary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_attributes = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally we can use this dictionary to attach the right attribute values to the right nodes using nx.set_node_attributes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now examine the nodes with data=True\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting to Gephi\n",
    "Finally we export the networkX graph to a Gephi compatible .gexf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export using nx.write_gexf  to filename 'marvel_graph.gexf'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/Minyall/sc207_materials/blob/master/images/gephi-logo-2010-transparent.png?raw=true\" align=\"left\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move over to Gephi to explore our Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Real Deal: Twitter Data\n",
    "\n",
    "- We're going to make a Retweet network. \n",
    "- In this network every Node will represent a different user, \n",
    "- An edge between user a and user b will represent one user retweeting the other\n",
    "- Eges will be given a `weight` that counts how many unique times user a retweeted user b. \n",
    "- We will make our network `directional` meaning that we will record seperately\n",
    "  -     how many times `a` is retweeted by -> `b` \n",
    "  -     and how many times `b` -> is retweeted by `a`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in 'large_brexit_tweets.pkl' using the pd.read_pickle method\n",
    "\n",
    "df = \n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we will create two dataframes, one for edges and one for node attributes.\n",
    "\n",
    "### Edges \n",
    "So we need to identify the columns that represent the two ends of our edges...\n",
    " - If we imagine that in our network an arrow goes from the original tweet (the 'retweeted status' as Twitter calls it) ---> to the new status (the retweet we collected)...\n",
    " - Look at the columns using `df.info()`\n",
    " - We probably want to have the `user.screen_name` and the `retweeted_status.user.screen_name` as the two ends of our edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the df info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset the dataframe so you are just looking at the two columns we'll use for our edges. Use .copy() so it is an independent object from our original data.\n",
    "\n",
    "edges = \n",
    "edges.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we took our df, which was a list of Tweets, and now are just focusing on...\n",
    "- `user.screen_name` the user that tweeted the status we collected\n",
    "- `retweeted_status.user.screen_name` the user that tweeted the original status update that was retweeted by `user.screen_name`\n",
    "\n",
    "Some of these tweets will not be Retweets, and so will have a `NaN` value in the `retweeted_status.user.screen_name` column. We can check with `edges.info()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the .info() to get an overview of the data we have\n",
    "edges.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop any rows that don't have a value under `retweeted_status.user.screen_name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop any rows that are empty in the column 'retweeted_status.user.screen_name'. Ensure you reset the index\n",
    "edges = \n",
    "edges.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make things simpler, lets rename our columns to reflect their roles in the graph, one will be the `source`, the other the `target`.\n",
    "How you conceptualise the direction of an edge is up to you, and should make sense in terms of the area you are looking it.\n",
    "\n",
    "We will make it so that...\n",
    "- `source` == `retweeted_status.user.screen_name`\n",
    "- `target` == `user.screen_name`\n",
    "\n",
    "... which frames it as the `retweeted_status.user.screen_name` -[influencing]-> the `user.screen_name` who then retweets that information.\n",
    "\n",
    "You could alternatively frame it as the `user.screen_name` -[retweeted the]-> `retweeted_status.user.screen_name` which would then require us to reverse our source and target assignments to make it go in the opposite direction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whilst conceptualising all this is tricky, actually making the change is simply a matter of renaming\n",
    "\n",
    "edges = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also said we were going to make sure we had just one edge between each pair, but assign the edge a 'weight' score that indicated how many times that retweeting had happened. We can do this quickly using Pandas Groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we give every edge a weight of 1 by creating a new 'weight' column and giving every row the value of 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we group by both the source and the target columns and sum together the weights. Make sure you reset the index too\n",
    "# run the groupby without assigning it to anything to check the result\n",
    "\n",
    "edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks good, lets finalise that by overwriting our edges variable- no need to sort it\n",
    "edges = \n",
    "\n",
    "# and create an edge_type column with the value 'retweet'\n",
    "\n",
    "\n",
    "edges['edge_type'] = 'retweet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nodes\n",
    "This dataframe will be a list of unique nodes and we will assign some attributes to the nodes that we can use in Gephi later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create your nodes list by concatenating the source and target columns on top of one another and reset the index\n",
    "\n",
    "nodes = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the .unique method on your nodes list to quickly filter out repeat values. \n",
    "# Wrap the result in a new dataframe with the column name 'node_name' as it will come out as a list\n",
    "\n",
    "\n",
    "nodes= "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example let's use the lifetime statuses count of each user as one attribute that we can use in Gephi later.\n",
    "That means we're going to need...\n",
    "\n",
    "`'user.screen_name', 'user.statuses_count','user.followers_count'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data = df[['user.screen_name', 'user.statuses_count','user.followers_count']].copy()\n",
    "user_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently user_data is essentially a list of tweets showing just the username, and then the status count and follower count of the user at the point they tweeted. This means that a user may occur more than once in the list, with different values. \n",
    "\n",
    "The solution is to ask Pandas to give us the highest value it can find for each column, for each user. We use groupby to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group user_data by the 'user.screen_name' then retrieve the max value and reset the index\n",
    "\n",
    "user_data =\n",
    "user_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have our list of unique nodes and a dataframe of attributes for all the users in our dataset. We can merge these attributes onto our nodes list so that the correct values line up with the correct screen_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge nodes with user data, merging on 'node_name' from the nodes list,\n",
    "# and 'user.screen_name' from the user_data, and we should merge prioritising the nodes dataframe\n",
    "nodes = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can lose 'user.screen_name'. \n",
    "# Pandas thinks it is a unique column because of the different name but we know it is the same as 'node_name'\n",
    "\n",
    "# Do this inplace for fun\n",
    "\n",
    "nodes.drop("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create the attribute dictionary using our string of methods we showed earlier.\n",
    "# Set the index on the node key, transpose, and transform into a dictionary.\n",
    "attribute_dict = \n",
    "attribute_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Using two lines of code we will create our graph and load the node attributes\n",
    "\n",
    "# create the graph from the edges edge list using a DiGraph - Directional Graph\n",
    "G = \n",
    "\n",
    "# set the node attributes of our new graph using our attribute_dict\n",
    "nx.set_node_attributes(G, values=attribute_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the nodes\n",
    "\n",
    "G.nodes(data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the graph to a gexf file named 'large_brexit.gexf'\n",
    "\n",
    "nx.write_gexf(G, 'large_brexit.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:teaching]",
   "language": "python",
   "name": "conda-env-teaching-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
