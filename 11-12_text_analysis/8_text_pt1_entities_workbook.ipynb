{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Text Mining\n",
    "## Part1: Extracting Key Entities from Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For today we're going to use spacy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy uses pre-trained models of language to do a lot of the tasks we need. To create our spacy text tool we need to load in a model. SpaCy has a [load of different models](https://spacy.io/usage/models) for different languages and different types of task.\n",
    "\n",
    "We're going to use `en_core_web_md` which means..\n",
    "- en: English\n",
    "- core: Can perform the core features of Spacy but not some of the more specialised techniques.\n",
    "- web: trained on content from the web such as blogs, news, comments, making it suitable for similar content.\n",
    "- md: medium verion. There is also the small and large models. Small is trained just on web text data from 2013. Medium is trained on [petabytes of data from the contemporary internet](https://commoncrawl.org/big-picture/) and so is much more up to date in how it understands contemporary language use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_phrase = \"I don't see my cat. He has a long tail, fluffy ears and big eyes!\"\\\n",
    "\" He also subscribes to Marxist historical materialism. It's just his way.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we load in our model...\n",
    "nlp = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and create our new text object by wrapping it in our model\n",
    "\n",
    "doc = \n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What just happened?\n",
    "\n",
    "<img src=\"https://github.com/Minyall/sc207_materials/blob/master/images/spacy_pipe.png?raw=true\" width=\"1000\">\n",
    "Our text has just gone on an incredible journey... or been mangled through a ton of processes. \n",
    "\n",
    "##### Tokenization\n",
    "Breaks a string up into individual 'tokens', individual words, pieces of punctuation etc (we'll cover this more later).\n",
    "\n",
    "##### Part-of-Speech Tagging (POS)\n",
    "Tags up each token with a grammatical label such as Noun, Pronoun, Adjective, etc. This tagging is based on the word itself, and the context of each word, i.e. the position of that word in relation to other words, punctuation etc. POS tagging is complex and not directly important for our purposes, but it is fundamental to supporting later processes.\n",
    "\n",
    "##### Dependency Parsing\n",
    "Works out how words within each sentence are related to one another. For example in the sentence below...\n",
    "<img src=\"https://github.com/Minyall/sc207_materials/blob/master/images/spacy_dependency.png?raw=true\" width=\"700\">\n",
    "   - \"big\" is the *adjective modifer (amod)* of \"cat\" as it modifies something about the \"cat\" object.\n",
    "   - \"home\" is the *adverbial modifier (advmod)* of ran as it modifies the generic running, to running \"home\"\n",
    "   \n",
    "Dependency parsing is reliant on the preceeding steps of tokenizing and POS tagging. Again the information this step generates is     not directly of use to us, but it is necessary for any processes that need to work out what words go together, or that cut up text into semantically meaningful chunks.\n",
    "##### Named Entity Recognition (NER)\n",
    "Uses all preceeding steps to be able to predict which tokens likely refer to particular types of entities like people, organisations, dates etc. It is not using any limited list or reference to \"look up\" these entities, but instead identifies them based on all the information generated in the preceeding steps.\n",
    "\n",
    "#### The value of SpaCy\n",
    "\n",
    "In many Natural Language Processing libraries we would have to code all these steps ourselves, making sure that the output of every step is processed in the right way to fit the input of the next step. SpaCy has a pre-built pipeline that covers all of these steps and then wraps the result in a useful object called a SpaCy `document`.\n",
    "\n",
    "Whilst our `doc` looks like a simple string but this is now a SpaCy `Document` which has a whole range of associated methods.\n",
    "See the SpaCy Documentation on the [Document object](https://spacy.io/api/doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy Document Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detect Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Break Down into Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare texts for similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_phrase = nlp(\"I have my cat. He has a long tail, fluffy ears and big eyes!\"\\\n",
    "                     \" He subscribes to Neoliberal free market monthly. It's just his way.\")\n",
    "\n",
    "bio_abstract = nlp(\"Helicobacter pylori pathogenesis and disease outcomes\"\\\n",
    "                        \" are mediated by a complex interplay between bacterial\"\\\n",
    "                         \" virulence factors, host, and environmental factors.\"\\\n",
    "                         \" After H. pylori enters the host stomach, four steps are\"\\\n",
    "                         \" critical for bacteria to establish successful colonization\")\n",
    "                        # extract from https://doi.org/10.1016/j.bj.2015.06.002\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Break Down into Noun Chunks\n",
    "Useful for examining phrases that might summarise the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.noun_chunks will produce a generator. To force the generator to run we wrap it in a list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the Entities in the text\n",
    "Named entity recognition (NER) is the technique of extracting key entities within a piece of text,\n",
    "- people\n",
    "- places\n",
    "- organisations\n",
    "- dates\n",
    "- values\n",
    "- currencies etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump = nlp(\"\"\"A New York judge has ordered President Donald Trump to pay $2m (Â£1.6m)\"\"\"\\\n",
    "            \"\"\" for misusing funds from his charity to finance his 2016 political campaign.\"\"\"\\\n",
    "            \"\"\" The Donald J Trump Foundation closed down in 2018. Prosecutors had accused it\"\"\"\\\n",
    "            \"\"\" of working as \"little more than a chequebook\" for Mr Trump's interests.\"\"\"\\\n",
    "            \"\"\" Charities such as the one Mr Trump and his three eldest children headed cannot\"\"\"\\\n",
    "            \"\"\" engage in politics, the judge ruled.\"\"\")\n",
    "\n",
    "# Source: https://www.bbc.co.uk/news/world-us-canada-50338231"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the entities (again spacy produces a generator that we need to wrap in a list)\n",
    "trump_ents = \n",
    "trump_ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# every object in the entities list has a text attribute and a label attribute to tell you the type of entity it is.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we're in Jupyter we can also use SpaCy's built in visualiser\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to save the annotated version of the\n",
    "# text you can save to html using this function.\n",
    "\n",
    "def save_displacy_to_html(doc, filename, style='ent'):\n",
    "    html_data = spacy.displacy.render(doc, style='ent', jupyter=False, page=True)\n",
    "    with open(filename, 'w+', encoding=\"utf-8\") as f:\n",
    "        f.write(html_data)\n",
    "\n",
    "save_displacy_to_html(trump, 'test.html', style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create a function that can extract specific types of entities from a text\n",
    "\n",
    "def entity_extractor(nlp_doc, entity_type):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_extractor(trump, 'PERSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing many documents\n",
    "A collection of pieces of text is referred to as a *Corpus* in Natural Language Processing. The majority of the time you will be analysing a large corpus of text material such as a collection of...\n",
    "\n",
    "- tweet texts\n",
    "- forum posts, \n",
    "- documents from an archive\n",
    "\n",
    "This means it is important that your code is able to process large numbers of documents simultaneously. Different Python libraries vary in how good they are at handling text at scale.\n",
    "\n",
    "SpaCy is blazing fast if used correctly..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data = pd.read_pickle('news_data.pkl')\n",
    "\n",
    "# examine the dataframe as usual, check the info\n",
    "news_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the head\n",
    "news_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and in this case we can get a sense of what the stories are about by seeing checking how many stories came from each query\n",
    "news_data['query'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `nlp.pipe` to process a list of documents all at once. This streamlines the process which speeds everything up.\n",
    "\n",
    "We feed it our column of texts from the dataframe, and then we wrap the whole thing in a `list`, because nlp.pipe is a generator that will not produce anything until it is iterated over. Wrapping a generator in a list forces it to run and actually output the results.\n",
    "\n",
    "The `%time ` command is a special piece of Jupyter \"\"Magic\"\" that tells us how long a command took to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use nlp.pipe to process ALL the texts in our text column and assign the list of processed texts to a new variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we now assign this to a new column, because the list of processed_docs is in the same order as the dataframe, everything will line up\n",
    "news_data['text_nlp'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember the function we built earlier??\n",
    "def entity_extractor(nlp_doc, entity_type):\n",
    "    ents = list(nlp_doc.ents)\n",
    "    ents_filtered = [ent.text for ent in ents if ent.label_ == entity_type]\n",
    "    unique = list(set(ents_filtered))\n",
    "    return unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now use this function on any value within our new text_nlp column\n",
    "\n",
    "processed_row = \n",
    "entity_extractor(processed_row, entity_type='PERSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Pandas .apply (Advanced usage)\n",
    "\n",
    "As the heavy lifting of the processing is already done we can do this on every row of our dataframe using Pandas `.apply`.\n",
    "\n",
    "First we select the column that has our values we want to use and the `.apply` method built into the column object.\n",
    "\n",
    "`news_data['text_nlp'].apply()`\n",
    "\n",
    "However we need  to tell `.apply` what function to apply to each row in our column...\n",
    "\n",
    "`news_data['text_nlp'].apply(entity_extractor)`\n",
    "\n",
    "Note that we do not need to give the function parenthesis to activate it, apply will handle that part.\n",
    "\n",
    "If our function only needed one argument the above would be fine as `.apply` presumes that it should feed the value of the row to the function as the function's first argument. In our case it would feed the value of the row to the function's `nlp_doc` argument, the first argument in the function.\n",
    "\n",
    "However we have TWO arguments, `nlp_doc` and `entity_type`, both of which are necessary. If we provide this information to the `.apply` function as a \"named argument\" pandas `.apply` will take the information and feed that into the function for us, like so...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that entity_type is an argument specified in our entity_extractor function, but we're passing it to apply.\n",
    "# Apply will recognise that it doesn't use an argument called entity_type and instead feed it to the function we\n",
    "# specified it should use. Apply is clever like that.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can assign the result to a new column, 'entity_person' and \n",
    "# lets also create the column 'entity_org' using the entity_type argument 'ORG'\n",
    "\n",
    "news_data['entity_person'] = \n",
    "news_data['entity_org'] = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data['entity_person']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data['entity_org']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can subset the data to just get the articles from the brexit query\n",
    "brexit_data = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we explode the data to transform our single column with rows of lists, to give each item in each list its own row\n",
    "\n",
    "exploded_df = \n",
    "exploded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see overall the most mentioned 'persons' in the dataset - note that Spacy think brexit is a person....\n",
    "# this might be an interesting indicator of the way brexit is talked about in the press.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we could see what the OVERALL most mentioned person entity is...\n",
    "\n",
    "news_data.explode('entity_person')['entity_person'].value_counts()\n",
    "\n",
    "\n",
    "# ...but this has issues..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Pandas .groupby (Advanced usage)\n",
    "\n",
    "An issue with the above is that the top people mentioned in the dataset will be greatly determined by the number of stories gathered per query. Let's say our end goal is to find the top 5 persons mentioned, but this time for each query individually.\n",
    "\n",
    "We could come up with some various filters and loops to cut up the dataset and spit out the results, but `.groupby` is designed for just these kinds of cases.\n",
    "\n",
    "Groupby works using the process of...\n",
    "\n",
    "#### SPLIT > APPLY > COMBINE\n",
    "\n",
    "<img src=\"https://github.com/Minyall/sc207_materials/blob/master/images/groupby.png?raw=true\" width=\"600\">\n",
    "\n",
    "- Split the Dataframe into seperate dataframes based on the values in a particular column - usually some sort of category\n",
    "- Apply a method or a function to each dataframe\n",
    "- Combine those dataframes back into a single dataframe and return it...\n",
    "\n",
    "\n",
    "First we explode the dataframe on the entity_column\n",
    "\n",
    "`news_data.explode('entity_person')`\n",
    "\n",
    "This will give us a dataframe where we convert our lists of persons in each row, to individual rows per person per article, as we did above. Then we move on to the groupby process.\n",
    "\n",
    "The next step we `.groupby` the 'query' column, this means we have an object that is in effect 7 different dataframes, one for each query (The Split stage). This creates a `DataFrameGroupBy` object. We can't examine its contents yet because we haven't applied any transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_data = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_query = \n",
    "groupby_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has applied a transformation to the groupby object, and it has returned us a single object of all 7 transformed dataframes combined.\n",
    "It has split into multiple dataframe based on our groupby command.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every command we give to this object will be applied to each of the 7 dataframes at the same time.\n",
    "If we try .value_counts() it will give us the value_counts() for the entity_person column, for each dataframe seperately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the value counts of how many times each PERSON entity was mentioned within each of the seperate query dataframes. However because it is returning the entire set of results for each dataframe, we can't see much. Currently we're just looking at the top and bottom of the entire set of results. The very top of the first dataframe, and the very end of the bottom dataframe, and everything in between is hidden because there is too much data.\n",
    "\n",
    "HOWEVER!\n",
    "If we take this result, and group it AGAIN by the 'query' column, we will be cutting up this set of results into indivdual dataframes again. We can then just ask for the top 5 results from each, by using `.head(5)`.\n",
    "\n",
    "This will take the top 5 from each dataframe, and then combine them into a single dataframe and pass it back to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make life easier we can always create a function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_entity(data, entity_col, groupby_col, top_n=5):\n",
    "    exploded = data.explode(entity_col)\n",
    "    g = exploded.groupby(groupby_col)\n",
    "    result = g[entity_col].value_counts().groupby(groupby_col).head(top_n)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_entity(data=news_data, entity_col='entity_person', groupby_col='query', top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_entity(data=news_data, entity_col='entity_org', groupby_col='query', top_n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Entity Lists to Entity Networks\n",
    "\n",
    "One analysis approach we can use is to build a network where each node is the name of an entity, and an edge exists between them if they co-occur in the same piece of text. For every co-occurence their edge is weighted +1.\n",
    "\n",
    "This is relatively easy to do with a few tricks in Pandas and Networkx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an edge list where the source is just an id number for the article and the target is the name in the article.\n",
    "\n",
    "# by exploding the entity column we get a dataframe where each row is repeated for every item in the entity list.\n",
    "exploded = \n",
    "\n",
    "# we take a copy of just two columns, the newly exploded 'entity_person' column, and the uuid column which is a unique id\n",
    "# assigned to each article\n",
    "\n",
    "edge_list = \n",
    "\n",
    "#then we just rename the appropriate columns\n",
    "edge_list = \n",
    "edge_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our graph from the edge list\n",
    "\n",
    "G = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have our graph which consists of nodes representing unique articles, and nodes representing people, with edges going between the articles and people if a person is mentioned in the article.\n",
    "\n",
    "We could examine this to see the result..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gexf(G, 'article_person.gexf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bipartite Graphs\n",
    "\n",
    "Our graph is a bi-partite graph.\n",
    "\n",
    "Bipartite graphs are networks where there are two types of nodes and where there are only edges between nodes of different types, never edges between nodes of the same type.\n",
    "\n",
    "In our graph we have nodes representing articles, and nodes representing people. Currently there are only edges between people and the articles that they appear in, not edges between people, and not edges between articles.\n",
    "\n",
    "##### Bi-Partite Projection\n",
    "<img src=\"https://github.com/Minyall/sc207_materials/blob/master/images/bipartite.png?raw=true\" align=\"right\" width=\"300\">\n",
    "Projection can be understood as the process of removing the 'intermediaries' between nodes, and connecting those nodes together directly. In the example image, Nodes A, B and C are not directly connected, but they do share intermediary nodes, 1, 2 and 3. The simple graph just represents that there is some form of connection, the multigraph demonstrates that...\n",
    "\n",
    "- there are two shared connections between A and B (intermediary nodes 1 & 2)\n",
    "- there are two shared connections between B and C (intermediaries, 2 and 3).\n",
    "- There is only one connection between A and C (intermediary node 2).\n",
    "\n",
    "\n",
    "Image from https://arxiv.org/pdf/1909.10977.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now if we check we can see our graph is a \"bipartite graph\"\n",
    "\n",
    "nx.bipartite.is_bipartite(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms we can can use projection techniques on this graph.\n",
    "We can use the networkx function `nx.bipartite.weighted_projected_graph` to create our projection.\n",
    "\n",
    "This function requires\n",
    "\n",
    "- The graph you want to project from\n",
    "- A list of the nodes you want to retain - in our case our list of people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can create a list of unique nodes by taking out 'target' column of our edge \n",
    "# list (which remember was our list of PERSON entities) and using .unique()\n",
    "\n",
    "nodes_to_keep =\n",
    "nodes_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create our final graph\n",
    "\n",
    "projected_G ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a graph of just PERSON entity nodes, with edges weighted by the number of times those PERSON entities co-occurred in an article. It is likely that the majority of edges will have a weight of 1. We're interested to see if entities co-occur a lot so we could filter out these edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current number of edges\n",
    "projected_G.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets identify all the edges that have a weight greater than 1\n",
    "# we'll use this to filter our graph\n",
    "\n",
    "edges_to_keep = []\n",
    "\n",
    "# the if we iterate over a graph using the .edges() method it gives us two values\n",
    "# the source and the target of the edge.\n",
    "\n",
    "# for source, target in projected_G.edges():\n",
    "#     Do something in this loop\n",
    "#\n",
    "#\n",
    "#\n",
    "\n",
    "# if we give .edges the argument data=True, it also gives us the attributes of each edge as a dictionary\n",
    "\n",
    "for source, target, edge_attributes in projected_G.edges(data=True):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is a quick view of some of our edges\n",
    "edges_to_keep[1050:1100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(edges_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the adventurous we could have achieved the same as above in a list comprehension,\n",
    "# though it is not as readable\n",
    "\n",
    "edges_to_keep = [(source,target) for source,target, edge_attributes in projected_G.edges(data=True) if edge_attributes['weight']>1]\n",
    "len(edges_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can easily filter the graph using the nx.edge_subgraph function\n",
    "# this function requires the graph to be filtered, and a list of edges to keep.\n",
    "\n",
    "filtered_G = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_original_edges = projected_G.number_of_edges()\n",
    "n_original_nodes = projected_G.number_of_nodes()\n",
    "\n",
    "n_new_edges = filtered_G.number_of_edges()\n",
    "n_new_nodes = filtered_G.number_of_nodes()\n",
    "\n",
    "def pct_decrease(original,new):\n",
    "    decrease = original - new\n",
    "    return round((decrease/original)*100,2)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Original Graph - Nodes: {n_original_nodes}\")\n",
    "print(f\"Original Graph - Edges: {n_original_edges}\")\n",
    "print(f\"New Graph - Nodes: {n_new_nodes}\")\n",
    "print(f\"New Graph - Edges: {n_new_edges}\")\n",
    "\n",
    "print(f\"Graph Nodes decreased by: {pct_decrease(n_original_nodes, n_new_nodes)}%\")\n",
    "print(f\"Graph Edges decreased by: {pct_decrease(n_original_edges, n_new_edges)}%\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the filtered graph to a gexf and take a look in Gephi\n",
    "\n",
    "nx.write_gexf(filtered_G, 'entity_network.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:teaching]",
   "language": "python",
   "name": "conda-env-teaching-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
