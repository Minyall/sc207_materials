{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Text Mining\n",
    "## Part1: Extracting Key Entities from Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For today we're going to use spacy\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy uses pre-trained models of language to do a lot of the tasks we need. To create our spacy text tool we need to load in a model. SpaCy has a [load of different models](https://spacy.io/usage/models) for different languages and different types of task.\n",
    "\n",
    "We're going to use `en_core_web_md` which means..\n",
    "- en: English\n",
    "- core: Can perform the core features of Spacy but not some of the more specialised techniques.\n",
    "- web: trained on content from the web such as blogs, news, comments, making it suitable for similar content.\n",
    "- md: medium verion. There is also the small and large models. Small is trained just on web text data from 2013. Medium is trained on [petabytes of data from the contemporary internet](https://commoncrawl.org/big-picture/) and so is much more up to date in how it understands contemporary language use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_phrase = \"I don't see my cat. He has a long tail, fluffy ears and big eyes!\"\\\n",
    "\" He also subscribes to Marxist historical materialism. It's just his way.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we load in our model...\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I don't see my cat. He has a long tail, fluffy ears and big eyes! He also subscribes to Marxist historical materialism. It's just his way."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and create our new text object by wrapping it in our model\n",
    "\n",
    "doc = nlp(test_phrase)\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What just happened?\n",
    "\n",
    "<img src=\"https://github.com/Minyall/sc207_materials/blob/master/images/spacy_pipe.png?raw=true\" width=\"1000\">\n",
    "Our text has just gone on an incredible journey... or been mangled through a ton of processes. \n",
    "\n",
    "##### Tokenization\n",
    "Breaks a string up into individual 'tokens', individual words, pieces of punctuation etc (we'll cover this more later).\n",
    "\n",
    "##### Part-of-Speech Tagging (POS)\n",
    "Tags up each token with a grammatical label such as Noun, Pronoun, Adjective, etc. This tagging is based on the word itself, and the context of each word, i.e. the position of that word in relation to other words, punctuation etc. POS tagging is complex and not directly important for our purposes, but it is fundamental to supporting later processes.\n",
    "\n",
    "##### Dependency Parsing\n",
    "Works out how words within each sentence are related to one another. For example in the sentence below...\n",
    "<img src=\"https://github.com/Minyall/sc207_materials/blob/master/images/spacy_dependency.png?raw=true\" width=\"700\">\n",
    "   - \"big\" is the *adjective modifer (amod)* of \"cat\" as it modifies something about the \"cat\" object.\n",
    "   - \"home\" is the *adverbial modifier (advmod)* of ran as it modifies the generic running, to running \"home\"\n",
    "   \n",
    "Dependency parsing is reliant on the preceeding steps of tokenizing and POS tagging. Again the information this step generates is     not directly of use to us, but it is necessary for any processes that need to work out what words go together, or that cut up text into semantically meaningful chunks.\n",
    "##### Named Entity Recognition (NER)\n",
    "Uses all preceeding steps to be able to predict which tokens likely refer to particular types of entities like people, organisations, dates etc. It is not using any limited list or reference to \"look up\" these entities, but instead identifies them based on all the information generated in the preceeding steps.\n",
    "\n",
    "#### The value of SpaCy\n",
    "\n",
    "In many Natural Language Processing libraries we would have to code all these steps ourselves, making sure that the output of every step is processed in the right way to fit the input of the next step. SpaCy has a pre-built pipeline that covers all of these steps and then wraps the result in a useful object called a SpaCy `document`.\n",
    "\n",
    "Whilst our `doc` looks like a simple string but this is now a SpaCy `Document` which has a whole range of associated methods.\n",
    "See the SpaCy Documentation on the [Document object](https://spacy.io/api/doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I don't see my cat. He has a long tail, fluffy ears and big eyes! He also subscribes to Marxist historical materialism. It's just his way."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy Document Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detect Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.lang_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Break Down into Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[I don't see my cat.,\n",
       " He has a long tail, fluffy ears and big eyes!,\n",
       " He also subscribes to Marxist historical materialism.,\n",
       " It's just his way.]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(doc.sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare texts for similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9850796595679977\n",
      "0.678126115963195\n"
     ]
    }
   ],
   "source": [
    "similar_phrase = nlp(\"I have my cat. He has a long tail, fluffy ears and big eyes!\"\\\n",
    "                     \" He subscribes to Neoliberal free market monthly. It's just his way.\")\n",
    "\n",
    "bio_abstract = nlp(\"Helicobacter pylori pathogenesis and disease outcomes\"\\\n",
    "                        \" are mediated by a complex interplay between bacterial\"\\\n",
    "                         \" virulence factors, host, and environmental factors.\"\\\n",
    "                         \" After H. pylori enters the host stomach, four steps are\"\\\n",
    "                         \" critical for bacteria to establish successful colonization\")\n",
    "                        # extract from https://doi.org/10.1016/j.bj.2015.06.002\n",
    "\n",
    "    \n",
    "print(doc.similarity(similar_phrase))\n",
    "\n",
    "print(doc.similarity(bio_abstract))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Break Down into Noun Chunks\n",
    "Useful for examining phrases that might summarise the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[I,\n",
       " my cat,\n",
       " He,\n",
       " a long tail,\n",
       " fluffy ears,\n",
       " big eyes,\n",
       " He,\n",
       " Marxist historical materialism,\n",
       " It,\n",
       " just his way]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#.noun_chunks will produce a generator. To force the generator to run we wrap it in a list.\n",
    "list(doc.noun_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Helicobacter pylori pathogenesis,\n",
       " disease outcomes,\n",
       " a complex interplay,\n",
       " bacterial virulence factors,\n",
       " host,\n",
       " environmental factors,\n",
       " H. pylori,\n",
       " the host stomach,\n",
       " four steps,\n",
       " bacteria,\n",
       " successful colonization]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bio_abstract.noun_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the Entities in the text\n",
    "Named entity recognition (NER) is the technique of extracting key entities within a piece of text,\n",
    "- people\n",
    "- places\n",
    "- organisations\n",
    "- dates\n",
    "- values\n",
    "- currencies etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump = nlp(\"\"\"A New York judge has ordered President Donald Trump to pay $2m (£1.6m)\"\"\"\\\n",
    "            \"\"\" for misusing funds from his charity to finance his 2016 political campaign.\"\"\"\\\n",
    "            \"\"\" The Donald J Trump Foundation closed down in 2018. Prosecutors had accused it\"\"\"\\\n",
    "            \"\"\" of working as \"little more than a chequebook\" for Mr Trump's interests.\"\"\"\\\n",
    "            \"\"\" Charities such as the one Mr Trump and his three eldest children headed cannot\"\"\"\\\n",
    "            \"\"\" engage in politics, the judge ruled.\"\"\")\n",
    "\n",
    "# Source: https://www.bbc.co.uk/news/world-us-canada-50338231"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[New York,\n",
       " Donald Trump,\n",
       " $2m,\n",
       " £1.6m,\n",
       " 2016,\n",
       " Trump Foundation,\n",
       " 2018,\n",
       " Trump,\n",
       " one,\n",
       " Trump,\n",
       " three]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list the entities (again spacy produces a generator that we need to wrap in a list)\n",
    "trump_ents = list(trump.ents)\n",
    "trump_ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New York GPE\n",
      "Donald Trump PERSON\n",
      "$2m MONEY\n",
      "£1.6m MONEY\n",
      "2016 DATE\n",
      "Trump Foundation ORG\n",
      "2018 DATE\n",
      "Trump PERSON\n",
      "one CARDINAL\n",
      "Trump PERSON\n",
      "three CARDINAL\n"
     ]
    }
   ],
   "source": [
    "# every object in the entities list has a text attribute and a label attribute to tell you the type of entity it is.\n",
    "\n",
    "for entity in trump_ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">A \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    New York\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " judge has ordered President \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Donald Trump\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " to pay \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $2m\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       " (\n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    £1.6m\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       ") for misusing funds from his charity to finance his \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2016\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " political campaign. The Donald J \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Trump Foundation\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " closed down in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2018\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ". Prosecutors had accused it of working as &quot;little more than a chequebook&quot; for Mr \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Trump\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       "'s interests. Charities such as the \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    one\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " Mr \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Trump\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " and his \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    three\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " eldest children headed cannot engage in politics, the judge ruled.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# as we're in Jupyter we can also use SpaCy's built in visualiser\n",
    "\n",
    "spacy.displacy.render(trump,style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to save the annotated version of the\n",
    "# text you can save to html using this function.\n",
    "\n",
    "def save_displacy_to_html(doc, filename, style='ent'):\n",
    "    html_data = spacy.displacy.render(doc, style='ent', jupyter=False, page=True)\n",
    "    with open(filename, 'w+', encoding=\"utf-8\") as f:\n",
    "        f.write(html_data)\n",
    "\n",
    "save_displacy_to_html(trump, 'test.html', style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create a function that can extract specific types of entities from a text\n",
    "\n",
    "def entity_extractor(nlp_doc, entity_type):\n",
    "    ents = list(nlp_doc.ents)\n",
    "    ents_filtered = [ent.text for ent in ents if ent.label_ == entity_type]\n",
    "    unique = list(set(ents_filtered))\n",
    "    return unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Trump', 'Donald Trump']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_extractor(trump, 'PERSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing many documents\n",
    "A collection of pieces of text is referred to as a *Corpus* in Natural Language Processing. The majority of the time you will be analysing a large corpus of text material such as a collection of...\n",
    "\n",
    "- tweet texts\n",
    "- forum posts, \n",
    "- documents from an archive\n",
    "\n",
    "This means it is important that your code is able to process large numbers of documents simultaneously. Different Python libraries vary in how good they are at handling text at scale.\n",
    "\n",
    "SpaCy is blazing fast if used correctly..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 972 entries, 0 to 971\n",
      "Data columns (total 6 columns):\n",
      "uuid                 972 non-null object\n",
      "query                972 non-null object\n",
      "thread.title_full    972 non-null object\n",
      "text                 972 non-null object\n",
      "published            972 non-null object\n",
      "thread.site          972 non-null object\n",
      "dtypes: object(6)\n",
      "memory usage: 45.7+ KB\n"
     ]
    }
   ],
   "source": [
    "news_data = pd.read_pickle('news_data.pkl')\n",
    "\n",
    "# examine the dataframe as usual, check the info\n",
    "news_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uuid</th>\n",
       "      <th>query</th>\n",
       "      <th>thread.title_full</th>\n",
       "      <th>text</th>\n",
       "      <th>published</th>\n",
       "      <th>thread.site</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e25fb36d29acc983cefbc8b61328b88dd4f7cffe</td>\n",
       "      <td>brexit</td>\n",
       "      <td>Have UK voters changed their minds on Brexit? ...</td>\n",
       "      <td>Image copyright Getty Images UK Prime Minister...</td>\n",
       "      <td>2019-10-16T03:00:00.000+03:00</td>\n",
       "      <td>bbc.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0b63b531f4ca3eb5f2c6968163787c6e95c1b2f2</td>\n",
       "      <td>brexit</td>\n",
       "      <td>Stock market news: October 16, 2019</td>\n",
       "      <td>Stocks were off slightly as investors consider...</td>\n",
       "      <td>2019-10-16T16:32:00.000+03:00</td>\n",
       "      <td>yahoo.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a7386fc9648e01a10d62b0b015a132329b349c86</td>\n",
       "      <td>brexit</td>\n",
       "      <td>Brexit: What are the backstop options? - BBC News</td>\n",
       "      <td>Image copyright Getty Images A key part of the...</td>\n",
       "      <td>2019-10-16T13:01:00.000+03:00</td>\n",
       "      <td>bbc.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>755f1df8cd75db18c59eb397bada0daa9e24cbf8</td>\n",
       "      <td>brexit</td>\n",
       "      <td>New IRA says border infrastructure would be ‘l...</td>\n",
       "      <td>Send Load more share options\\nThe New IRA has ...</td>\n",
       "      <td>2019-10-16T03:00:00.000+03:00</td>\n",
       "      <td>channel4.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d25f1165001889962c0f8370cded464737309468</td>\n",
       "      <td>brexit</td>\n",
       "      <td>Brexit: 'No deal tonight', UK government sourc...</td>\n",
       "      <td>The issue of the Irish border - and how to han...</td>\n",
       "      <td>2019-10-16T03:00:00.000+03:00</td>\n",
       "      <td>bbc.co.uk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       uuid   query  \\\n",
       "0  e25fb36d29acc983cefbc8b61328b88dd4f7cffe  brexit   \n",
       "1  0b63b531f4ca3eb5f2c6968163787c6e95c1b2f2  brexit   \n",
       "2  a7386fc9648e01a10d62b0b015a132329b349c86  brexit   \n",
       "3  755f1df8cd75db18c59eb397bada0daa9e24cbf8  brexit   \n",
       "4  d25f1165001889962c0f8370cded464737309468  brexit   \n",
       "\n",
       "                                   thread.title_full  \\\n",
       "0  Have UK voters changed their minds on Brexit? ...   \n",
       "1                Stock market news: October 16, 2019   \n",
       "2  Brexit: What are the backstop options? - BBC News   \n",
       "3  New IRA says border infrastructure would be ‘l...   \n",
       "4  Brexit: 'No deal tonight', UK government sourc...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Image copyright Getty Images UK Prime Minister...   \n",
       "1  Stocks were off slightly as investors consider...   \n",
       "2  Image copyright Getty Images A key part of the...   \n",
       "3  Send Load more share options\\nThe New IRA has ...   \n",
       "4  The issue of the Irish border - and how to han...   \n",
       "\n",
       "                       published   thread.site  \n",
       "0  2019-10-16T03:00:00.000+03:00     bbc.co.uk  \n",
       "1  2019-10-16T16:32:00.000+03:00     yahoo.com  \n",
       "2  2019-10-16T13:01:00.000+03:00     bbc.co.uk  \n",
       "3  2019-10-16T03:00:00.000+03:00  channel4.com  \n",
       "4  2019-10-16T03:00:00.000+03:00     bbc.co.uk  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the head\n",
    "news_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "brexit            352\n",
       "billionaire       278\n",
       "Hong Kong         150\n",
       "Tesla              98\n",
       "alt-right          41\n",
       "cryptocurrency     28\n",
       "bitcoin            25\n",
       "Name: query, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and in this case we can get a sense of what the stories are about by seeing checking how many soriest came from each query\n",
    "news_data['query'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `nlp.pipe` to process a list of documents all at once. This streamlines the process which speeds everything up.\n",
    "\n",
    "We feed it our column of texts from the dataframe, and then we wrap the whole thing in a `list`, because nlp.pipe is a generator that will not produce anything until it is iterated over. Wrapping a generator in a list forces it to run and actually output the results.\n",
    "\n",
    "The `%time ` command is a special piece of Jupyter \"\"Magic\"\" that tells us how long a command took to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 s, sys: 1.15 s, total: 13.1 s\n",
      "Wall time: 1min 43s\n"
     ]
    }
   ],
   "source": [
    "# we can use nlp.pipe to process ALL the texts in our text column and assign the list of processed texts to a new variable\n",
    "%time processed_docs = list(nlp.pipe(news_data['text'], n_process=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we now assign this to a new column, because the list of processed_docs is in the same order as the dataframe, everything will line up\n",
    "news_data['text_nlp'] = processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uuid</th>\n",
       "      <th>query</th>\n",
       "      <th>thread.title_full</th>\n",
       "      <th>text</th>\n",
       "      <th>published</th>\n",
       "      <th>thread.site</th>\n",
       "      <th>text_nlp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e25fb36d29acc983cefbc8b61328b88dd4f7cffe</td>\n",
       "      <td>brexit</td>\n",
       "      <td>Have UK voters changed their minds on Brexit? ...</td>\n",
       "      <td>Image copyright Getty Images UK Prime Minister...</td>\n",
       "      <td>2019-10-16T03:00:00.000+03:00</td>\n",
       "      <td>bbc.co.uk</td>\n",
       "      <td>(Image, copyright, Getty, Images, UK, Prime, M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0b63b531f4ca3eb5f2c6968163787c6e95c1b2f2</td>\n",
       "      <td>brexit</td>\n",
       "      <td>Stock market news: October 16, 2019</td>\n",
       "      <td>Stocks were off slightly as investors consider...</td>\n",
       "      <td>2019-10-16T16:32:00.000+03:00</td>\n",
       "      <td>yahoo.com</td>\n",
       "      <td>(Stocks, were, off, slightly, as, investors, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a7386fc9648e01a10d62b0b015a132329b349c86</td>\n",
       "      <td>brexit</td>\n",
       "      <td>Brexit: What are the backstop options? - BBC News</td>\n",
       "      <td>Image copyright Getty Images A key part of the...</td>\n",
       "      <td>2019-10-16T13:01:00.000+03:00</td>\n",
       "      <td>bbc.co.uk</td>\n",
       "      <td>(Image, copyright, Getty, Images, A, key, part...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>755f1df8cd75db18c59eb397bada0daa9e24cbf8</td>\n",
       "      <td>brexit</td>\n",
       "      <td>New IRA says border infrastructure would be ‘l...</td>\n",
       "      <td>Send Load more share options\\nThe New IRA has ...</td>\n",
       "      <td>2019-10-16T03:00:00.000+03:00</td>\n",
       "      <td>channel4.com</td>\n",
       "      <td>(Send, Load, more, share, options, \\n, The, Ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d25f1165001889962c0f8370cded464737309468</td>\n",
       "      <td>brexit</td>\n",
       "      <td>Brexit: 'No deal tonight', UK government sourc...</td>\n",
       "      <td>The issue of the Irish border - and how to han...</td>\n",
       "      <td>2019-10-16T03:00:00.000+03:00</td>\n",
       "      <td>bbc.co.uk</td>\n",
       "      <td>(The, issue, of, the, Irish, border, -, and, h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       uuid   query  \\\n",
       "0  e25fb36d29acc983cefbc8b61328b88dd4f7cffe  brexit   \n",
       "1  0b63b531f4ca3eb5f2c6968163787c6e95c1b2f2  brexit   \n",
       "2  a7386fc9648e01a10d62b0b015a132329b349c86  brexit   \n",
       "3  755f1df8cd75db18c59eb397bada0daa9e24cbf8  brexit   \n",
       "4  d25f1165001889962c0f8370cded464737309468  brexit   \n",
       "\n",
       "                                   thread.title_full  \\\n",
       "0  Have UK voters changed their minds on Brexit? ...   \n",
       "1                Stock market news: October 16, 2019   \n",
       "2  Brexit: What are the backstop options? - BBC News   \n",
       "3  New IRA says border infrastructure would be ‘l...   \n",
       "4  Brexit: 'No deal tonight', UK government sourc...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Image copyright Getty Images UK Prime Minister...   \n",
       "1  Stocks were off slightly as investors consider...   \n",
       "2  Image copyright Getty Images A key part of the...   \n",
       "3  Send Load more share options\\nThe New IRA has ...   \n",
       "4  The issue of the Irish border - and how to han...   \n",
       "\n",
       "                       published   thread.site  \\\n",
       "0  2019-10-16T03:00:00.000+03:00     bbc.co.uk   \n",
       "1  2019-10-16T16:32:00.000+03:00     yahoo.com   \n",
       "2  2019-10-16T13:01:00.000+03:00     bbc.co.uk   \n",
       "3  2019-10-16T03:00:00.000+03:00  channel4.com   \n",
       "4  2019-10-16T03:00:00.000+03:00     bbc.co.uk   \n",
       "\n",
       "                                            text_nlp  \n",
       "0  (Image, copyright, Getty, Images, UK, Prime, M...  \n",
       "1  (Stocks, were, off, slightly, as, investors, c...  \n",
       "2  (Image, copyright, Getty, Images, A, key, part...  \n",
       "3  (Send, Load, more, share, options, \\n, The, Ne...  \n",
       "4  (The, issue, of, the, Irish, border, -, and, h...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember the function we built earlier??\n",
    "def entity_extractor(nlp_doc, entity_type):\n",
    "    ents = list(nlp_doc.ents)\n",
    "    ents_filtered = [ent.text for ent in ents if ent.label_ == entity_type]\n",
    "    unique = list(set(ents_filtered))\n",
    "    return unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Boris Johnson',\n",
       " 'Johnson',\n",
       " 'Deltapoll',\n",
       " 'Brexit',\n",
       " 'Dominic Bailey',\n",
       " 'Panelbase',\n",
       " 'David Brown',\n",
       " 'Getty Images UK',\n",
       " 'Opinium',\n",
       " 'John Curtice',\n",
       " 'Kantar']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now use this function on any value within our new text_nlp column\n",
    "\n",
    "processed_row = news_data.loc[0, 'text_nlp']\n",
    "entity_extractor(processed_row, entity_type='PERSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Pandas .apply (Advanced usage)\n",
    "\n",
    "As the heavy lifting of the processing is already done we can do this on every row of our dataframe using Pandas `.apply`.\n",
    "\n",
    "First we select the column that has our values we want to use and the `.apply` method built into the column object.\n",
    "\n",
    "`news_data['text_nlp'].apply()`\n",
    "\n",
    "However we need  to tell `.apply` what function to apply to each row in our column...\n",
    "\n",
    "`news_data['text_nlp'].apply(entity_extractor)`\n",
    "\n",
    "Note that we do not need to give the function parenthesis to activate it, apply will handle that part.\n",
    "\n",
    "If our function only needed one argument the above would be fine as `.apply` presumes that it should feed the value of the row to the function as the function's first argument. In our case it would feed the value of the row to the function's `nlp_doc` argument, the first argument in the function.\n",
    "\n",
    "However we have TWO arguments, `nlp_doc` and `entity_type`, both of which are necessary. If we provide this information to the `.apply` function as a \"named argument\" pandas `.apply` will take the information and feed that into the function for us, like so...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [Boris Johnson, Johnson, Deltapoll, Brexit, Do...\n",
       "1                                     [Brendan McDermid]\n",
       "2                           [Boris Johnson, Brexit, May]\n",
       "3      [Alex Thomson, Ben De Pear, Theresa May, Brexi...\n",
       "4      [Boris Johnson, Johnson, Tory Brexiteers, Stev...\n",
       "                             ...                        \n",
       "967                    [Pepe, Frog, Donald Trump, Wojak]\n",
       "968    [Jonathan Turton, Turton, Violet Chachki, Jess...\n",
       "969    [Fuentes, Nick Fuentes, Heather Heyer, Spencer...\n",
       "970    [Nigel Farage, Mark Reckless, Boris Johnson, N...\n",
       "971                                                   []\n",
       "Name: text_nlp, Length: 972, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that entity_type is an argument specified in our entity_extractor function, but we're passing it to apply.\n",
    "# Apply will recognise that it doesn't use an argument called entity_type and instead feed it to the function we\n",
    "# specified it should use. Apply is clever like that.\n",
    "\n",
    "news_data['text_nlp'].apply(entity_extractor, entity_type='PERSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can assign the result to a new column, 'entity_person' and \n",
    "# lets also create the column 'entity_org' using the entity_type argument 'ORG'\n",
    "\n",
    "news_data['entity_person'] = news_data['text_nlp'].apply(entity_extractor, entity_type='PERSON')\n",
    "news_data['entity_org'] = news_data['text_nlp'].apply(entity_extractor, entity_type='ORG')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [Boris Johnson, Johnson, Deltapoll, Brexit, Do...\n",
       "1                                     [Brendan McDermid]\n",
       "2                           [Boris Johnson, Brexit, May]\n",
       "3      [Alex Thomson, Ben De Pear, Theresa May, Brexi...\n",
       "4      [Boris Johnson, Johnson, Tory Brexiteers, Stev...\n",
       "                             ...                        \n",
       "967                    [Pepe, Frog, Donald Trump, Wojak]\n",
       "968    [Jonathan Turton, Turton, Violet Chachki, Jess...\n",
       "969    [Fuentes, Nick Fuentes, Heather Heyer, Spencer...\n",
       "970    [Nigel Farage, Mark Reckless, Boris Johnson, N...\n",
       "971                                                   []\n",
       "Name: entity_person, Length: 972, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data['entity_person']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [BMG, 27%-28, the European Union, Leave, Strat...\n",
       "1      [House, Dow, Senate, the New York Stock Exchan...\n",
       "2      [Theresa May's, Twitter\\n, GPS, Theresa, DUP, ...\n",
       "3      [PSNI, the Police Service of Northern Ireland,...\n",
       "4      [the European Research Group, the House of Com...\n",
       "                             ...                        \n",
       "967    [NGO, Victoria Police, EAD, the Anti-Defamatio...\n",
       "968    [the Ryerson University, RT, Mélançon-Golden's...\n",
       "969    [CNN, Twitter, Charlottesville, Guardian, Ex-B...\n",
       "970    [Tice, https://guardianbookshop.com/decline-an...\n",
       "971    [Paint, BMP, Caps, Snip & Sketch, Snipping Too...\n",
       "Name: entity_org, Length: 972, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data['entity_org']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can subset the data to just get the articles from the brexit query\n",
    "brexit_data = news_data[news_data['query'] == 'brexit'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uuid</th>\n",
       "      <th>query</th>\n",
       "      <th>thread.title_full</th>\n",
       "      <th>text</th>\n",
       "      <th>published</th>\n",
       "      <th>thread.site</th>\n",
       "      <th>text_nlp</th>\n",
       "      <th>entity_person</th>\n",
       "      <th>entity_org</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e25fb36d29acc983cefbc8b61328b88dd4f7cffe</td>\n",
       "      <td>brexit</td>\n",
       "      <td>Have UK voters changed their minds on Brexit? ...</td>\n",
       "      <td>Image copyright Getty Images UK Prime Minister...</td>\n",
       "      <td>2019-10-16T03:00:00.000+03:00</td>\n",
       "      <td>bbc.co.uk</td>\n",
       "      <td>(Image, copyright, Getty, Images, UK, Prime, M...</td>\n",
       "      <td>Boris Johnson</td>\n",
       "      <td>[BMG, 27%-28, the European Union, Leave, Strat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e25fb36d29acc983cefbc8b61328b88dd4f7cffe</td>\n",
       "      <td>brexit</td>\n",
       "      <td>Have UK voters changed their minds on Brexit? ...</td>\n",
       "      <td>Image copyright Getty Images UK Prime Minister...</td>\n",
       "      <td>2019-10-16T03:00:00.000+03:00</td>\n",
       "      <td>bbc.co.uk</td>\n",
       "      <td>(Image, copyright, Getty, Images, UK, Prime, M...</td>\n",
       "      <td>Johnson</td>\n",
       "      <td>[BMG, 27%-28, the European Union, Leave, Strat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e25fb36d29acc983cefbc8b61328b88dd4f7cffe</td>\n",
       "      <td>brexit</td>\n",
       "      <td>Have UK voters changed their minds on Brexit? ...</td>\n",
       "      <td>Image copyright Getty Images UK Prime Minister...</td>\n",
       "      <td>2019-10-16T03:00:00.000+03:00</td>\n",
       "      <td>bbc.co.uk</td>\n",
       "      <td>(Image, copyright, Getty, Images, UK, Prime, M...</td>\n",
       "      <td>Deltapoll</td>\n",
       "      <td>[BMG, 27%-28, the European Union, Leave, Strat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e25fb36d29acc983cefbc8b61328b88dd4f7cffe</td>\n",
       "      <td>brexit</td>\n",
       "      <td>Have UK voters changed their minds on Brexit? ...</td>\n",
       "      <td>Image copyright Getty Images UK Prime Minister...</td>\n",
       "      <td>2019-10-16T03:00:00.000+03:00</td>\n",
       "      <td>bbc.co.uk</td>\n",
       "      <td>(Image, copyright, Getty, Images, UK, Prime, M...</td>\n",
       "      <td>Brexit</td>\n",
       "      <td>[BMG, 27%-28, the European Union, Leave, Strat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e25fb36d29acc983cefbc8b61328b88dd4f7cffe</td>\n",
       "      <td>brexit</td>\n",
       "      <td>Have UK voters changed their minds on Brexit? ...</td>\n",
       "      <td>Image copyright Getty Images UK Prime Minister...</td>\n",
       "      <td>2019-10-16T03:00:00.000+03:00</td>\n",
       "      <td>bbc.co.uk</td>\n",
       "      <td>(Image, copyright, Getty, Images, UK, Prime, M...</td>\n",
       "      <td>Dominic Bailey</td>\n",
       "      <td>[BMG, 27%-28, the European Union, Leave, Strat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>1faf83bfead571acc2fe725ad5a6d44c9dcee446</td>\n",
       "      <td>brexit</td>\n",
       "      <td>General election 2019: Farage calls on Johnson...</td>\n",
       "      <td>Media playback is unsupported on your device M...</td>\n",
       "      <td>2019-11-01T14:16:00.000+02:00</td>\n",
       "      <td>bbc.co.uk</td>\n",
       "      <td>(Media, playback, is, unsupported, on, your, d...</td>\n",
       "      <td>pro-Brexit</td>\n",
       "      <td>[Plaid Cymru, Brexit Party, the Lib Dems, NHS,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>1faf83bfead571acc2fe725ad5a6d44c9dcee446</td>\n",
       "      <td>brexit</td>\n",
       "      <td>General election 2019: Farage calls on Johnson...</td>\n",
       "      <td>Media playback is unsupported on your device M...</td>\n",
       "      <td>2019-11-01T14:16:00.000+02:00</td>\n",
       "      <td>bbc.co.uk</td>\n",
       "      <td>(Media, playback, is, unsupported, on, your, d...</td>\n",
       "      <td>Farage</td>\n",
       "      <td>[Plaid Cymru, Brexit Party, the Lib Dems, NHS,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>1faf83bfead571acc2fe725ad5a6d44c9dcee446</td>\n",
       "      <td>brexit</td>\n",
       "      <td>General election 2019: Farage calls on Johnson...</td>\n",
       "      <td>Media playback is unsupported on your device M...</td>\n",
       "      <td>2019-11-01T14:16:00.000+02:00</td>\n",
       "      <td>bbc.co.uk</td>\n",
       "      <td>(Media, playback, is, unsupported, on, your, d...</td>\n",
       "      <td>Nigel</td>\n",
       "      <td>[Plaid Cymru, Brexit Party, the Lib Dems, NHS,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>1faf83bfead571acc2fe725ad5a6d44c9dcee446</td>\n",
       "      <td>brexit</td>\n",
       "      <td>General election 2019: Farage calls on Johnson...</td>\n",
       "      <td>Media playback is unsupported on your device M...</td>\n",
       "      <td>2019-11-01T14:16:00.000+02:00</td>\n",
       "      <td>bbc.co.uk</td>\n",
       "      <td>(Media, playback, is, unsupported, on, your, d...</td>\n",
       "      <td>John Curtice</td>\n",
       "      <td>[Plaid Cymru, Brexit Party, the Lib Dems, NHS,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>1faf83bfead571acc2fe725ad5a6d44c9dcee446</td>\n",
       "      <td>brexit</td>\n",
       "      <td>General election 2019: Farage calls on Johnson...</td>\n",
       "      <td>Media playback is unsupported on your device M...</td>\n",
       "      <td>2019-11-01T14:16:00.000+02:00</td>\n",
       "      <td>bbc.co.uk</td>\n",
       "      <td>(Media, playback, is, unsupported, on, your, d...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>[Plaid Cymru, Brexit Party, the Lib Dems, NHS,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5277 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         uuid   query  \\\n",
       "0    e25fb36d29acc983cefbc8b61328b88dd4f7cffe  brexit   \n",
       "0    e25fb36d29acc983cefbc8b61328b88dd4f7cffe  brexit   \n",
       "0    e25fb36d29acc983cefbc8b61328b88dd4f7cffe  brexit   \n",
       "0    e25fb36d29acc983cefbc8b61328b88dd4f7cffe  brexit   \n",
       "0    e25fb36d29acc983cefbc8b61328b88dd4f7cffe  brexit   \n",
       "..                                        ...     ...   \n",
       "351  1faf83bfead571acc2fe725ad5a6d44c9dcee446  brexit   \n",
       "351  1faf83bfead571acc2fe725ad5a6d44c9dcee446  brexit   \n",
       "351  1faf83bfead571acc2fe725ad5a6d44c9dcee446  brexit   \n",
       "351  1faf83bfead571acc2fe725ad5a6d44c9dcee446  brexit   \n",
       "351  1faf83bfead571acc2fe725ad5a6d44c9dcee446  brexit   \n",
       "\n",
       "                                     thread.title_full  \\\n",
       "0    Have UK voters changed their minds on Brexit? ...   \n",
       "0    Have UK voters changed their minds on Brexit? ...   \n",
       "0    Have UK voters changed their minds on Brexit? ...   \n",
       "0    Have UK voters changed their minds on Brexit? ...   \n",
       "0    Have UK voters changed their minds on Brexit? ...   \n",
       "..                                                 ...   \n",
       "351  General election 2019: Farage calls on Johnson...   \n",
       "351  General election 2019: Farage calls on Johnson...   \n",
       "351  General election 2019: Farage calls on Johnson...   \n",
       "351  General election 2019: Farage calls on Johnson...   \n",
       "351  General election 2019: Farage calls on Johnson...   \n",
       "\n",
       "                                                  text  \\\n",
       "0    Image copyright Getty Images UK Prime Minister...   \n",
       "0    Image copyright Getty Images UK Prime Minister...   \n",
       "0    Image copyright Getty Images UK Prime Minister...   \n",
       "0    Image copyright Getty Images UK Prime Minister...   \n",
       "0    Image copyright Getty Images UK Prime Minister...   \n",
       "..                                                 ...   \n",
       "351  Media playback is unsupported on your device M...   \n",
       "351  Media playback is unsupported on your device M...   \n",
       "351  Media playback is unsupported on your device M...   \n",
       "351  Media playback is unsupported on your device M...   \n",
       "351  Media playback is unsupported on your device M...   \n",
       "\n",
       "                         published thread.site  \\\n",
       "0    2019-10-16T03:00:00.000+03:00   bbc.co.uk   \n",
       "0    2019-10-16T03:00:00.000+03:00   bbc.co.uk   \n",
       "0    2019-10-16T03:00:00.000+03:00   bbc.co.uk   \n",
       "0    2019-10-16T03:00:00.000+03:00   bbc.co.uk   \n",
       "0    2019-10-16T03:00:00.000+03:00   bbc.co.uk   \n",
       "..                             ...         ...   \n",
       "351  2019-11-01T14:16:00.000+02:00   bbc.co.uk   \n",
       "351  2019-11-01T14:16:00.000+02:00   bbc.co.uk   \n",
       "351  2019-11-01T14:16:00.000+02:00   bbc.co.uk   \n",
       "351  2019-11-01T14:16:00.000+02:00   bbc.co.uk   \n",
       "351  2019-11-01T14:16:00.000+02:00   bbc.co.uk   \n",
       "\n",
       "                                              text_nlp   entity_person  \\\n",
       "0    (Image, copyright, Getty, Images, UK, Prime, M...   Boris Johnson   \n",
       "0    (Image, copyright, Getty, Images, UK, Prime, M...         Johnson   \n",
       "0    (Image, copyright, Getty, Images, UK, Prime, M...       Deltapoll   \n",
       "0    (Image, copyright, Getty, Images, UK, Prime, M...          Brexit   \n",
       "0    (Image, copyright, Getty, Images, UK, Prime, M...  Dominic Bailey   \n",
       "..                                                 ...             ...   \n",
       "351  (Media, playback, is, unsupported, on, your, d...      pro-Brexit   \n",
       "351  (Media, playback, is, unsupported, on, your, d...          Farage   \n",
       "351  (Media, playback, is, unsupported, on, your, d...           Nigel   \n",
       "351  (Media, playback, is, unsupported, on, your, d...    John Curtice   \n",
       "351  (Media, playback, is, unsupported, on, your, d...    Donald Trump   \n",
       "\n",
       "                                            entity_org  \n",
       "0    [BMG, 27%-28, the European Union, Leave, Strat...  \n",
       "0    [BMG, 27%-28, the European Union, Leave, Strat...  \n",
       "0    [BMG, 27%-28, the European Union, Leave, Strat...  \n",
       "0    [BMG, 27%-28, the European Union, Leave, Strat...  \n",
       "0    [BMG, 27%-28, the European Union, Leave, Strat...  \n",
       "..                                                 ...  \n",
       "351  [Plaid Cymru, Brexit Party, the Lib Dems, NHS,...  \n",
       "351  [Plaid Cymru, Brexit Party, the Lib Dems, NHS,...  \n",
       "351  [Plaid Cymru, Brexit Party, the Lib Dems, NHS,...  \n",
       "351  [Plaid Cymru, Brexit Party, the Lib Dems, NHS,...  \n",
       "351  [Plaid Cymru, Brexit Party, the Lib Dems, NHS,...  \n",
       "\n",
       "[5277 rows x 9 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we explode the data to transform our single column with rows of lists, to give each item in each list its own row\n",
    "\n",
    "exploded_df = brexit_data.explode('entity_person')\n",
    "exploded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Brexit             228\n",
       "Boris Johnson      177\n",
       "Johnson            158\n",
       "Jeremy Corbyn       81\n",
       "Tory                51\n",
       "Boris Johnson’s     44\n",
       "Boris Johnson's     42\n",
       "John Bercow         41\n",
       "Donald Tusk         36\n",
       "Donald Trump        36\n",
       "Name: entity_person, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can see overall the most mentioned 'persons' in the dataset - note that Spacy think brexit is a person....\n",
    "# this might be an interesting indicator of the way brexit is talked about in the press.\n",
    "\n",
    "exploded_df['entity_person'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Brexit               238\n",
       "Boris Johnson        183\n",
       "Donald Trump         175\n",
       "Johnson              166\n",
       "Trump                149\n",
       "                    ... \n",
       "femenoid               1\n",
       "Rick Rieder            1\n",
       "Michael Dukakis        1\n",
       "Andrew Stephenson      1\n",
       "David Henderson        1\n",
       "Name: entity_person, Length: 6705, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we could see what the OVERALL most mentioned person entity is...\n",
    "\n",
    "news_data.explode('entity_person')['entity_person'].value_counts()\n",
    "\n",
    "\n",
    "# ...but this has issues..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Pandas .groupby (Advanced usage)\n",
    "\n",
    "An issue with the above is that the top people mentioned in the dataset will be greatly determined by the number of stories gathered per query. Let's say our end goal is to find the top 5 persons mentioned, but this time for each query individually.\n",
    "\n",
    "We could come up with some various filters and loops to cut up the dataset and spit out the results, but `.groupby` is designed for just these kinds of cases.\n",
    "\n",
    "Groupby works using the process of...\n",
    "\n",
    "#### SPLIT > APPLY > COMBINE\n",
    "\n",
    "<img src=\"https://github.com/Minyall/sc207_materials/blob/master/images/groupby.png?raw=true\" width=\"600\">\n",
    "\n",
    "- Split the Dataframe into seperate dataframes based on the values in a particular column - usually some sort of category\n",
    "- Apply a method or a function to each dataframe\n",
    "- Combine those dataframes back into a single dataframe and return it...\n",
    "\n",
    "\n",
    "First we explode the dataframe on the entity_column\n",
    "\n",
    "`news_data.explode('entity_person')`\n",
    "\n",
    "This will give us a dataframe where we convert our lists of persons in each row, to individual rows per person per article, as we did above. Then we move on to the groupby process.\n",
    "\n",
    "The next step we `.groupby` the 'query' column, this means we have an object that is in effect 7 different dataframes, one for each query (The Split stage). This creates a `DataFrameGroupBy` object. We can't examine its contents yet because we haven't applied any transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_data = news_data.explode('entity_person')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.core.groupby.generic.DataFrameGroupBy object at 0x140038400>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupby_query = exploded_data.groupby('query')\n",
    "groupby_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has applied a transformation to the groupby object, and it has returned us a single object of all 7 transformed dataframes combined.\n",
    "It has split into multiple dataframe based on our groupby command.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every command we give to this object will be applied to each of the 7 dataframes at the same time.\n",
    "If we try .value_counts() it will give us the value_counts() for the entity_person column, for each dataframe seperately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "query           entity_person\n",
       "Hong Kong       Donald Trump     26\n",
       "                Carrie Lam       22\n",
       "                Trump            16\n",
       "                Xi Jinping       12\n",
       "                LeBron James     10\n",
       "                                 ..\n",
       "cryptocurrency  Xi                1\n",
       "                Yang              1\n",
       "                Zuck              1\n",
       "                drukken Foto      1\n",
       "                ’m                1\n",
       "Name: entity_person, Length: 7485, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupby_query['entity_person'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the value counts of how many times each PERSON entity was mentioned within each of the seperate query dataframes. However because it is returning the entire set of results for each dataframe, we can't see much. Currently we're just looking at the top and bottom of the entire set of results. The very top of the first dataframe, and the very end of the bottom dataframe, and everything in between is hidden because there is too much data.\n",
    "\n",
    "HOWEVER!\n",
    "If we take this result, and group it AGAIN by the 'query' column, we will be cutting up this set of results into indivdual dataframes again. We can then just ask for the top 5 results from each, by using `.head(5)`.\n",
    "\n",
    "This will take the top 5 from each dataframe, and then combine them into a single dataframe and pass it back to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "query           entity_person                \n",
       "Hong Kong       Donald Trump                      26\n",
       "                Carrie Lam                        22\n",
       "                Trump                             16\n",
       "                Xi Jinping                        12\n",
       "                LeBron James                      10\n",
       "Tesla           Tesla                             27\n",
       "                Elon Musk                         24\n",
       "                Musk                              15\n",
       "                Brexit                             6\n",
       "                Gigafactory                        5\n",
       "alt-right       Trump                             14\n",
       "                Donald Trump                       9\n",
       "                Richard Spencer                    9\n",
       "                Breitbart                          7\n",
       "                Donald Trump Jr.                   6\n",
       "billionaire     Donald Trump                      97\n",
       "                Trump                             88\n",
       "                David Cay Johnston’s DCReport     73\n",
       "                Unhinged                          73\n",
       "                Elizabeth Warren                  60\n",
       "bitcoin         Jong Woo Son                       4\n",
       "                Bitcoin                            3\n",
       "                David Marcus                       3\n",
       "                Mark Zuckerberg                    3\n",
       "                Bloomberg                          2\n",
       "brexit          Brexit                           228\n",
       "                Boris Johnson                    177\n",
       "                Johnson                          158\n",
       "                Jeremy Corbyn                     81\n",
       "                Tory                              51\n",
       "cryptocurrency  Mark Zuckerberg                   13\n",
       "                David Marcus                       5\n",
       "                Alexandria Ocasio-Cortez           4\n",
       "                Elizabeth Warren                   4\n",
       "                Zuckerberg                         4\n",
       "Name: entity_person, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupby_query['entity_person'].value_counts().groupby('query').head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make life easier we can always create a function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_entity(data, entity_col, groupby_col, top_n=5):\n",
    "    exploded = data.explode(entity_col)\n",
    "    g = exploded.groupby(groupby_col)\n",
    "    result = g[entity_col].value_counts().groupby(groupby_col).head(top_n)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_entity(data=news_data, entity_col='entity_person', groupby_col='query', top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_entity(data=news_data, entity_col='entity_org', groupby_col='query', top_n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Entity Lists to Entity Networks\n",
    "\n",
    "One analysis approach we can use is to build a network where each node is the name of an entity, and an edge exists between them if they co-occur in the same piece of text. For every co-occurence their edge is weighted +1.\n",
    "\n",
    "This is relatively easy to do with a few tricks in Pandas and Networkx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an edge list where the source is just an id number for the article and the target is the name in the article.\n",
    "\n",
    "# by exploding the entity column we get a dataframe where each row is repeated for every item in the entity list.\n",
    "exploded = news_data.explode('entity_person')\n",
    "\n",
    "# we take a copy of just two columns, the newly exploded 'entity_person' column, and the uuid column which is a unique id\n",
    "# assigned to each article\n",
    "\n",
    "edge_list = exploded[['uuid', 'entity_person']].copy()\n",
    "\n",
    "#then we just rename the appropriate columns\n",
    "edge_list = edge_list.rename(columns={'uuid':'source','entity_person':'target'})\n",
    "edge_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our graph from the edge list\n",
    "\n",
    "G = nx.from_pandas_edgelist(edge_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have our graph which consists of nodes representing unique articles, and nodes representing people, with edges going between the articles and people if a person is mentioned in the article.\n",
    "\n",
    "We could examine this to see the result..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gexf(G, 'article_person.gexf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bipartite Graphs\n",
    "\n",
    "Our graph is a bi-partite graph.\n",
    "\n",
    "Bipartite graphs are networks where there are two types of nodes and where there are only edges between nodes of different types, never edges between nodes of the same type.\n",
    "\n",
    "In our graph we have nodes representing articles, and nodes representing people. Currently there are only edges between people and the articles that they appear in, not edges between people, and not edges between articles.\n",
    "\n",
    "##### Bi-Partite Projection\n",
    "<img src=\"https://github.com/Minyall/sc207_materials/blob/master/images/bipartite.png?raw=true\" align=\"right\" width=\"300\">\n",
    "Projection can be understood as the process of removing the 'intermediaries' between nodes, and connecting those nodes together directly. In the example image, Nodes A, B and C are not directly connected, but they do share intermediary nodes, 1, 2 and 3. The simple graph just represents that there is some form of connection, the multigraph demonstrates that...\n",
    "\n",
    "- there are two shared connections between A and B (intermediary nodes 1 & 2)\n",
    "- there are two shared connections between B and C (intermediaries, 2 and 3).\n",
    "- There is only one connection between A and C (intermediary node 2).\n",
    "\n",
    "\n",
    "Image from https://arxiv.org/pdf/1909.10977.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now if we check we can see our graph is a \"bipartite graph\"\n",
    "\n",
    "nx.bipartite.is_bipartite(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms we can can use projection techniques on this graph.\n",
    "We can use the networkx function `nx.bipartite.weighted_projected_graph` to create our projection.\n",
    "\n",
    "This function requires\n",
    "\n",
    "- The graph you want to project from\n",
    "- A list of the nodes you want to retain - in our case our list of people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can create a list of unique nodes by taking out 'target' column of our edge \n",
    "# list (which remember was our list of PERSON entities) and using .unique()\n",
    "\n",
    "nodes_to_keep = edge_list['target'].unique()\n",
    "nodes_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create our final graph\n",
    "\n",
    "projected_G = nx.bipartite.weighted_projected_graph(G,nodes=nodes_to_keep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a graph of just PERSON entity nodes, with edges weighted by the number of times those PERSON entities co-occurred in an article. It is likely that the majority of edges will have a weight of 1. We're interested to see if entities co-occur a lot so we could filter out these edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current number of edges\n",
    "projected_G.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets identify all the edges that have a weight greater than 1\n",
    "# we'll use this to filter our graph\n",
    "\n",
    "edges_to_keep = []\n",
    "\n",
    "# the if we iterate over a graph using the .edges() method it gives us two values\n",
    "# the source and the target of the edge.\n",
    "\n",
    "# for source, target in projected_G.edges():\n",
    "#     Do something in this loop\n",
    "#\n",
    "#\n",
    "#\n",
    "\n",
    "# if we give .edges the argument data=True, it also gives us the attributes of each edge as a dictionary\n",
    "\n",
    "for source, target, edge_attributes in projected_G.edges(data=True):\n",
    "    if edge_attributes['weight'] > 1:\n",
    "        edge = (source, target)\n",
    "        edges_to_keep.append(edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is a quick view of some of our edges\n",
    "edges_to_keep[1050:1100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(edges_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the adventurous we could have achieved the same as above in a list comprehension,\n",
    "# though it is not as readable\n",
    "\n",
    "edges_to_keep = [(source,target) for source,target, edge_attributes in projected_G.edges(data=True) if edge_attributes['weight']>1]\n",
    "len(edges_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can easily filter the graph using the nx.edge_subgraph function\n",
    "# this function requires the graph to be filtered, and a list of edges to keep.\n",
    "\n",
    "filtered_G = nx.edge_subgraph(projected_G,edges_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_original_edges = projected_G.number_of_edges()\n",
    "n_original_nodes = projected_G.number_of_nodes()\n",
    "\n",
    "n_new_edges = filtered_G.number_of_edges()\n",
    "n_new_nodes = filtered_G.number_of_nodes()\n",
    "\n",
    "def pct_decrease(original,new):\n",
    "    decrease = original - new\n",
    "    return round((decrease/original)*100,2)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Original Graph - Nodes: {n_original_nodes}\")\n",
    "print(f\"Original Graph - Edges: {n_original_edges}\")\n",
    "print(f\"New Graph - Nodes: {n_new_nodes}\")\n",
    "print(f\"New Graph - Edges: {n_new_edges}\")\n",
    "\n",
    "print(f\"Graph Nodes decreased by: {pct_decrease(n_original_nodes, n_new_nodes)}%\")\n",
    "print(f\"Graph Edges decreased by: {pct_decrease(n_original_edges, n_new_edges)}%\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the filtered graph to a gexf and take a look in Gephi\n",
    "\n",
    "nx.write_gexf(filtered_G, 'entity_network.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:teaching]",
   "language": "python",
   "name": "conda-env-teaching-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
