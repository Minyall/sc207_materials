{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Text Mining\n",
    "## 2. Pre-Processing Documents to be Data\n",
    "- A key part of text analysis is preparing your text for different kinds of analysis.\n",
    "\n",
    "- Different types of analysis require different types of preparation but usually two steps are fundamental. \n",
    "    - Tokenizing\n",
    "    - Filtering.\n",
    "\n",
    "\n",
    "All the processes that we engaged with in Part 1 would have used some sort of Tokenizing in order to understand the underling text. When we prepare text to use other kinds of analysis we also need to tokenize appropriately.\n",
    "\n",
    "\n",
    "### a) Tokenizing\n",
    "The process of splitting up the text into a list of individual words that can be treated as individual units.\n",
    "\n",
    "How exactly you split up text is not necessarily straight forward and there are a range of different strategies. The correct one to use varies depending on the type of text you are using and the type of analysis that you want to do.\n",
    "\n",
    "To see how different tokenizer strategies interpret a piece of text you can check out the [Python NLTK Demo Page](https://text-processing.com/demo/tokenize/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing: Example of the problem\n",
    "We already know how to split up a string into a series of items in a list. It's pretty simple using `.split()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_phrase = \"I don't see my cat. He has a long tail, fluffy ears and big eyes!\"\\\n",
    "\" He also subscribes to Marxist historical materialism. It's just his way.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " \"don't\",\n",
       " 'see',\n",
       " 'my',\n",
       " 'cat.',\n",
       " 'He',\n",
       " 'has',\n",
       " 'a',\n",
       " 'long',\n",
       " 'tail,',\n",
       " 'fluffy',\n",
       " 'ears',\n",
       " 'and',\n",
       " 'big',\n",
       " 'eyes!',\n",
       " 'He',\n",
       " 'also',\n",
       " 'subscribes',\n",
       " 'to',\n",
       " 'Marxist',\n",
       " 'historical',\n",
       " 'materialism.',\n",
       " \"It's\",\n",
       " 'just',\n",
       " 'his',\n",
       " 'way.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tokens = test_phrase.split()\n",
    "test_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first we check if the string 'ears' is in the list\n",
    "'ears' in test_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so we would imagine that 'eyes' is also in the list?\n",
    "'eyes' in test_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having punctuation atached to words like this can cause us problems because the tokens `eyes` and `eyes!` would be considered two seperate things. This messes with a lot of analysis further down the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider this tweet\n",
    "\n",
    "test_tweet = \"How will #Brexit affect #customsdeclarations? Check out the @britishchambers\"\\\n",
    "\" guide to find out! https://t.co/CpoGudZAcb https://t.co/Pr4RW4Tyhw\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How',\n",
       " 'will',\n",
       " '#Brexit',\n",
       " 'affect',\n",
       " '#customsdeclarations?',\n",
       " 'Check',\n",
       " 'out',\n",
       " 'the',\n",
       " '@britishchambers',\n",
       " 'guide',\n",
       " 'to',\n",
       " 'find',\n",
       " 'out!',\n",
       " 'https://t.co/CpoGudZAcb',\n",
       " 'https://t.co/Pr4RW4Tyhw']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if we split it we get\n",
    "test_tweet.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweets pose further challenges because because of the unusual use of language that is common within the domain of twitter and other social media (hashtags)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing: Using Tokenizers\n",
    "Tokenizers are functions that split up text for us. Some of them are based on complex sets of rules, others are based on training computers using lots of examples of text and the ideal way to split them.\n",
    "\n",
    "There are many (many!) packages available for handling text data. These include..\n",
    "\n",
    "- [The Natural Language Toolkit (NLTK)](https://www.nltk.org/): Very well established package. More of a focus on linguistics. Can do most tasks but a bit of a steep learning curve compared to TextBlob. People often dip into its toolset whilst using other packages.\n",
    "- [TextBlob](https://textblob.readthedocs.io/en/dev/): Good for beginners but lacks some features. Built on top of NLTK but doesn't have all of NLTK's functionality.\n",
    "- [Gensim](https://radimrehurek.com/gensim/): Good to handle very large amounts of text (100,000's). However to achieve this it relies on fairly high level concepts in Python, making its approach a little tricky to get a grip on. \n",
    "- [SpaCy](https://spacy.io/): Relatively new. Utilises a lot of language models that are pre-trained on very large datasets of text. Incredibly fast with industry level complex tools. Very flexible if used correctly and has a relatively accesible set of tools once you understand the SpaCy approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I don't see my cat. He has a long tail, fluffy ears and big eyes! He also subscribes to Marxist historical materialism. It's just his way."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In SpaCy tokenization happens the moment you wrap a string in your language model object nlp()\n",
    "\n",
    "doc = nlp(test_phrase)\n",
    "\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "do\n",
      "n't\n",
      "see\n",
      "my\n",
      "cat\n",
      ".\n",
      "He\n",
      "has\n",
      "a\n",
      "long\n",
      "tail\n",
      ",\n",
      "fluffy\n",
      "ears\n",
      "and\n",
      "big\n",
      "eyes\n",
      "!\n",
      "He\n",
      "also\n",
      "subscribes\n",
      "to\n",
      "Marxist\n",
      "historical\n",
      "materialism\n",
      ".\n",
      "It\n",
      "'s\n",
      "just\n",
      "his\n",
      "way\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# if we iterate over the doc we can see the tokens.\n",
    "for word in doc:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a list of tokens we can do one of two things...\n",
    "\n",
    "- a) Use a `for loop`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_a = []\n",
    "\n",
    "for word in doc:\n",
    "    tokens_a.append(word)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I, do, n't, see, my, cat, ., He, has, a, long, tail, ,, fluffy, ears, and, big, eyes, !, He, also, subscribes, to, Marxist, historical, materialism, ., It, 's, just, his, way, .]\n"
     ]
    }
   ],
   "source": [
    "print(tokens_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- b) Use a *List Comprehension*\n",
    "\n",
    "List comprehensions allow us to do in 1 line what would normally take 3. We'll see how they can be used more later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_b = [word for word in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I, do, n't, see, my, cat, ., He, has, a, long, tail, ,, fluffy, ears, and, big, eyes, !, He, also, subscribes, to, Marxist, historical, materialism, ., It, 's, just, his, way, .]\n"
     ]
    }
   ],
   "source": [
    "print(tokens_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing: Understanding Tokenization\n",
    "A number of things have been done by the tokenizer.\n",
    "- Punctuation has been seperated from words into their own tokens.\n",
    "- Words that are contractions of two words (It's > It is / Don't > Do not) have been split into two.\n",
    "- This becomes more useful in a minute and is all part of the process of reducing the nuance of language to make documents more comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"I don't like rabbits in space\",\n",
    "         \"I do not like rabbits in space\",\n",
    "         \"I'm loving these rabbits\",\n",
    "         \"I love this rabbit\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Language.pipe at 0x7fa3ed9d2e50>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remember we can use nlp.pipe to convert a list of documents quicker than iterating over the list one at a time.\n",
    "\n",
    "docs = nlp.pipe(texts, n_process=-1)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[I don't like rabbits in space,\n",
       " I do not like rabbits in space,\n",
       " I'm loving these rabbits,\n",
       " I love this rabbit]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can force the generator to produce the results by wrapping it in a list\n",
    "docs = list(docs)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n",
      "[I, do, n't, like, rabbits, in, space]\n"
     ]
    }
   ],
   "source": [
    "# each of these list items is now a SpaCy document...\n",
    "\n",
    "one_doc = docs[0]\n",
    "print(one_doc.lang_)\n",
    "\n",
    "one_doc_tokens = [word for word in one_doc]\n",
    "print(one_doc_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Filtering\n",
    "Often in text analysis, there is a lot of material that is useful to humans but less useful to computers when performing analysis. Language is very nuanced in real life, but part of the filtering process involves reducing that nuance to strip back to a piece of text's bare bones.\n",
    "\n",
    "For example, how different would we consider these two sentences...?\n",
    "\n",
    "```\n",
    "\"I don't like rabbits in space\"\n",
    "\"I do not like rabbits in space\"\n",
    "```\n",
    "Semantically they are the same, computationally they are different.\n",
    "```\n",
    "\"I am loving these rabbits\"\n",
    "\"I love this rabbit\"\n",
    "```\n",
    "Semantically a little different but still similar. Computationally very different as they only share one word, 'I'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SpaCy Tokens\n",
    "As we saw above, once a string is wrapped in a nlp model, it becomes a SpaCy [Document object](https://spacy.io/api/doc) giving access to a range of methods. The SpaCy document object, tokenizes our string meaning that the Document object is also a list of SpaCy [Token objects](https://spacy.io/api/token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't like rabbits in space\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "rabbits\n",
      "<class 'spacy.tokens.token.Token'>\n"
     ]
    }
   ],
   "source": [
    "# and to reiterate, a spacy document is a collection of spacy tokens\n",
    "print(one_doc)\n",
    "print(type(one_doc))\n",
    "\n",
    "print(one_doc_tokens[4])\n",
    "print(type(one_doc_tokens[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SpaCy Tokens: Lemmatization\n",
    "\n",
    "This is the process of reducing a word down to its 'root' form whilst still retaining the meaning. By reducing to the root of a word, you reduce the variation of words used and increase the chances that semantically similar phrases have the same tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-PRON-', 'do', 'not', 'like', 'rabbit', 'in', 'space']\n",
      "['-PRON-', 'do', 'not', 'like', 'rabbit', 'in', 'space']\n",
      "['-PRON-', 'be', 'love', 'these', 'rabbit']\n",
      "['-PRON-', 'love', 'this', 'rabbit']\n"
     ]
    }
   ],
   "source": [
    "texts = [\"I don't like rabbits in space\",\n",
    "         \"I do not like rabbits in space\",\n",
    "         \"I'm loving these rabbits\",\n",
    "         \"I love this rabbit\"]\n",
    "\n",
    "docs = list(nlp.pipe(texts))\n",
    "\n",
    "for doc in docs:\n",
    "    print([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### '-PRON-'\n",
    "In SpaCy the `-PRON-` is a stand-in for any pronoun. From [the documentation](https://spacy.io/api/annotation).\n",
    "\n",
    "\n",
    "##### About spaCy's custom pronoun lemma for English\n",
    "\n",
    "*spaCy adds a special case for English pronouns: all English pronouns are lemmatized to the special token -PRON-. Unlike verbs and common nouns, there’s no clear base form of a personal pronoun. Should the lemma of “me” be “I”, or should we normalize person as well, giving “it” — or maybe “he”? spaCy’s solution is to introduce a novel symbol, -PRON-, which is used as the lemma for all personal pronouns.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SpaCy Tokens: Stop Words\n",
    "\n",
    "\"Stop\" words are words within a language that provide structure to the language but are often do not convey a lot of information in themselves. Examples include...\n",
    "- the\n",
    "- and\n",
    "- it\n",
    "\n",
    "Normally for analysis processes beyond what SpaCy provides in terms of document level analysis we would want to filter out these stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can load a list of stopwwords from spacy\n",
    "stopwords = stopwords = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't see my cat. He has a long tail, fluffy ears and big eyes! He also subscribes to Marxist historical materialism. It's just his way.\n"
     ]
    }
   ],
   "source": [
    "nlp_phrase = nlp(test_phrase)\n",
    "print(nlp_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and\n",
      "True\n",
      "cat\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "stop_word = nlp_phrase[15].text\n",
    "not_stop_word = nlp_phrase[5].text\n",
    "\n",
    "print(stop_word)\n",
    "print(stop_word in stopwords)\n",
    "\n",
    "print(not_stop_word)\n",
    "print(not_stop_word in stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I : True\n",
      "do : True\n",
      "n't : True\n",
      "see : True\n",
      "my : True\n",
      "cat : False\n",
      ". : False\n",
      "He : True\n",
      "has : True\n",
      "a : True\n",
      "long : False\n",
      "tail : False\n",
      ", : False\n",
      "fluffy : False\n",
      "ears : False\n",
      "and : True\n",
      "big : False\n",
      "eyes : False\n",
      "! : False\n",
      "He : True\n",
      "also : True\n",
      "subscribes : False\n",
      "to : True\n",
      "Marxist : False\n",
      "historical : False\n",
      "materialism : False\n",
      ". : False\n",
      "It : True\n",
      "'s : True\n",
      "just : True\n",
      "his : True\n",
      "way : False\n",
      ". : False\n"
     ]
    }
   ],
   "source": [
    "for word in nlp_phrase:\n",
    "    print(f\"{word} : {word.text.lower() in stopwords}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'do',\n",
       " 'not',\n",
       " 'see',\n",
       " 'my',\n",
       " 'cat',\n",
       " '.',\n",
       " 'He',\n",
       " 'have',\n",
       " 'a',\n",
       " 'long',\n",
       " 'tail',\n",
       " ',',\n",
       " 'fluffy',\n",
       " 'ear',\n",
       " 'and',\n",
       " 'big',\n",
       " 'eye',\n",
       " '!',\n",
       " 'He',\n",
       " 'also',\n",
       " 'subscribe',\n",
       " 'to',\n",
       " 'marxist',\n",
       " 'historical',\n",
       " 'materialism',\n",
       " '.',\n",
       " 'It',\n",
       " 'be',\n",
       " 'just',\n",
       " 'his',\n",
       " 'way',\n",
       " '.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first we lemmatize\n",
    "\n",
    "lemmas = [word.lemma_ if word.lemma_ != '-PRON-'\n",
    "          else word.text\n",
    "          for word in nlp_phrase]\n",
    "lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat',\n",
       " '.',\n",
       " 'long',\n",
       " 'tail',\n",
       " ',',\n",
       " 'fluffy',\n",
       " 'ear',\n",
       " 'big',\n",
       " 'eye',\n",
       " '!',\n",
       " 'subscribe',\n",
       " 'marxist',\n",
       " 'historical',\n",
       " 'materialism',\n",
       " '.',\n",
       " 'way',\n",
       " '.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can then filter out the stopwords like so\n",
    "\n",
    "lemmas_filtered = [word for word in lemmas if word.lower() not in stopwords]\n",
    "lemmas_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat',\n",
       " 'long',\n",
       " 'tail',\n",
       " 'fluffy',\n",
       " 'ear',\n",
       " 'big',\n",
       " 'eye',\n",
       " 'subscribe',\n",
       " 'marxist',\n",
       " 'historical',\n",
       " 'materialism',\n",
       " 'way']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can also filter for only alphabetical tokens (not punctuation) using .is_alpha\n",
    "\n",
    "nlp_phrase_filtered = [word for word in lemmas_filtered if word.isalpha()]\n",
    "nlp_phrase_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['do',\n",
       " 'not',\n",
       " 'see',\n",
       " 'cat',\n",
       " 'have',\n",
       " 'a',\n",
       " 'long',\n",
       " 'tail',\n",
       " 'fluffy',\n",
       " 'ear',\n",
       " 'and',\n",
       " 'big',\n",
       " 'eye',\n",
       " 'also',\n",
       " 'subscribe',\n",
       " 'to',\n",
       " 'marxist',\n",
       " 'historical',\n",
       " 'materialism',\n",
       " 'be',\n",
       " 'just',\n",
       " 'way']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if we put all our filters together...\n",
    "\n",
    "tokens = [word.lemma_ for word in nlp_phrase]\n",
    "tokens = [word for word in tokens if word.isalpha()]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_text(spacy_doc, lower=True):\n",
    "    tokens = [word.lemma_ for word in spacy_doc]\n",
    "    tokens = [word for word in tokens if word.lower() not in stopwords]\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    if lower:\n",
    "        tokens = [word.lower() for word in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat',\n",
       " 'long',\n",
       " 'tail',\n",
       " 'fluffy',\n",
       " 'ear',\n",
       " 'big',\n",
       " 'eye',\n",
       " 'subscribe',\n",
       " 'marxist',\n",
       " 'historical',\n",
       " 'materialism',\n",
       " 'way']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_phrase = \"I don't see my cat. He has a long tail, fluffy ears and big eyes!\"\\\n",
    "\" He also subscribes to Marxist historical materialism. It's just his way.\"\n",
    "doc = nlp(test_phrase)\n",
    "filter_text(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning our News Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('news_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_nlp'] = list(nlp.pipe(df['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_tokens'] = df['text_nlp'].apply(filter_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [image, copyright, getty, images, uk, prime, m...\n",
       "1      [stock, slightly, investor, consider, mixed, b...\n",
       "2      [image, copyright, getty, images, key, brexit,...\n",
       "3      [send, load, share, option, new, ira, break, l...\n",
       "4      [issue, irish, border, handle, flow, good, peo...\n",
       "                             ...                        \n",
       "967    [victoria, police, denounce, inappropriate, me...\n",
       "968    [photo, old, canadian, drag, queen, pose, half...\n",
       "969    [ex, breitbart, writer, milo, yiannopoulos, re...\n",
       "970    [brexit, party, leader, continue, worry, longe...\n",
       "971    [screenshot, minor, task, everyday, task, impo...\n",
       "Name: cleaned_tokens, Length: 972, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleaned_tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the original and the cleaned versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image copyright Getty Images UK Prime Minister Boris Johnson hopes to persuade MPs to back a deal to take the UK out of the EU.\n",
      "Doing so would implement the result of the referendum of June 2016, in which 52% of voters backed Leave and 48% Remain.\n",
      "But where do voters stand on Brexit now, after more than three years of debate and negotiation?\n",
      "There is no majority for any course of action First, no single course of action is preferred by a majority of voters.\n",
      "For example, polling firm Kantar has asked voters on a number of occasions which of four possible outcomes they prefer.\n",
      "The most popular choice has been to remain in the EU. However, this secured the support of only about one in three.\n",
      "The next most popular, leaving without a deal, is preferred by slightly less than a quarter.\n",
      "Much the same picture has been painted by another survey. BMG asked people which of five alternatives they would prefer if a deal is not agreed by the end of this month. None has come even close to being backed by more than half of voters.\n",
      "Should no agreement be reached, the single most popular option is to leave the EU without a deal. Even so, it is still only backed by about one in three.\n",
      "Both of the next most popular options - holding another referendum and reversing Brexit without a referendum, are only chosen by about one in five.\n",
      "Three polls, by Opinium, Panelbase and ComRes have asked people what they thought of proposals for a deal put forward by Mr Johnson .\n",
      "All three found that slightly more voters were in favour of them than against. However, they were still backed by well under half.\n",
      "In Opinium's poll, just 27% thought the proposals would represent a good deal, while 22% reckoned they would represent a bad one.\n",
      "Brexit deal 'still possible, but more difficult' A simple guide to Brexit What is 'no-deal Brexit'? Most people either said it would neither be good nor bad, or that they did not know.\n",
      "Both Panelbase and ComRes found that 31%-32% support the proposals, while 27%-28% oppose them. But in both cases 41% said they did not know.\n",
      "Against this backdrop, what voters will make of any compromise deal that Mr Johnson might strike with the EU is far from clear.\n",
      "Leave and Remain voters hold very different views Second, those who voted Remain and those who backed Leave have very different preferences.\n",
      "The single most popular option among Leave voters is to exit the EU without a deal. According to Kantar, at least half of them prefer that course of action.\n",
      "Only about three in 10 pick either of the deals put before them by Kantar: the agreement Mrs May negotiated with the EU, or a \"soft\" Brexit under which the UK will still be part of the single market and customs union.\n",
      "Meanwhile, in the event of no deal, on average nearly seven in 10 Leave voters tell BMG they back leaving without one.\n",
      "In contrast, most of those who voted Remain believe that Brexit should be reversed. On average two in three of them tell Kantar they think Article 50 should be revoked.\n",
      "BMG offered its respondents both the possibility of holding another referendum and of reversing Brexit without a ballot.\n",
      "On average, nearly four in 10 Remain voters say Brexit should simply be reversed, while about three in 10 opt for another vote.\n",
      "Few voters have changed their minds Third, very few voters on either side of the argument have changed their minds about whether the UK should leave the EU. The country appears to be just as divided as it was three years ago.\n",
      "On average, during the last month, polls that ask people how they would vote in another referendum suggest that 88% of those who backed Remain would do so again. Among those who voted Leave, 86% have not changed their minds.\n",
      "These figures have changed very little during the last two years.\n",
      "True, most polls suggest - and have done so for some time - that the balance of opinion might be tilted narrowly in favour of remaining a member of the EU. On average, this is by 53% to 47%.\n",
      "However, this lead for Remain rests primarily on the views expressed by those who did not vote three years ago - and perhaps might not do so again.\n",
      "In truth, nobody can be sure what would happen if there were to be another referendum.\n",
      "More like this\n",
      "Do voters support a no-deal Brexit? Who would win if a general election were held now? What do voters make of Brexit now? What the EU elections tell us about support for Brexit Voters are split on holding another referendum Fourth, voters are divided about whether any agreement that might be reached with the EU should be put to a referendum.\n",
      "The balance of opinion differs from poll to poll.\n",
      "When people are asked about a \"public vote\" they are more likely to show support for another ballot than when asked about a \"referendum\" on the UK's membership of the EU.\n",
      "But they all agree that those who voted Remain are much keener on another vote than those who backed Leave.\n",
      "All five of the polls put support for another ballot among Remain voters at over two-thirds.\n",
      "In contrast, four of them find that fewer than 20% of Leave supporters are in favour of the idea. The fifth, by Kantar, puts it only somewhat higher, at 37%.\n",
      "So those who voted Remain are much more likely than those who voted Leave to welcome a ballot that might overturn the result of three years ago.\n",
      "Whatever the outcome this week, the division between Remainers and Leavers does not look as though it is going to be easy to heal.\n",
      "About this piece\n",
      "This analysis piece was commissioned by the BBC from an expert working for an outside organisation .\n",
      "Further details of the research on which it is based are available here .\n",
      "Sir John Curtice is professor of politics, Strathclyde University, and senior research fellow at NatCen Social Research and The UK in a Changing Europe .\n",
      "*Full wording of questions for the chart \"attitudes towards a second referendum\". Kantar: Should the final deal/agreement reached by the government be put to a public vote?; YouGov: Would you support or oppose a public vote on Brexit?; Deltapoll: Would you support or oppose a second referendum on British membership of the European Union?; Panelbase: Do you think there should be a new referendum on Brexit?; BMG: To what extent do you support or oppose [holding] a second in-out EU referendum?\n",
      "Edited by Duncan Walker\n",
      "Charts by David Brown and Dominic Bailey\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[0,'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image copyright getty images uk prime minister boris johnson hope persuade mps deal uk eu implement result referendum june voter leave remain voter stand brexit year debate negotiation majority course action single course action prefer majority voter example polling firm kantar ask voter number occasion possible outcome prefer popular choice remain eu secure support popular leave deal prefer slightly quarter picture paint survey bmg ask people alternative prefer deal agree end month come close half voter agreement reach single popular option leave eu deal popular option hold referendum reverse brexit referendum choose poll opinium panelbase comres ask people think proposal deal forward mr johnson find slightly voter favour half opinium poll think proposal represent good deal reckon represent bad brexit deal possible difficult simple guide brexit deal brexit people good bad know panelbase comres find support proposal oppose case know backdrop voter compromise deal mr johnson strike eu far clear leave remain voter hold different view second vote remain leave different preference single popular option leave voter exit eu deal accord kantar half prefer course action pick deal kantar agreement mrs negotiate eu soft brexit uk single market custom union event deal average nearly seven leave voter tell bmg leave contrast vote remain believe brexit reverse average tell kantar think article revoke bmg offer respondent possibility hold referendum reverse brexit ballot average nearly remain voter brexit simply reverse opt vote voter change mind voter argument change mind uk leave eu country appear divide year ago average month poll ask people vote referendum suggest remain vote leave change mind figure change little year true poll suggest time balance opinion tilt narrowly favour remain member eu average lead remain rest primarily view express vote year ago truth sure happen referendum like voter support deal brexit win general election hold voter brexit eu election tell support brexit voters split hold referendum fourth voter divide agreement reach eu referendum balance opinion differ poll poll people ask public vote likely support ballot ask referendum uk membership eu agree vote remain keen vote leave poll support ballot remain voter contrast find leave supporter favour idea fifth kantar somewhat high vote remain likely vote leave welcome ballot overturn result year ago outcome week division remainers leavers look easy heal piece analysis piece commission bbc expert work outside organisation detail research base available sir john curtice professor politic strathclyde university senior research fellow natcen social research uk changing europe wording question chart attitude second referendum kantar final deal agreement reach government public vote yougov support oppose public vote brexit deltapoll support oppose second referendum british membership european union panelbase think new referendum brexit bmg extent support oppose hold second eu referendum edit duncan walker charts david brown dominic bailey\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(df.loc[0,'cleaned_tokens']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use this technique to clean our text in the next sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:teaching]",
   "language": "python",
   "name": "conda-env-teaching-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
