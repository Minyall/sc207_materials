{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 11 - Text Mining\n",
    "## Introduction to Text as Data\n",
    "\n",
    "- We broadly understand how text might be used as data for qualitative analysis.\n",
    "- Words are not treated simply as individual units of data, but we recognise context, structure, pattern.\n",
    "- How then can text be analysed quantitatively, and how can it be interpreted by a computer to aide that analysis?\n",
    "- Usually text analysis techniques require text to be prepared for analysis through two stages\n",
    "\n",
    "### 1. Tokenizing\n",
    "\n",
    "- Computers break down text into units of analysis known as *Tokens*. Tokens are often individual words, but they can also be parts of words, common phrases etc. \n",
    "- Tokenizing is the first fundamental step in any text analysis.\n",
    "- How exactly you split up text is not necessarily straight forward.\n",
    "- There are a range of different strategies which you can see at the [Python NLTK Demo Page](https://text-processing.com/demo/tokenize/).\n",
    "\n",
    "### 2. Pre-Processing\n",
    "- Exactly what happens in pre-processing tends to depend on the type of analysis you are doing.\n",
    "- In general it tends to involve...\n",
    "        - Filtering out of common words and punctuation\n",
    "        - Standardising the text to make it less complex for the computer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of the problem\n",
    "We already know how to split up a string into a series of items in a list. It's pretty simple using `.split()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_phrase = \"I don't see my cat. He has a long tail, fluffy ears and big eyes!\"\\\n",
    "\" He also subscribes to Marxist historical materialism. It's just his way.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"don't\", 'see', 'my', 'cat.', 'He', 'has', 'a', 'long', 'tail,', 'fluffy', 'ears', 'and', 'big', 'eyes!', 'He', 'also', 'subscribes', 'to', 'Marxist', 'historical', 'materialism.', \"It's\", 'just', 'his', 'way.']\n"
     ]
    }
   ],
   "source": [
    "test_tokens = test_phrase.split()\n",
    "print(test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first we check if the string 'ears' is in the list\n",
    "'ears' in test_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so we would imagine that 'eyes' is also in the list?\n",
    "'eyes' in test_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having punctuation atached to words like this can cause us problems because the tokens `eyes` and `eyes!` would be considered two seperate things. This messes with a lot of analysis further down the line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Tokenizers\n",
    "- Tokenizers are functions that split up text for us. \n",
    "- Some of them are based on complex sets of rules, others are based on training computers using lots of examples of text.\n",
    "- There are many (many!) packages available for handling text data. See Moodle for a list of common ones.\n",
    "\n",
    "#### SpaCy\n",
    "\n",
    "SpaCy uses pre-trained models of language to do a lot of the tasks we need. To create our spacy text tool we need to load in a model. SpaCy has a [load of different models](https://spacy.io/usage/models) for different languages and different types of task.\n",
    "\n",
    "We're going to use `en_core_web_md` which means..\n",
    "- en: English\n",
    "- core: Can perform the core features of Spacy but not some of the more specialised techniques.\n",
    "- web: trained on content from the web such as blogs, news, comments, making it suitable for similar content.\n",
    "- md: medium version. There is also the small and large models. Small is trained just on web text data from 2013. Medium is trained on [petabytes of data from the contemporary internet](https://commoncrawl.org/big-picture/) and so is much more up to date in how it understands contemporary language use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp represents the trained language model provided by Spacy...\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I don't see my cat. He has a long tail, fluffy ears and big eyes! He also subscribes to Marxist historical materialism. It's just his way."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In SpaCy tokenization happens the moment you wrap a string in your language model object nlp()\n",
    "\n",
    "doc = nlp(test_phrase)\n",
    "\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I, do, n't, see, my, cat, ., He, has, a, long, tail, ,, fluffy, ears, and, big, eyes, !, He, also, subscribes, to, Marxist, historical, materialism, ., It, 's, just, his, way, .]\n"
     ]
    }
   ],
   "source": [
    "# if we iterate over the doc we can see the tokens.\n",
    "tokens_a = []\n",
    "\n",
    "for word in doc:\n",
    "    tokens_a.append(word)\n",
    "print(tokens_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**List comprehensions** allow us to do in 1 line what would normally take 3. They are much more efficient than using a `for` loop. We'll see how they can be used more later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I, do, n't, see, my, cat, ., He, has, a, long, tail, ,, fluffy, ears, and, big, eyes, !, He, also, subscribes, to, Marxist, historical, materialism, ., It, 's, just, his, way, .]\n"
     ]
    }
   ],
   "source": [
    "tokens_b = [word for word in doc]\n",
    "print(tokens_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A number of things have been done by the tokenizer.\n",
    "- Punctuation has been seperated from words into their own tokens.\n",
    "- Words that are contractions of two words (It's > It is / Don't > Do not) have been split into two.\n",
    "- This becomes more useful in a minute and is all part of the process of reducing the nuance of language to make documents more comparable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Pre-Processing\n",
    "- Whilst the above looks just like the strings again it is actually a SpaCy **Document**.\n",
    "- Once a string is processed by SpaCy it becomes a SpaCy [Document object](https://spacy.io/api/doc). \n",
    "- The SpaCy document object itself is made up of SpaCy [Token objects](https://spacy.io/api/token).\n",
    "\n",
    "This means that Documents and Tokens have a range of associated methods and attributes based on SpaCy's analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do that again\n",
    "test_phrase = \"I don't see my cat. He has a long tail, fluffy ears and big eyes!\"\\\n",
    "\" He also subscribes to Marxist historical materialism. It's just his way.\"\n",
    "doc = nlp(test_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.lang_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cat"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = doc[5]\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.token.Token"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-Processing: Lemmatisation\n",
    "Language is very nuanced in real life, but part of the filtering process involves reducing that nuance to strip back to a piece of text's bare bones.\n",
    "\n",
    "```\n",
    "\"I don't like rabbits in space\"\n",
    "\"I do not like rabbits in space\"\n",
    "```\n",
    "- Semantically the same, computationally different.\n",
    "- Lemmatising using the token method `.lemma_` allows us to roll words back to a common 'root'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-PRON-', 'do', 'not', 'like', 'rabbit', 'in', 'space']\n",
      "['-PRON-', 'do', 'not', 'like', 'rabbit', 'in', 'space']\n"
     ]
    }
   ],
   "source": [
    "rabbit_1 = nlp(\"I don't like rabbits in space\")\n",
    "rabbit_2 = nlp(\"I do not like rabbits in space\")\n",
    "\n",
    "print( [token.lemma_ for token in rabbit_1])\n",
    "print( [token.lemma_ for token in rabbit_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These phrases below are semantically similar, but only share 1 word. Lemmatising brings them closer together computationally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-PRON-', 'be', 'love', 'these', 'rabbit']\n",
      "['-PRON-', 'love', 'this', 'rabbit', '!']\n"
     ]
    }
   ],
   "source": [
    "rabbit_1 = nlp(\"I'm loving these rabbits\")\n",
    "rabbit_2 = nlp(\"I love this rabbit!\")\n",
    "\n",
    "print( [token.lemma_ for token in rabbit_1])\n",
    "print( [token.lemma_ for token in rabbit_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I don't see my cat. He has a long tail, fluffy ears and big eyes! He also subscribes to Marxist historical materialism. It's just his way."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see what it does to our test phrase\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-PRON-', 'do', 'not', 'see', '-PRON-', 'cat', '.', '-PRON-', 'have', 'a', 'long', 'tail', ',', 'fluffy', 'ear', 'and', 'big', 'eye', '!', '-PRON-', 'also', 'subscribe', 'to', 'marxist', 'historical', 'materialism', '.', '-PRON-', 'be', 'just', '-PRON-', 'way', '.']\n"
     ]
    }
   ],
   "source": [
    "print( [token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-Processing: Punctuation\n",
    "Unless punctuation matters for your analysis (such as needing to break text down into sentences), we normally will clear out punctuation from text. We can use SpaCy's `.is_alpha` attribute to include only tokens that are alphabetical, and still just return the lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-PRON-', 'do', 'see', '-PRON-', 'cat', '-PRON-', 'have', 'a', 'long', 'tail', 'fluffy', 'ear', 'and', 'big', 'eye', '-PRON-', 'also', 'subscribe', 'to', 'marxist', 'historical', 'materialism', '-PRON-', 'just', '-PRON-', 'way']\n"
     ]
    }
   ],
   "source": [
    "processed_doc = [token.lemma_ for token in doc if token.is_alpha]\n",
    "print(processed_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-Processing: Stop Words\n",
    "Stop words are common words that tend to be structurally useful in sentences, but are too common to provide much meaning alone. They are often stripped out before text analysis.\n",
    "Lets add this to our list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-PRON-', '-PRON-', 'cat', '-PRON-', 'long', 'tail', 'fluffy', 'ear', 'big', 'eye', '-PRON-', 'subscribe', 'marxist', 'historical', 'materialism', '-PRON-', '-PRON-', 'way']\n"
     ]
    }
   ],
   "source": [
    "print([word for word in processed_doc if word.lower() not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll add .lower() to our result to ensure that we \n",
    "# get rid of any distinction when it comes to capitalisation as well\n",
    "\n",
    "def process_text(doc):\n",
    "    return [token.lemma_.lower() for token in doc if token.is_alpha]\n",
    "    \n",
    "def filter_stops(tokens, stop_words):\n",
    "    return [tok for tok in tokens if tok.lower() not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-pron-', 'do', 'see', '-pron-', 'cat', '-pron-', 'have', 'a', 'long', 'tail', 'fluffy', 'ear', 'and', 'big', 'eye', '-pron-', 'also', 'subscribe', 'to', 'marxist', 'historical', 'materialism', '-pron-', 'just', '-pron-', 'way']\n",
      "\n",
      "['-pron-', '-pron-', 'cat', '-pron-', 'long', 'tail', 'fluffy', 'ear', 'big', 'eye', '-pron-', 'subscribe', 'marxist', 'historical', 'materialism', '-pron-', '-pron-', 'way']\n"
     ]
    }
   ],
   "source": [
    "test_phrase = \"I don't see my cat. He has a long tail, fluffy ears and big eyes!\"\\\n",
    "\" He also subscribes to Marxist historical materialism. It's just his way.\"\n",
    "doc = nlp(test_phrase)\n",
    "\n",
    "print(process_text(doc))\n",
    "print()\n",
    "print(filter_stops(process_text(doc), stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing a Corpus\n",
    "A \"Corpus\" is a collection of textual documents. Often in computational textual analysis a corpus size will run into the hundreds, thousands or even hundreds of thousands. Whilst we could process each document seperately with a `for` loop, it is much more efficient to use Spacy's `pipe` which can process multiple documents in parallel and is memory efficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"I don't like rabbits in space\",\n",
    "         \"I do not like rabbits in space\",\n",
    "         \"I'm loving these rabbits\",\n",
    "         \"I love this rabbit!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Language.pipe at 0x7fbcdfa2dad0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = nlp.pipe(corpus)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy has created a generator object. This means that at the moment, no processing has been done. Each document is only processed when we iterate over the generator object as if it were a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[I don't like rabbits in space,\n",
       " I do not like rabbits in space,\n",
       " I'm loving these rabbits,\n",
       " I love this rabbit!]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can force the generator to produce the results by iterating over it in a list comprehension.\n",
    "docs = [doc for doc in docs]\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For peace of mind we can check and see that yes the objects are spacy documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    print(type(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which means we can run our processor on each Spacy doc as it is spat out and retain the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['-pron-', 'do', 'like', 'rabbit', 'in', 'space'],\n",
       " ['-pron-', 'do', 'not', 'like', 'rabbit', 'in', 'space'],\n",
       " ['-pron-', 'love', 'these', 'rabbit'],\n",
       " ['-pron-', 'love', 'this', 'rabbit']]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[process_text(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_documents(corpus, stop_words=None):\n",
    "    docs = nlp.pipe(corpus)\n",
    "    processed = [process_text(doc) for doc in docs]\n",
    "    if stop_words is not None:\n",
    "        processed = [filter_stops(doc, stop_words) for doc in processed]\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['-pron-', 'do', 'like', 'rabbit', 'in', 'space'], ['-pron-', 'do', 'not', 'like', 'rabbit', 'in', 'space'], ['-pron-', 'love', 'these', 'rabbit'], ['-pron-', 'love', 'this', 'rabbit']]\n",
      "\n",
      "[['-pron-', 'like', 'rabbit', 'space'], ['-pron-', 'like', 'rabbit', 'space'], ['-pron-', 'love', 'rabbit'], ['-pron-', 'love', 'rabbit']]\n"
     ]
    }
   ],
   "source": [
    "stop_words = nlp.Defaults.stop_words\n",
    "\n",
    "print( process_documents(corpus) )\n",
    "print()\n",
    "print( process_documents(corpus, stop_words) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Data Test\n",
    "So far we've been working on a toy dataset. Lets see what happens with a real dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sample_news.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 6 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   uuid               100 non-null    object\n",
      " 1   query              100 non-null    object\n",
      " 2   thread.title_full  100 non-null    object\n",
      " 3   text               100 non-null    object\n",
      " 4   published          100 non-null    object\n",
      " 5   thread.site        100 non-null    object\n",
      "dtypes: object(6)\n",
      "memory usage: 4.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10 s, sys: 2.12 s, total: 12.1 s\n",
      "Wall time: 12.2 s\n"
     ]
    }
   ],
   "source": [
    "stop_words = nlp.Defaults.stop_words\n",
    "\n",
    "%time result = process_documents(df['text'], stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image copyright Getty Images UK Prime Minister Boris Johnson hopes to persuade MPs to back a deal to take the UK out of the EU.\n",
      "Doing so would implement the result of the referendum of June 2016, in which 52% of voters backed Leave and 48% Remain.\n",
      "But where do voters stand on Brexit now, after more than three years of debate and negotiation?\n",
      "There is no majority for any course of action First, no single course of action is preferred by a majority of voters.\n",
      "For example, polling firm Kantar has asked voters on a number of occasions which of four possible outcomes they prefer.\n",
      "The most popular choice has been to remain in the EU. However, this secured the support of only about one in three.\n",
      "The next most popular, leaving without a deal, is preferred by slightly less than a quarter.\n",
      "Much the same picture has been painted by another survey. BMG asked people which of five alternatives they would prefer if a deal is not agreed by the end of this month. None has come even close to being backed by more than half of voters.\n",
      "Should no agreement be reached, the single most popular option is to leave the EU without a deal. Even so, it is still only backed by about one in three.\n",
      "Both of the next most popular options - holding another referendum and reversing Brexit without a referendum, are only chosen by about one in five.\n",
      "Three polls, by Opinium, Panelbase and ComRes have asked people what they thought of proposals for a deal put forward by Mr Johnson .\n",
      "All three found that slightly more voters were in favour of them than against. However, they were still backed by well under half.\n",
      "In Opinium's poll, just 27% thought the proposals would represent a good deal, while 22% reckoned they would represent a bad one.\n",
      "Brexit deal 'still possible, but more difficult' A simple guide to Brexit What is 'no-deal Brexit'? Most people either said it would neither be good nor bad, or that they did not know.\n",
      "Both Panelbase and ComRes found that 31%-32% support the proposals, while 27%-28% oppose them. But in both cases 41% said they did not know.\n",
      "Against this backdrop, what voters will make of any compromise deal that Mr Johnson might strike with the EU is far from clear.\n",
      "Leave and Remain voters hold very different views Second, those who voted Remain and those who backed Leave have very different preferences.\n",
      "The single most popular option among Leave voters is to exit the EU without a deal. According to Kantar, at least half of them prefer that course of action.\n",
      "Only about three in 10 pick either of the deals put before them by Kantar: the agreement Mrs May negotiated with the EU, or a \"soft\" Brexit under which the UK will still be part of the single market and customs union.\n",
      "Meanwhile, in the event of no deal, on average nearly seven in 10 Leave voters tell BMG they back leaving without one.\n",
      "In contrast, most of those who voted Remain believe that Brexit should be reversed. On average two in three of them tell Kantar they think Article 50 should be revoked.\n",
      "BMG offered its respondents both the possibility of holding another referendum and of reversing Brexit without a ballot.\n",
      "On average, nearly four in 10 Remain voters say Brexit should simply be reversed, while about three in 10 opt for another vote.\n",
      "Few voters have changed their minds Third, very few voters on either side of the argument have changed their minds about whether the UK should leave the EU. The country appears to be just as divided as it was three years ago.\n",
      "On average, during the last month, polls that ask people how they would vote in another referendum suggest that 88% of those who backed Remain would do so again. Among those who voted Leave, 86% have not changed their minds.\n",
      "These figures have changed very little during the last two years.\n",
      "True, most polls suggest - and have done so for some time - that the balance of opinion might be tilted narrowly in favour of remaining a member of the EU. On average, this is by 53% to 47%.\n",
      "However, this lead for Remain rests primarily on the views expressed by those who did not vote three years ago - and perhaps might not do so again.\n",
      "In truth, nobody can be sure what would happen if there were to be another referendum.\n",
      "More like this\n",
      "Do voters support a no-deal Brexit? Who would win if a general election were held now? What do voters make of Brexit now? What the EU elections tell us about support for Brexit Voters are split on holding another referendum Fourth, voters are divided about whether any agreement that might be reached with the EU should be put to a referendum.\n",
      "The balance of opinion differs from poll to poll.\n",
      "When people are asked about a \"public vote\" they are more likely to show support for another ballot than when asked about a \"referendum\" on the UK's membership of the EU.\n",
      "But they all agree that those who voted Remain are much keener on another vote than those who backed Leave.\n",
      "All five of the polls put support for another ballot among Remain voters at over two-thirds.\n",
      "In contrast, four of them find that fewer than 20% of Leave supporters are in favour of the idea. The fifth, by Kantar, puts it only somewhat higher, at 37%.\n",
      "So those who voted Remain are much more likely than those who voted Leave to welcome a ballot that might overturn the result of three years ago.\n",
      "Whatever the outcome this week, the division between Remainers and Leavers does not look as though it is going to be easy to heal.\n",
      "About this piece\n",
      "This analysis piece was commissioned by the BBC from an expert working for an outside organisation .\n",
      "Further details of the research on which it is based are available here .\n",
      "Sir John Curtice is professor of politics, Strathclyde University, and senior research fellow at NatCen Social Research and The UK in a Changing Europe .\n",
      "*Full wording of questions for the chart \"attitudes towards a second referendum\". Kantar: Should the final deal/agreement reached by the government be put to a public vote?; YouGov: Would you support or oppose a public vote on Brexit?; Deltapoll: Would you support or oppose a second referendum on British membership of the European Union?; Panelbase: Do you think there should be a new referendum on Brexit?; BMG: To what extent do you support or oppose [holding] a second in-out EU referendum?\n",
      "Edited by Duncan Walker\n",
      "Charts by David Brown and Dominic Bailey\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[0,'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['image', 'copyright', 'getty', 'images', 'uk', 'prime', 'minister', 'boris', 'johnson', 'hope', 'persuade', 'mps', 'deal', 'uk', 'eu', 'implement', 'result', 'referendum', 'june', 'voter', 'leave', 'remain', 'voter', 'stand', 'brexit', 'year', 'debate', 'negotiation', 'majority', 'course', 'action', 'single', 'course', 'action', 'prefer', 'majority', 'voter', 'example', 'polling', 'firm', 'kantar', 'ask', 'voter', 'number', 'occasion', 'possible', 'outcome', '-pron-', 'prefer', 'popular', 'choice', 'remain', 'eu', 'secure', 'support', 'popular', 'leave', 'deal', 'prefer', 'slightly', 'quarter', 'picture', 'paint', 'survey', 'bmg', 'ask', 'people', 'alternative', '-pron-', 'prefer', 'deal', 'agree', 'end', 'month', 'come', 'close', 'half', 'voter', 'agreement', 'reach', 'single', 'popular', 'option', 'leave', 'eu', 'deal', '-pron-', 'popular', 'option', 'hold', 'referendum', 'reverse', 'brexit', 'referendum', 'choose', 'poll', 'opinium', 'panelbase', 'comres', 'ask', 'people', '-pron-', 'think', 'proposal', 'deal', 'forward', 'mr', 'johnson', 'find', 'slightly', 'voter', 'favour', '-pron-', '-pron-', 'half', 'opinium', 'poll', 'think', 'proposal', 'represent', 'good', 'deal', 'reckon', '-pron-', 'represent', 'bad', 'brexit', 'deal', 'possible', 'difficult', 'simple', 'guide', 'brexit', 'deal', 'brexit', 'people', '-pron-', 'good', 'bad', '-pron-', 'know', 'panelbase', 'comres', 'find', 'support', 'proposal', 'oppose', '-pron-', 'case', '-pron-', 'know', 'backdrop', 'voter', 'compromise', 'deal', 'mr', 'johnson', 'strike', 'eu', 'far', 'clear', 'leave', 'remain', 'voter', 'hold', 'different', 'view', 'second', 'vote', 'remain', 'leave', 'different', 'preference', 'single', 'popular', 'option', 'leave', 'voter', 'exit', 'eu', 'deal', 'accord', 'kantar', 'half', '-pron-', 'prefer', 'course', 'action', 'pick', 'deal', '-pron-', 'kantar', 'agreement', 'mrs', 'negotiate', 'eu', 'soft', 'brexit', 'uk', 'single', 'market', 'custom', 'union', 'event', 'deal', 'average', 'nearly', 'seven', 'leave', 'voter', 'tell', 'bmg', '-pron-', 'leave', 'contrast', 'vote', 'remain', 'believe', 'brexit', 'reverse', 'average', '-pron-', 'tell', 'kantar', '-pron-', 'think', 'article', 'revoke', 'bmg', 'offer', '-pron-', 'respondent', 'possibility', 'hold', 'referendum', 'reverse', 'brexit', 'ballot', 'average', 'nearly', 'remain', 'voter', 'brexit', 'simply', 'reverse', 'opt', 'vote', 'voter', 'change', '-pron-', 'mind', 'voter', 'argument', 'change', '-pron-', 'mind', 'uk', 'leave', 'eu', 'country', 'appear', 'divide', '-pron-', 'year', 'ago', 'average', 'month', 'poll', 'ask', 'people', '-pron-', 'vote', 'referendum', 'suggest', 'remain', 'vote', 'leave', 'change', '-pron-', 'mind', 'figure', 'change', 'little', 'year', 'true', 'poll', 'suggest', 'time', 'balance', 'opinion', 'tilt', 'narrowly', 'favour', 'remain', 'member', 'eu', 'average', 'lead', 'remain', 'rest', 'primarily', 'view', 'express', 'vote', 'year', 'ago', 'truth', 'sure', 'happen', 'referendum', 'like', 'voter', 'support', 'deal', 'brexit', 'win', 'general', 'election', 'hold', 'voter', 'brexit', 'eu', 'election', 'tell', '-pron-', 'support', 'brexit', 'voters', 'split', 'hold', 'referendum', 'fourth', 'voter', 'divide', 'agreement', 'reach', 'eu', 'referendum', 'balance', 'opinion', 'differ', 'poll', 'poll', 'people', 'ask', 'public', 'vote', '-pron-', 'likely', 'support', 'ballot', 'ask', 'referendum', 'uk', 'membership', 'eu', '-pron-', 'agree', 'vote', 'remain', 'keen', 'vote', 'leave', 'poll', 'support', 'ballot', 'remain', 'voter', 'contrast', '-pron-', 'find', 'leave', 'supporter', 'favour', 'idea', 'fifth', 'kantar', '-pron-', 'somewhat', 'high', 'vote', 'remain', 'likely', 'vote', 'leave', 'welcome', 'ballot', 'overturn', 'result', 'year', 'ago', 'outcome', 'week', 'division', 'remainers', 'leavers', 'look', '-pron-', 'easy', 'heal', 'piece', 'analysis', 'piece', 'commission', 'bbc', 'expert', 'work', 'outside', 'organisation', 'detail', 'research', '-pron-', 'base', 'available', 'sir', 'john', 'curtice', 'professor', 'politic', 'strathclyde', 'university', 'senior', 'research', 'fellow', 'natcen', 'social', 'research', 'uk', 'changing', 'europe', 'wording', 'question', 'chart', 'attitude', 'second', 'referendum', 'kantar', 'final', 'deal', 'agreement', 'reach', 'government', 'public', 'vote', 'yougov', '-pron-', 'support', 'oppose', 'public', 'vote', 'brexit', 'deltapoll', '-pron-', 'support', 'oppose', 'second', 'referendum', 'british', 'membership', 'european', 'union', 'panelbase', '-pron-', 'think', 'new', 'referendum', 'brexit', 'bmg', 'extent', '-pron-', 'support', 'oppose', 'hold', 'second', 'eu', 'referendum', 'edit', 'duncan', 'walker', 'charts', 'david', 'brown', 'dominic', 'bailey']\n"
     ]
    }
   ],
   "source": [
    "print(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to create a new column of tokenized documents we simply assign the result like so...\n",
    "\n",
    "df['tokens'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Image copyright Getty Images UK Prime Minister...</td>\n",
       "      <td>[image, copyright, getty, images, uk, prime, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stocks were off slightly as investors consider...</td>\n",
       "      <td>[stock, slightly, investor, consider, mixed, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Image copyright Getty Images A key part of the...</td>\n",
       "      <td>[image, copyright, getty, images, key, brexit,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Send Load more share options\\nThe New IRA has ...</td>\n",
       "      <td>[send, load, share, option, new, ira, break, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The issue of the Irish border - and how to han...</td>\n",
       "      <td>[issue, irish, border, handle, flow, good, peo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>IN GRAPHICS: What happens now? How has the gov...</td>\n",
       "      <td>[graphics, happen, government, react, mr, raab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Report\\nProtesters travelled from across the U...</td>\n",
       "      <td>[report, protester, travel, uk, attend, march,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>UK's Johnson asks for a Brexit delay that he d...</td>\n",
       "      <td>[uk, johnson, ask, brexit, delay, -pron-, want...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>The MP who is bidding to replace him next mont...</td>\n",
       "      <td>[mp, bid, replace, -pron-, month, warn, health...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>'I'd be behind bars if I had acted like PM has...</td>\n",
       "      <td>[-pron-, bar, -pron-, act, like, pm, lord, hes...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  \\\n",
       "0   Image copyright Getty Images UK Prime Minister...   \n",
       "1   Stocks were off slightly as investors consider...   \n",
       "2   Image copyright Getty Images A key part of the...   \n",
       "3   Send Load more share options\\nThe New IRA has ...   \n",
       "4   The issue of the Irish border - and how to han...   \n",
       "..                                                ...   \n",
       "95  IN GRAPHICS: What happens now? How has the gov...   \n",
       "96  Report\\nProtesters travelled from across the U...   \n",
       "97  UK's Johnson asks for a Brexit delay that he d...   \n",
       "98  The MP who is bidding to replace him next mont...   \n",
       "99  'I'd be behind bars if I had acted like PM has...   \n",
       "\n",
       "                                               tokens  \n",
       "0   [image, copyright, getty, images, uk, prime, m...  \n",
       "1   [stock, slightly, investor, consider, mixed, b...  \n",
       "2   [image, copyright, getty, images, key, brexit,...  \n",
       "3   [send, load, share, option, new, ira, break, -...  \n",
       "4   [issue, irish, border, handle, flow, good, peo...  \n",
       "..                                                ...  \n",
       "95  [graphics, happen, government, react, mr, raab...  \n",
       "96  [report, protester, travel, uk, attend, march,...  \n",
       "97  [uk, johnson, ask, brexit, delay, -pron-, want...  \n",
       "98  [mp, bid, replace, -pron-, month, warn, health...  \n",
       "99  [-pron-, bar, -pron-, act, like, pm, lord, hes...  \n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['text','tokens']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:teaching]",
   "language": "python",
   "name": "conda-env-teaching-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
