{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 12(a) - Text Mining\n",
    "## Analysing and summarising collections of text\n",
    "### Phrases\n",
    "\n",
    "Having learned how to clean and simplify our text for processing, the next stage is to ask what our text is about. This session will cover finding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sample_news_large.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['query'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(50, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrases (n-grams)\n",
    "<img src=\"https://github.com/Minyall/sc207_materials/blob/master/images/Archer-phrasing.jpg?raw=true\" align=\"right\" width=\"300\">\n",
    "\n",
    "- Operates on the assumption that if words often co-occur together in a corpus, they should be considered as a single 'phrase', rather than as individual words.\n",
    "- Phrasing improves the accuracy of various analyses as it recognises that words may be transformed by their context.\n",
    "- For example: \n",
    "    - In one document we have the phrase \"human rights\", in the other, \"human biology\". \n",
    "    - **Without phrasing** these may be considered similar as they both use the word \"human\".\n",
    "    - However **with phrasing** these would be transformed into two seperate tokens, human_rights and human_biology, and therefore be more likely to be distinguished as different.\n",
    "\n",
    "#### Training the Phraser\n",
    "\n",
    "`train_phraser` has three stages. \n",
    "- First we create a list of tokenized *sentences*. \n",
    "- We then feed that list of sentences to a Gensim `Phrases` model. This model looks at which token co-occur, how often and [makes a judgement](https://arxiv.org/abs/1310.4546) about whether co-occurence is common enough to consider it a 'phrase'.\n",
    "- Why sentences? Sentences mark out boundaries between words. Consider the phrase 'Human Rights'...\n",
    "\n",
    "\n",
    "```\n",
    "... and so recognising that he was only human. Rights based discussions can only....\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "... and so recognising that he was only human rights based discussions can only..\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim is a text processing library that has the Phrasing tools we need\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = nlp.Defaults.stop_words\n",
    "def process_text(doc):\n",
    "    return [token.lemma_.lower() for token in doc if token.is_alpha]\n",
    "\n",
    "# process sentences function\n",
    "def process_sentences(doc):\n",
    "    return # TO FILL\n",
    "\n",
    "def train_phraser(corpus, stop_words):\n",
    "    \n",
    "    return phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phraser = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = nlp(df['text'].iloc[0])\n",
    "test_text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = \n",
    "tokens[10:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrased = \n",
    "phrased[10:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[token for token in phrased if '_' in token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can save our phraser to disk so we don't have to do it again\n",
    "\n",
    "phraser.save('phraser.bin')\n",
    "\n",
    "# and load it when we need it\n",
    "\n",
    "phraser = Phrases.load('phraser.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating Phrasing\n",
    "Let's adjust our original functions to accommodate phrasing. We'll make it an optional part of the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUR REPLACEMENT FUNCTIONS\n",
    "\n",
    "def process_text(doc):\n",
    "    return [token.lemma_.lower() for token in doc if token.is_alpha] \n",
    "\n",
    "def process_sentences(doc):\n",
    "    return [process_text(sent) for sent in doc.sents]\n",
    "    \n",
    "def filter_stops(tokens, stop_words):\n",
    "    return [tok for tok in tokens if tok.lower() not in stop_words]\n",
    "\n",
    "def process_documents(corpus, stop_words=None): #change here\n",
    "    docs = nlp.pipe(corpus)\n",
    "    processed = [process_text(doc) for doc in docs] # and here\n",
    "    if stop_words is not None:\n",
    "        processed = [filter_stops(doc, stop_words) for doc in processed]\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_text(test_text)[10:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_text(test_text, phraser=phraser)[10:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_docs = process_documents(corpus, phraser=phraser, stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_docs[3][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to reiterate the steps\n",
    "df = pd.read_csv('sample_news_large.csv')\n",
    "\n",
    "df = df.sample(50, random_state=1) # only use if you want to reduce the number of rows for testing\n",
    "\n",
    "# Get your list of texts and stop words\n",
    "corpus = df['text']\n",
    "stop_words = nlp.Defaults.stop_words\n",
    "\n",
    "# train your phraser and save it to avoid retraining later\n",
    "phraser = train_phraser(corpus, stop_words)\n",
    "phraser.save('phraser.bin')\n",
    "\n",
    "tokenized_docs = process_documents(corpus,phraser=phraser, stop_words=stop_words)\n",
    "# Done!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = tokenized_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_phrases(tokens):\n",
    "    return [token for token in tokens if '_' in token]\n",
    "\n",
    "df['phrases'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phrases for one document\n",
    "df['phrases'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top ten phrases overall\n",
    "print(df.explode('phrases')['phrases'].value_counts()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top ten phrases per group\n",
    "for query,data in df.groupby('query'):\n",
    "    print(f\"****{query}****\")\n",
    "    print(data.explode('phrases')['phrases'].value_counts()[:10])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:teaching]",
   "language": "python",
   "name": "conda-env-teaching-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
