{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Text Mining\n",
    "## 2. Pre-Processing Documents to be Data\n",
    "- A key part of text analysis is preparing your text for different kinds of analysis.\n",
    "\n",
    "- Different types of analysis require different types of preparation but usually two steps are fundamental. \n",
    "    - Tokenizing\n",
    "    - Filtering.\n",
    "\n",
    "\n",
    "All the processes that we engaged with in Part 1 would have used some sort of Tokenizing in order to understand the underling text. When we prepare text to use other kinds of analysis we also need to tokenize appropriately.\n",
    "\n",
    "\n",
    "### a) Tokenizing\n",
    "The process of splitting up the text into a list of individual words that can be treated as individual units.\n",
    "\n",
    "How exactly you split up text is not necessarily straight forward and there are a range of different strategies. The correct one to use varies depending on the type of text you are using and the type of analysis that you want to do.\n",
    "\n",
    "To see how different tokenizer strategies interpret a piece of text you can check out the [Python NLTK Demo Page](https://text-processing.com/demo/tokenize/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing: Example of the problem\n",
    "We already know how to split up a string into a series of items in a list. It's pretty simple using `.split()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_phrase = \"I don't see my cat. He has a long tail, fluffy ears and big eyes!\"\\\n",
    "\" He also subscribes to Marxist historical materialism. It's just his way.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokens = \n",
    "test_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we check if the string 'ears' is in the list\n",
    "'ears' in test_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so we would imagine that 'eyes' is also in the list?\n",
    "'eyes' in test_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having punctuation atached to words like this can cause us problems because the tokens `eyes` and `eyes!` would be considered two seperate things. This messes with a lot of analysis further down the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider this tweet\n",
    "\n",
    "test_tweet = \"How will #Brexit affect #customsdeclarations? Check out the @britishchambers\"\\\n",
    "\" guide to find out! https://t.co/CpoGudZAcb https://t.co/Pr4RW4Tyhw\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we split it we get\n",
    "test_tweet.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweets pose further challenges because because of the unusual use of language that is common within the domain of twitter and other social media (hashtags)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing: Using Tokenizers\n",
    "Tokenizers are functions that split up text for us. Some of them are based on complex sets of rules, others are based on training computers using lots of examples of text and the ideal way to split them.\n",
    "\n",
    "There are many (many!) packages available for handling text data. These include..\n",
    "\n",
    "- [The Natural Language Toolkit (NLTK)](https://www.nltk.org/): Very well established package. More of a focus on linguistics. Can do most tasks but a bit of a steep learning curve compared to TextBlob. People often dip into its toolset whilst using other packages.\n",
    "- [TextBlob](https://textblob.readthedocs.io/en/dev/): Good for beginners but lacks some features. Built on top of NLTK but doesn't have all of NLTK's functionality.\n",
    "- [Gensim](https://radimrehurek.com/gensim/): Good to handle very large amounts of text (100,000's). However to achieve this it relies on fairly high level concepts in Python, making its approach a little tricky to get a grip on. \n",
    "- [SpaCy](https://spacy.io/): Relatively new. Utilises a lot of language models that are pre-trained on very large datasets of text. Incredibly fast with industry level complex tools. Very flexible if used correctly and has a relatively accesible set of tools once you understand the SpaCy approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In SpaCy tokenization happens the moment you wrap a string in your language model object nlp()\n",
    "\n",
    "doc = \n",
    "\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we iterate over the doc we can see the tokens.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a list of tokens we can do one of two things...\n",
    "\n",
    "- a) Use a `for loop`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_a = []\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- b) Use a *List Comprehension*\n",
    "\n",
    "List comprehensions allow us to do in 1 line what would normally take 3. We'll see how they can be used more later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_b = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing: Understanding Tokenization\n",
    "A number of things have been done by the tokenizer.\n",
    "- Punctuation has been seperated from words into their own tokens.\n",
    "- Words that are contractions of two words (It's > It is / Don't > Do not) have been split into two.\n",
    "- This becomes more useful in a minute and is all part of the process of reducing the nuance of language to make documents more comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"I don't like rabbits in space\",\n",
    "         \"I do not like rabbits in space\",\n",
    "         \"I'm loving these rabbits\",\n",
    "         \"I love this rabbit\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember we can use nlp.pipe to convert a list of documents quicker than iterating over the list one at a time.\n",
    "\n",
    "docs = \n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can force the generator to produce the results by wrapping it in a list\n",
    "docs = \n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each of these list items is now a SpaCy document...\n",
    "\n",
    "one_doc = \n",
    "print()\n",
    "\n",
    "one_doc_tokens = \n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Filtering\n",
    "Often in text analysis, there is a lot of material that is useful to humans but less useful to computers when performing analysis. Language is very nuanced in real life, but part of the filtering process involves reducing that nuance to strip back to a piece of text's bare bones.\n",
    "\n",
    "For example, how different would we consider these two sentences...?\n",
    "\n",
    "```\n",
    "\"I don't like rabbits in space\"\n",
    "\"I do not like rabbits in space\"\n",
    "```\n",
    "Semantically they are the same, computationally they are different.\n",
    "```\n",
    "\"I am loving these rabbits\"\n",
    "\"I love this rabbit\"\n",
    "```\n",
    "Semantically a little different but still similar. Computationally very different as they only share one word, 'I'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SpaCy Tokens\n",
    "As we saw above, once a string is wrapped in a nlp model, it becomes a SpaCy [Document object](https://spacy.io/api/doc) giving access to a range of methods. The SpaCy document object, tokenizes our string meaning that the Document object is also a list of SpaCy [Token objects](https://spacy.io/api/token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and to reiterate, a spacy document is a collection of spacy tokens\n",
    "print(one_doc)\n",
    "print(type(one_doc))\n",
    "\n",
    "print(one_doc_tokens[4])\n",
    "print(type(one_doc_tokens[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SpaCy Tokens: Lemmatization\n",
    "\n",
    "This is the process of reducing a word down to its 'root' form whilst still retaining the meaning. By reducing to the root of a word, you reduce the variation of words used and increase the chances that semantically similar phrases have the same tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"I don't like rabbits in space\",\n",
    "         \"I do not like rabbits in space\",\n",
    "         \"I'm loving these rabbits\",\n",
    "         \"I love this rabbit\"]\n",
    "\n",
    "docs = \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### '-PRON-'\n",
    "In SpaCy the `-PRON-` is a stand-in for any pronoun. From [the documentation](https://spacy.io/api/annotation).\n",
    "\n",
    "\n",
    "##### About spaCy's custom pronoun lemma for English\n",
    "\n",
    "*spaCy adds a special case for English pronouns: all English pronouns are lemmatized to the special token -PRON-. Unlike verbs and common nouns, there’s no clear base form of a personal pronoun. Should the lemma of “me” be “I”, or should we normalize person as well, giving “it” — or maybe “he”? spaCy’s solution is to introduce a novel symbol, -PRON-, which is used as the lemma for all personal pronouns.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SpaCy Tokens: Stop Words\n",
    "\n",
    "\"Stop\" words are words within a language that provide structure to the language but are often do not convey a lot of information in themselves. Examples include...\n",
    "- the\n",
    "- and\n",
    "- it\n",
    "\n",
    "Normally for analysis processes beyond what SpaCy provides in terms of document level analysis we would want to filter out these stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can load a list of stopwwords from spacy\n",
    "stopwords = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_phrase = \n",
    "print(nlp_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in nlp_phrase:\n",
    "    print(f\"{word} : {word.text.lower() in stopwords}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we lemmatize\n",
    "\n",
    "lemmas = \n",
    "lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can then filter out the stopwords like so\n",
    "\n",
    "lemmas_filtered = \n",
    "lemmas_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also filter for only alphabetical tokens (not punctuation) using .is_alpha\n",
    "\n",
    "nlp_phrase_filtered = \n",
    "nlp_phrase_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we put all our filters together...\n",
    "\n",
    "tokens = \n",
    "tokens = \n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_text(spacy_doc, lower=True):\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_phrase = \"I don't see my cat. He has a long tail, fluffy ears and big eyes!\"\\\n",
    "\" He also subscribes to Marxist historical materialism. It's just his way.\"\n",
    "doc = nlp(test_phrase)\n",
    "filter_text(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning our News Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('news_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_nlp'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_tokens'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the original and the cleaned versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.loc[0,'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' '.join(df.loc[0,'cleaned_tokens']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use this technique to clean our text in the next sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:teaching]",
   "language": "python",
   "name": "conda-env-teaching-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
