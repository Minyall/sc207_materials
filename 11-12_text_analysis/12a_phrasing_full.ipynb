{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 12(a) - Text Mining\n",
    "## Analysing and summarising collections of text\n",
    "### Phrases\n",
    "\n",
    "Having learned how to clean and simplify our text for processing, the next stage is to ask what our text is about. This session will cover finding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sample_news_large.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['query'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.sample(50, random_state=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Phrases (n-grams)\n",
    "<img src=\"https://github.com/Minyall/sc207_materials/blob/master/images/Archer-phrasing.jpg?raw=true\" align=\"right\" width=\"300\">\n",
    "\n",
    "- Operates on the assumption that if words often co-occur together in a corpus, they should be considered as a single 'phrase', rather than as individual words.\n",
    "- Phrasing improves the accuracy of various analyses as it recognises that words may be transformed by their context.\n",
    "- For example: \n",
    "    - In one document we have the phrase \"human rights\", in the other, \"human biology\". \n",
    "    - **Without phrasing** these may be considered similar as they both use the word \"human\".\n",
    "    - However **with phrasing** these would be transformed into two seperate tokens, human_rights and human_biology, and therefore be more likely to be distinguished as different.\n",
    "\n",
    "#### Training the Phraser\n",
    "\n",
    "`train_phraser` has three stages. \n",
    "- First we create a list of tokenized *sentences*. \n",
    "- We then feed that list of sentences to a Gensim `Phrases` model. This model looks at which token co-occur, how often and [makes a judgement](https://arxiv.org/abs/1310.4546) about whether co-occurence is common enough to consider it a 'phrase'.\n",
    "- Why sentences? Sentences mark out boundaries between words. Consider the phrase 'Human Rights'...\n",
    "\n",
    "\n",
    "```\n",
    "... and so recognising that he was only human. Rights based discussions can only....\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "... and so recognising that he was only human rights based discussions can only..\n",
    "\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# gensim is a text processing library that has the Phrasing tools we need\n",
    "from gensim.models.phrases import Phrases, ENGLISH_CONNECTOR_WORDS"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def process_text(doc):\n",
    "    return [token.lemma_.lower() for token in doc if token.text != '\\n' and token.is_alpha]\n",
    "\n",
    "# process sentences function\n",
    "def process_sentences(doc):\n",
    "    return [process_text(sent) for sent in doc.sents]\n",
    "\n",
    "def train_phraser(corpus):\n",
    "    sentences = []\n",
    "    for doc in nlp.pipe(corpus):\n",
    "        doc_sents = process_sentences(doc)\n",
    "        sentences.extend(doc_sents)\n",
    "    phraser = Phrases(sentences, connector_words=ENGLISH_CONNECTOR_WORDS)\n",
    "    return phraser"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "phraser = train_phraser(df['text'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_text = nlp(df['text'].iloc[0])\n",
    "test_text[:100]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokens = process_text(test_text)\n",
    "print(tokens[10:35])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "phrased = phraser[tokens]\n",
    "print(phrased[10:35])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print([token for token in phrased if '_' in token])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# We can save our phraser to disk so we don't have to do it again\n",
    "\n",
    "phraser.save('phraser.bin')\n",
    "\n",
    "# and load it when we need it\n",
    "\n",
    "phraser = Phrases.load('phraser.bin')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = tokenized_docs"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# OUR REPLACEMENT FUNCTIONS\n",
    "\n",
    "def new_process_text(doc, phraser=None):\n",
    "    if phraser is None:\n",
    "        tokens = [token.lemma_.lower() for token in doc if token.text != '\\n' and token.is_alpha]\n",
    "    else:\n",
    "        tokens = []\n",
    "        sentences = process_sentences(doc)\n",
    "        for sent in sentences:\n",
    "            phrased = phraser[sent]\n",
    "            tokens.extend(phrased)\n",
    "    return tokens \n",
    "    \n",
    "def filter_stops(tokens, stop_words):\n",
    "    return [tok for tok in tokens if tok.lower() not in stop_words]\n",
    "\n",
    "def process_documents(corpus, phraser=None, stop_words=None): #change here\n",
    "    docs = nlp.pipe(corpus)\n",
    "    processed = [new_process_text(doc, phraser=phraser) for doc in docs] # and here\n",
    "    if stop_words is not None:\n",
    "        processed = [filter_stops(doc, stop_words) for doc in processed]\n",
    "    return processed\n",
    "\n",
    "def stringify_tokens(tokenized_documents):\n",
    "    return [' '.join(doc) for doc in tokenized_documents]\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(new_process_text(test_text)[10:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(new_process_text(test_text, phraser=phraser)[10:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = nlp.Defaults.stop_words\n",
    "\n",
    "tokenized_docs = process_documents(df['text'], phraser=phraser, stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(tokenized_docs[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# to reiterate the steps\n",
    "df = pd.read_csv('sample_news_large.csv')\n",
    "\n",
    "df = df.sample(50, random_state=1) # only use if you want to reduce the number of rows for testing\n",
    "\n",
    "# Get your list of texts and stop words\n",
    "corpus = df['text']\n",
    "stop_words = nlp.Defaults.stop_words\n",
    "\n",
    "# train your phraser and save it to avoid retraining later\n",
    "phraser = train_phraser(corpus)\n",
    "phraser.save('phraser.bin')\n",
    "\n",
    "phraser = Phrases.load('phraser.bin')\n",
    "\n",
    "tokenized_docs = process_documents(corpus,phraser=phraser, stop_words=stop_words)\n",
    "# Done!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(tokenized_docs[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stringified = stringify_tokens(tokenized_docs)\n",
    "print(stringified[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['tokens'] = stringified\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Top Phrases\n",
    "A useful exploratory technique to get a quick sense of your data is to examine the phrases used."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def extract_phrases(tokens):\n",
    "    return [token for token in tokens.split() if '_' in token]\n",
    "\n",
    "df['phrases'] = df['tokens'].apply(extract_phrases)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# phrases for one document\n",
    "df['phrases'].iloc[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# top ten phrases overall\n",
    "print(df.explode('phrases')['phrases'].value_counts()[:10])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# top ten phrases per group\n",
    "for query,data in df.groupby('query'):\n",
    "    print(f\"****{query}****\")\n",
    "    print(data.explode('phrases')['phrases'].value_counts()[:10])\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:teaching]",
   "language": "python",
   "name": "conda-env-teaching-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}