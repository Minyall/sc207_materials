{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! conda install bs4 lxml --yes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WebScraping\n",
    "## 1. Extracting one Row of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reviewing the source\n",
    "\n",
    "- First we take a look at the page of HTML we want to capture. In a seperate window open up http://www.uberpeople.net/forums/Tips.\n",
    "\n",
    "- Explore the page as it is rendered in the browser, and the underlying code by right clicking on the page and using 'View Page Source'.\n",
    "\n",
    "- For now, we're just going to pull this entire page into memory and then we'll work out how to extract the parts we want.\n",
    "\n",
    "\n",
    "To retrieve the HTML we're going to use the Requests library which we will import below.\n",
    "\n",
    "The documentation for Requests can be found at http://docs.python-requests.org/en/master/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to website design we will need to present ourselves as a web browser\n",
    "# Using the file of 'user-agent' data we can send the correct headers when making our requests \n",
    "from random import choice\n",
    "with open('user_agent.txt','r') as f:\n",
    "    agents = f.readlines()\n",
    "    agents = [x.strip() for x in agents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('http://uberpeople.net/forums/Tips/', headers={'user-agent': choice(agents)}) # Yes it is that simple - thanks Requests!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  If we look at our html object we get a simple response code. 200 is a success, 404 for example would be a failure.\n",
    "#  For a full list of Http response codes see https://httpstatuses.com\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We can look at the content of the retrieved package. \n",
    "# Click the bar to the left of the text to expand or contract its screen usage.\n",
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting your source\n",
    "- Ok that big block of mess isn't that helpful...\n",
    "- We need to find a systematic way of combing through the entire HTML, and picking out what we need.\n",
    "- Make sure the page is open in Chrome or Firefox and then right click on the first title and choose 'Inspect (Element)' to see the underlying code.\n",
    "\n",
    "We can see that each row is in its own division. All these divisions sit inside a parent division with the class `\"structItemContainer-group js-threadList\"`. Knowing this will let us drill down into each row, and later iterate over each row and perform the same actions. To do this we will use a library called **Beautiful Soup**.\n",
    "\n",
    "Documentation for Beautiful Soup https://www.crummy.com/software/BeautifulSoup/bs4/doc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we get Beautiful soup to break down the HTML into something that can be navigated.\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "soup = BeautifulSoup(response.text,'lxml') # we have to make sure we give it the html.text content, and define the parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we look at our soup it is a little more structured, but lets keep refining....\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first focus in on the section of page we want - the division containing all the thread entries\n",
    "threads_container = soup.find('div', {'class':\"structItemContainer-group js-threadList\"})\n",
    "threads_container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# if we take our thread_frame we can use 'find_all' to return a list of child elements that match our criteria.\n",
    "# Each row has multiple classes, we will just pass the one that seems specific to rows of the table.\n",
    "\n",
    "threads = threads_container.find_all('div',{'class':'structItem--thread','data-author':True})\n",
    "threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can check this has worked and not given us any more than  the rows by counting the number of rows on the page \n",
    "# (20) and checking against the length (len) of the list here...\n",
    "len(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's find that first list item i.e. the first row.\n",
    "first_item = threads[0]\n",
    "\n",
    "print(first_item.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting row items\n",
    "The items we want from this row are...\n",
    "- Author\n",
    "- Thread-id > useful for ensuring no duplicates and for quickly locating threads later.\n",
    "- Title\n",
    "- Date\n",
    "- URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author is located inside the division acts as the container for our first item.\n",
    "# it is known as an attribute of the division called data-author. \n",
    "# We can retrieve the content of a tag attrribute as if the tag were a dictionary.\n",
    "\n",
    "author = first_item['data-author']\n",
    "print(author)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thread_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique IDs are not necesarily present in all websites, but this site happens to use them\n",
    "# It's not necessarily clear straight away from the code exactly what counts as the id.\n",
    "# making these decisions often requires you to look around the site and\n",
    "# get a feel for its structure.\n",
    "\n",
    "# In this case we can see the same number being used in the top level row division, and in the \n",
    "# url for the thread content. We could extract this from either division but here we'll take it\n",
    "# from the row data, where the id is in the 'class' \n",
    "\n",
    "first_item['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class has multiple elements which beautifulsoup returns as a list,\n",
    "# we need the last item in the list\n",
    "id_item = first_item['class'][-1]\n",
    "id_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the last item is a string and we just need everything after the '-'\n",
    "# we split up the string on the '-'...\n",
    "id_item.split('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the last item...\n",
    "id_item.split('-')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and convert it into an integer rather than keep the string\n",
    "thread_id = int(id_item.split('-')[-1])\n",
    "thread_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the remainder of items we need to step into the sub-divisions of the element, the div's inside our div. The row is made up of multiple subsections containing the information we want so we will need to step from our top level division into the various subsections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# starting with our first_item we use .find to search within its subordinates to find the\n",
    "# division containing the title.\n",
    "\n",
    "title_div = first_item.find('div', class_='structItem-title')\n",
    "title_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inside the title division is a url division which is always tagged with the <a> tag. \n",
    "# we can access this either with .find('a') or the convenience method of simply .a which\n",
    "# selects the first child element that is an <a> tag.\n",
    "\n",
    "title_div.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .text allows us to just get the plain text <a> between the tags </a>\n",
    "title = title_div.a.text\n",
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.strip() is a string function that cleans up the ends removing white space and breaks.\n",
    "# it is a good precautinary measure when gathering text from sites to ensure you aren't collecting\n",
    "# erroneous whitespace.\n",
    "\n",
    "title = title.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we inspect the date we can see it sits within a <time> tag within our row division\n",
    "first_item.find('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time is represented in a lot of different ways here, and Dates and times \n",
    "# are special types of object because they do not behave like normal numbers, \n",
    "# nor are they useful only as strings. The best thing to do is to save the string\n",
    "# called 'datetime' and convert it from a string into a 'datetime' object.\n",
    "\n",
    "\n",
    "date_string = first_item.find('time')['datetime']\n",
    "print(date_string)\n",
    "print(type(date_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Currently this is a string, which will be problematic later if we want any analysis to\n",
    "# understand it as time.\n",
    "\n",
    "# We can convert it from a string to a datetime object using the datetime library\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# we need to instruct the datetime .strptime function what parts of the string pertain to \n",
    "# the different divisions of time. For this site our instruction looks like this...\n",
    "\n",
    "date_format = '%Y-%m-%dT%H:%M:%S%z'\n",
    "# the string instructs strptime which parts of the string \n",
    "# are time components and which parts are just string such as dashes and colons.\n",
    "# See https://docs.python.org/2/library/datetime.html#strftime-and-strptime-behavior \n",
    "# for full documentation\n",
    "\n",
    "\n",
    "\n",
    "# %Y - Year\n",
    "# %m - Month\n",
    "# %d - day\n",
    "# %H - Hour\n",
    "# %M - Minute\n",
    "# %S - Second\n",
    "# %z - Timezone offset from GMT in hours\n",
    "\n",
    "date = datetime.strptime(date_string, date_format )\n",
    "print(date)\n",
    "print(type(date))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get to each thread, the user would click the title of the thread,\n",
    "# meaning the url for the thread must be in the title division somewhere\n",
    "title_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# yes it is in the href attribute of the child <a>\n",
    "title_div.a['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But this is not a whole url, it's relative to the domain 'http://uberpeople.net'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So let's put it all together\n",
    "relative_url = title_div.a['href']\n",
    "url = 'http://uberpeople.net' + relative_url\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# However the safe way to do it, because URLS can go wonky sometimes is to use part of the standard library.\n",
    "import urllib\n",
    "url = urllib.parse.urljoin('http://uberpeople.net', relative_url)\n",
    "# This has some verification features to make sure the URL makes sense.\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_info_extractor(row): # We'll feed it the isolated html for a row and let it pull it apart.\n",
    "    \n",
    "    #author\n",
    "    author = row['data-author']\n",
    "    \n",
    "    #id\n",
    "    id_item = row['class'][-1]\n",
    "    thread_id = int(id_item.split('-')[-1])\n",
    "    \n",
    "    #title\n",
    "    title_div = row.find('div', class_='structItem-title')\n",
    "    title = title_div.a.text.strip() # remember to .strip() off the useless spaces on the ends.\n",
    "    \n",
    "    #date\n",
    "    date_format = '%Y-%m-%dT%H:%M:%S%z'\n",
    "    date_string = row.find('time')['datetime']\n",
    "\n",
    "\n",
    "    date = datetime.strptime(date_string, date_format)\n",
    "    \n",
    "    #url\n",
    "    relative_url = title_div.a['href']\n",
    "    # remember the url is only relative so it needs to be made full using urlib.parse.urljoin\n",
    "    full_url = urllib.parse.urljoin('http://uberpeople.net',relative_url)\n",
    "    \n",
    "    # And now we spit out our final product - in this case we'll go for a list for Pandas use later.\n",
    "    # We'll also re-order\n",
    "    data_package = {'author':author,\n",
    "                   'title':title,\n",
    "                   'thread_id':thread_id,\n",
    "                   'date':date,\n",
    "                   'url':full_url}\n",
    "    \n",
    "    return data_package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's try it out\n",
    "\n",
    "soup = BeautifulSoup(response.text,'lxml')\n",
    "threads_container = soup.find('div', class_=\"structItemContainer-group js-threadList\")\n",
    "threads = threads_container.find_all('div', {'class':'structItem--thread', 'data-author':True} )\n",
    "\n",
    "first_item = threads[0]\n",
    "\n",
    "\n",
    "row_info_extractor(first_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for row in threads:\n",
    "    row_info = row_info_extractor(row)\n",
    "    data.append(row_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:teaching]",
   "language": "python",
   "name": "conda-env-teaching-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
