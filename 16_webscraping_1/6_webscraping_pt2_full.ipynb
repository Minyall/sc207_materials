{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WebScraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract all rows of data\n",
    "So we've automated the process of extracting each item from a row, but how do we do that for all rows?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "First we import our required libraries up front and define our function we designed in the last session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_info_extractor(row): # We'll feed it the isolated html for a row and let it pull it apart.\n",
    "    author = row['data-author']\n",
    "    \n",
    "    id_item = row['class'][-1]\n",
    "    thread_id = int(id_item.split('-')[-1])\n",
    "    \n",
    "    title_div = row.find('div', class_='structItem-title')\n",
    "    title = title_div.a.text.strip() # remember to .strip() off the useless spaces on the ends.\n",
    "    \n",
    "    date = row.find('time')['datetime']\n",
    "    \n",
    "    views = row.find('dl',class_='pairs pairs--justified structItem-minor').dd.text\n",
    "\n",
    "    relative_url = title_div.a['href']\n",
    "    full_url = urllib.parse.urljoin('http://uberpeople.net',relative_url)\n",
    "    \n",
    "    data_package = {'id': thread_id,\n",
    "                  'author': author,\n",
    "                  'title': title,\n",
    "                  'date': date,\n",
    "                  'views': views,\n",
    "                  'url': full_url}\n",
    "    \n",
    "    return data_package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-establish our list of threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('http://uberpeople.net/forums/Tips/')\n",
    "soup = BeautifulSoup(response.text,'lxml')\n",
    "threads_container = soup.find('div', class_=\"structItemContainer\")\n",
    "threads = threads_container.find_all('div',class_='structItem--thread')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Remember, our `threads` variable is actually a list of rows that was created after we asked BeautifulSoup to \n",
    "# .find_all of a particular type of divison within the threads_container.\n",
    "threads[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We've been working on the first row\n",
    "first_row = threads[0]\n",
    "row_info_extractor(first_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can easily use a loop to capture ALL the rows\n",
    "\n",
    "rows_data = []\n",
    "\n",
    "for row in threads:\n",
    "    result = row_info_extractor(row)\n",
    "    rows_data.append(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACTIVITY: Build a Function\n",
    "This function should...\n",
    "- take a requests response\n",
    "- convert it into soup - remember to use the .text attribute of the response\n",
    "- isolate the list of rows and then...\n",
    "- iterate across each row to extract the relevant data.\n",
    "- Data should be saved to a list\n",
    "- The function should return this filled list\n",
    "- Note: Use the function we built in the previous activity inside your new function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR NEW FUNCTION HERE\n",
    "\n",
    "def page_info_extractor(response):\n",
    "    soup = BeautifulSoup(response.text,'lxml')\n",
    "    threads_container = soup.find('div', class_=\"structItemContainer\")\n",
    "    threads = threads_container.find_all('div',class_='structItem--thread')\n",
    "    \n",
    "    page_data = []\n",
    "    for row in threads:\n",
    "        result = row_info_extractor(row)\n",
    "        page_data.append(result)\n",
    "    return page_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST THE FUNCTION\n",
    "\n",
    "response = requests.get('https://uberpeople.net/forums/Tips/')\n",
    "page_data = page_info_extractor(response)\n",
    "page_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
