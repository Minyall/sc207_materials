{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WebScraping\n",
    "## Extracting all rows on multiple pages\n",
    "Now we can ask requests to retrieve a page, feed it to our function, and recieve a structured list of relevant information. Now let's repeat this across multiple pages on the forum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiating here to ensure everyone has the same functions\n",
    "def row_info_extractor(row): # We'll feed it the isolated html for a row and let it pull it apart.\n",
    "    author = row['data-author']\n",
    "    \n",
    "    id_item = row['class'][-1]\n",
    "    thread_id = int(id_item.split('-')[-1])\n",
    "    \n",
    "    title_div = row.find('div', class_='structItem-title')\n",
    "    title = title_div.a.text.strip() # remember to .strip() off the useless spaces on the ends.\n",
    "    \n",
    "    date_format = '%Y-%m-%dT%H:%M:%S%z'\n",
    "    date_string = row.find('time')['datetime']\n",
    "    date = datetime.strptime(date_string, date_format)\n",
    "    \n",
    "    relative_url = title_div.a['href']\n",
    "    full_url = urllib.parse.urljoin('http://uberpeople.net',relative_url)\n",
    "\n",
    "    data_package = {'author':author,\n",
    "                   'title':title,\n",
    "                   'thread_id':thread_id,\n",
    "                   'date':date,\n",
    "                   'url':full_url}\n",
    "    \n",
    "    return data_package\n",
    "\n",
    "# NEW function to simplify extracting a whole page\n",
    "\n",
    "def page_info_extractor(response):\n",
    "    soup = BeautifulSoup(response.text,'lxml')\n",
    "    threads_container = soup.find('div', class_=\"structItemContainer-group js-threadList\")\n",
    "    threads = threads_container.find_all('div', {'class':'structItem--thread', 'data-author':True} )\n",
    "    \n",
    "    page_data = []\n",
    "    for row in threads:\n",
    "        result = row_info_extractor(row)\n",
    "        page_data.append(result)\n",
    "    return page_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the page structure\n",
    "- Look at page 2 of the threads.\n",
    "- Note the url https://uberpeople.net/forums/Tips/page-2\n",
    "- https://uberpeople.net/forums/Tips/page-1 takes us back to our original first page\n",
    "- Does this link work https://uberpeople.net/forums/Tips/page-300 ?\n",
    "- We can also see various points where the page provides us information on how many pages there are in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "with open('user_agent.txt','r') as f:\n",
    "    agents = f.readlines()\n",
    "    agents = [x.strip() for x in agents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example with our data - gather the data for two pages\n",
    "\n",
    "response_1 = requests.get('https://uberpeople.net/forums/Tips/page-1', headers={'user-agent':choice(agents)})\n",
    "response_2 = requests.get('https://uberpeople.net/forums/Tips/page-2', headers={'user-agent':choice(agents)})\n",
    "\n",
    "page_1 = page_info_extractor(response_1)\n",
    "page_2 = page_info_extractor(response_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember if we have two lists that we want to turn into one longer list we have to .extend()\n",
    "final_data = []\n",
    "\n",
    "final_data.extend(page_1)\n",
    "final_data.extend(page_2)\n",
    "df = pd.DataFrame(final_data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visiting multiple pages automatically\n",
    "We could, as we have above, manually create a new response object for each page, but that's not what programming is all about! How then do we automate the process.\n",
    "\n",
    "- We already know that we can predict the url for the any page of threads because it has the same structure `https://uberpeople.net/forums/Tips/page-{pick a number}`\n",
    "- This means we just need to increment that number by 1 every time we want to move to a new page.\n",
    "- Let's begin by working out how to generate urls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can do this by setting a max number of pages and simply generating urls\n",
    "\n",
    "maximum_pages = 5\n",
    "\n",
    "for number in range(1, maximum_pages+1): #we do +1 so it actually outputs up to AND INCLUDING the number we set as our maximum.\n",
    "    url = f'https://uberpeople.net/forums/Tips/page-{number}' # f-strings allows us to easily insert values into strings.\n",
    "    print(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So lets break down the steps\n",
    "\n",
    "# We set out maximum number of pages so we don't lose control!\n",
    "max_page = 5\n",
    "\n",
    "# We create an empty list to contain all the results from every page\n",
    "\n",
    "data = []\n",
    "\n",
    "# We create our range generator that spits out numbers between 1 and our maximum number of pages\n",
    "for page_no in range(1, max_page+1):\n",
    "    \n",
    "    # build the url\n",
    "    url = f'https://uberpeople.net/forums/Tips/page-{page_no}'\n",
    "    \n",
    "    # retrieve the page\n",
    "    response = requests.get(url, headers={'user-agent':choice(agents)})\n",
    "    page_data = page_info_extractor(response)\n",
    "    \n",
    "    # we EXTEND the final data list with our results and the loop starts from the beginning to collect the next set\n",
    "    data.extend(page_data) # \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethical Scraping\n",
    "Web Scraping uses the resources of the websites that we draw data from. Scripted scrapers can access these resources much faster than the site would expect a user to 'browse' the site. Some sites will even block connections from computers that they believe are displaying unusual browsing activity.\n",
    "\n",
    "Therefore from an ethical and practical perspective it is important not to simply run the script at full speed, but to artificially slow it down a little.\n",
    "\n",
    "We can also use this opportunity to provide ourselves with a little more insight into what is going on in the script as it runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from random import randint\n",
    "\n",
    "max_page = 3\n",
    "data = []\n",
    "for page_no in range(1, max_page+1):\n",
    "    print(f'Now retrieving page {page_no}')\n",
    "    \n",
    "    url = f'https://uberpeople.net/forums/Tips/page-{page_no}'\n",
    "    \n",
    "    response = requests.get(url, headers={'user-agent':choice(agents)})\n",
    "    page_data = page_info_extractor(response)\n",
    "    \n",
    "    data.extend(page_data)\n",
    "    \n",
    "    wait_time = randint(2,8) # randomly select an integer between 2 and 8\n",
    "    print(f'Waiting {wait_time} seconds...')\n",
    "    \n",
    "    sleep(wait_time)\n",
    "print('Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll now save our gathered data to disk to use later\n",
    "\n",
    "df.to_csv('my_uber_df.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:teaching]",
   "language": "python",
   "name": "conda-env-teaching-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
