{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SC207 - Session 3\n",
    "# Exploring, structuring and visualising data with Pandas\n",
    "<img src=\"https://github.com/Minyall/sc207_materials/blob/master/images/python_pandas.jpg?raw=true\" align=\"right\">\n",
    "\n",
    "\n",
    "- A major part of computational social science is the storing, manipulation and reporting of data. \n",
    "- Pandas is a powerful data management library specifically built for these kinds of tasks.\n",
    "- It can handle very large amounts of data whilst remaining quick and responsive.\n",
    "\n",
    "- We will be using Pandas throughout our practical sessions as a general purpose data management tool but this week we will focus on learning its features.\n",
    "\n",
    "[__Pandas Documentation__](http://pandas.pydata.org/pandas-docs/stable/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/Minyall/sc207_materials/blob/master/images/spotify.png?raw=true\" align=\"right\" width=150>\n",
    "\n",
    "### The Data\n",
    "\n",
    "Today we will be using two sets of data.\n",
    "\n",
    "1. The first dataset we will use for demonstration purposes is from Spotify. Spotify provides access to some of its data through their API. For this session we will be using data from Spotify, collected from a number of playlists including mixed pop, UK top 50, all out decades etc.\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/Minyall/sc207_materials/blob/master/images/RMS_Titanic_3.jpg?raw=true\" align=\"right\" width=150>\n",
    "\n",
    "2. The second dataset, to be used for the exercises, is the Titanic Dataset. A common dataset used for data science courses, the Titanic dataset provides the details of the passengers on the Titanic during its catastrophic last voyage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "Importing modules in python is standard practice. Rather than everyone create their own unique code from scratch every time, modules allow us to integrate code developed by others into our own work. In most instances it is better to use a well supported pre-existing library than to write your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we import the `pandas` module and the seaborn module. We could simply use `import pandas` however `as` allows us to use a shorter name.\n",
    "# As social convention many modules are referred to with these short names.\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this line is referred to as notebook 'magic' # and allows Jupyter notebooks to show \n",
    "# generated graphs and plots 'inline' within the notebook itself.\n",
    "# this does not necessarily work in other programs and is exclusive to Jupyter and related software.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "- [1 - Importing Data](#importing)\n",
    "- [2 - Anatomy of a DataFrame](#anatomy)\n",
    "- [4.1 - Basic Descriptives](#descriptives)\n",
    "- [4.2 - Time Series](#timeseries)\n",
    "- [4.3 - Exporting Data](#export)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='importing'></a>\n",
    "\n",
    "## Importing Data\n",
    "Often (though not always) you will be importing data from a file into Pandas. Pandas can handle a range of import types...\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/Minyall/sc207_materials/blob/master/images/pandas_import.png?raw=true\">\n",
    "\n",
    "[Source - Pandas Documentation](https://pandas.pydata.org/pandas-docs/stable/io.html)\n",
    "\n",
    "...some will be familiar to you, others less so.\n",
    "\n",
    "Today's data is stored as a __CSV file__, a common format for storing data in a simple way that can be read by lots of different programs including text readers, Microsoft Excel, etc.\n",
    "\n",
    "We need to provide either the relative or full path to the file so Python knows where in your computer to look. If the file is in the same place as your notebook you can provide the *relative* path which is the file path relative to the notebook. In our case the path is simply `spotify_top_songs.csv`. \n",
    "\n",
    "Whilst you're still learning it is best to just keep all relevant files in the same folder as your notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'spotify_top_songs.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load your saved data as variable df, short for DataFrame\n",
    "\n",
    "df = pd.read_csv(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head() # The Head of the dataframe is the top 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail() # Whilst the tail is the last 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the type of `df`\n",
    "# having loaded in the data what we have as variable `df` is a Pandas DataFrame object. \n",
    "# DataFrames are the main data structure of Pandas and have many associated methods to\n",
    "# help you manipulate and explore data.\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Anatomy of a DataFrame\n",
    "<img src=\"https://github.com/Minyall/sc207_materials/blob/master/images/pandas_df.png?raw=true\" align=\"right\" width=600>\n",
    "\n",
    "- DataFrames are like big excel spreadsheets with rows of data seperated into different columns. You can imagine a Dataframe as a collection of lists. In our Dataframe we have a track_name list, an artists list, a genre list etc. \n",
    "\n",
    "\n",
    "- What is important is that the items in each of these lists stays in the right order so that you always have the right track name in the same position as the right artist name and genre etc. Otherwise the data will get muddled up and be useless. \n",
    "\n",
    "\n",
    "- Pandas handles this for us, ensuring that the right values are always together, no matter how much you re-sort, reshape and manipulate the data.\n",
    "\n",
    "\n",
    "- The labels for each column are contained in `df.columns`. Each column of data is split up into rows, and each row also has a label called the 'index', accessible via `df.index`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our column names are what we would expect\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# often we want to access an individual series (column) of the Dataframe, this is done using a 'key' similar to a dictionary\n",
    "\n",
    "df['track_name']\n",
    "\n",
    "# Note that in Jupyter if there is too much data to display \n",
    "# it provides a condensed version - the first and last 30 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also select a subset of columns by providing a list of column names as the key.\n",
    "\n",
    "subset = ['track_name','artists','genre']\n",
    "\n",
    "df[subset].head() # use .head() just to keep it tidier to view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our index is known as a RangeIndex because it is just a range of numbers, 0-50. \n",
    "# These numbers act as the identifiers for each row.\n",
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can select individual rows from our index by using the '.loc' method.\n",
    "\n",
    "df.loc[1] # this provides us the row at index number 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An index doesn't have to be a set of numbers. For example we could make our track names our index.\n",
    "throwaway_df = df.copy() # Make a copy of the dataframe so we leave our original data alone.\n",
    "\n",
    "# This can be done in two ways \n",
    "throwaway_df.index = throwaway_df['track_name'] #replace the index with the track_name series\n",
    "throwaway_df = throwaway_df.set_index('track_name') #replace the index with the track_name series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "throwaway_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again with this index we would need to pass the index value to select the row we want,\n",
    "# but this time it is a string rather than a number....\n",
    "\n",
    "throwaway_df.loc[#track name as string here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also have the option of using .iloc if we still want to select by a row's position rather than its index label\n",
    "\n",
    "throwaway_df.iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the time you will be working primarily with entire Series or subsets of the data rather than drilling down to individual rows, but it is useful to know it is possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we want a specific value from a specific row we provide the column name to .loc\n",
    "# or the column index position to .iloc as well...\n",
    "\n",
    "print(throwaway_df.iloc[1,2])\n",
    "print(throwaway_df.loc[#track name as string here# ,'artists'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also provide the column index/name as a seperate key that accesses the result.\n",
    "print(throwaway_df.iloc[1][2])\n",
    "print(throwaway_df.loc['Last Christmas']['artists'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Exercises*\n",
    "## Loading and Inspecting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Your dataset for the exercises is `titanic.csv`. Load the dataset as a Pandas Dataframe \n",
    "# and assign it to the variable `titanic`\n",
    "\n",
    "titanic = pd.read_csv('titanic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. How many rows are there in the dataset? Work out your answer in this cell, and check it in the cell below.\n",
    "\n",
    "titanic.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2a. Assign your answer to the variable `num_records`\n",
    "\n",
    "num_records = 887\n",
    "\n",
    "assert num_records == titanic.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. What are the ages of the last 5 people in the dataset? \n",
    "\n",
    "titanic.tail()['Age']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3a. Once you have your answer make a list called last_5_ages.\n",
    "\n",
    "last_5_ages = [27,19,7,26,32]\n",
    "\n",
    "\n",
    "assert pd.np.mean(last_5_ages) == 22.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4a. Set the index of `titanic` to use the 'Name' column and assign the newly arranged dataframe to `titanic_by_name`\n",
    "\n",
    "titanic_by_name = titanic.set_index('Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4b. Using your `titanic_by_name` dataframe, \n",
    "# locate the row for 'Mr. Howard Hugh Williams' and assign \n",
    "# his age to the variable `age` and the fare he paid to `fare`.\n",
    "\n",
    "\n",
    "\n",
    "age = titanic_by_name.loc['Mr. Howard Hugh Williams','Age']\n",
    "fare = titanic_by_name.loc['Mr. Howard Hugh Williams','Fare']\n",
    "\n",
    "\n",
    "assert(fare / age == 0.28750000000000003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Cleaning\n",
    "<img src=\"https://github.com/Minyall/sc207_materials/blob/master/images/DC_cleaning_data.png?raw=true\" align=\"right\" width=150 href=https://www.datacamp.com/courses/cleaning-data-in-python>\n",
    "\n",
    "\n",
    "\n",
    "- Depending on how you sourced your data, it may have missing values, duplicates or anomalies. \n",
    "\n",
    "- There are a number of techniques we can use to handle these issues. The theory of handling missing data is itself a big topic particularly in Quantitative methods and cannot be covered in great detail here.\n",
    "\n",
    "- More information can be found in the DataCamp course ['Cleaning Data with Python'](https://www.datacamp.com/courses/cleaning-data-in-python). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first steps when reviewing any new data should be to get a bird-eye view. \n",
    "# There are a few simple methods built into pandas that can help us.\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.info()` tells us how many rows are in the dataframe, what the column names are, and how many filled values each column has.\n",
    "\n",
    "This quick overview tells us that some have missing values, and a range of data types.\n",
    "\n",
    "- object = often a column of strings, lists or a dictionaries but can be any data object.\n",
    "\n",
    "- bool = Boolean True or False column.`\n",
    " \n",
    "- int64 = integers\n",
    " \n",
    "- float64 = floats\n",
    "\n",
    "Other notable types which we'll cover later include...\n",
    "\n",
    "- category\n",
    "\n",
    "- datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.describe()` provides a very quick and dirty way to get some insights into the numerical columns in your data covering...\n",
    "- Count of records\n",
    "- Mean value\n",
    "- Standard Deviation\n",
    "- Minimum Value\n",
    "- 25th / 50th / 75th percentiles\n",
    "- Maximum Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may already be getting a sense of how DataFrames rely a lot on methods built in to the DataFrame object. Pandas has A LOT of methods. You will find youself referring to [the documentation](https://pandas.pydata.org/pandas-docs/stable/) and Cheat Sheets often when you first start. This is normal!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we look at our data we can see that 'danceability' looks like a float, but it didn't turn up in the summary statistics above..\n",
    "# Let's check it.\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see the type for this column is object rather than float.\n",
    "# this might cause problems later because it's being treated\n",
    "# as a string, not a number.\n",
    "df['danceability'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can do this by exchanging our current series df['danceability'] for a version of df['danceability'] that \n",
    "# has been transformed into float type data.\n",
    "# We do this using the method .astype()\n",
    "\n",
    "df['danceability'] = df['danceability'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now if we check our .info() again we can see the type of the genre series has changed.\n",
    "# reassigning a new series to df['genre'] is a permanent change to the dataframe\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's also worth checking for duplicate data. Spotify provides a unique id number for every track which can be helpful in our case.\n",
    "# Pandas has the .duplicated() and .drop_duplicates() methods for this very issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see if there are duplicates using .duplicated() which creates a boolean 'filter' for our data..\n",
    "# We'll cover these in more detail in the next section.\n",
    "\n",
    "dupe_filter = df.duplicated(subset=['track_id'])\n",
    "df[dupe_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We could even test this by manually looking for those tracks. \n",
    "# Remember when we set our index to be the track name?\n",
    "\n",
    "# by doing our operation without assigning it to a variable we can \n",
    "# just view the result without necessarily storing it.\n",
    "# chaining methods allows us to achieve this with one line. Think through each stage of the chain.\n",
    "# What does each stage produce and pass on to the next method?\n",
    "\n",
    "df.set_index('track_name').loc[#Track name here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's drop those duplicates. We also need to reset the index after because .drop_duplicates() will \n",
    "# remove rows leaving us with gaps in our index - e.g. 1,2,4,5,6,8,10.\n",
    "# Again we can achieve this by chaining together the methods.\n",
    "\n",
    "df = df.drop_duplicates(subset=['track_id']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets check how much data we have left\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We might also be concerned about missing data. \n",
    "# \"Speechiness\" has one less value than all the other columns. This inidcates that one row has a missing value\n",
    "# We can locate this by making a filter using .isna()\n",
    "\n",
    "missing_filter = df['speechiness'].isna()\n",
    "df[missing_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could replace this value with something... perhaps based on the average speechiness of the genre?\n",
    "# but as it is only one record we'll just drop it from analysis using .dropna()\n",
    "\n",
    "# dropna requires the subset (the columns we're basing our selections on, to be a list)\n",
    "# we say axis = 'index' to say drop rows that have a missing value in speechiness\n",
    "# if we say axis='column' it will drop the entire speechiness column if there is a single missing value!\n",
    "df = df.dropna(subset=['speechiness'], axis='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sometimes even if Pandas doesn't THINK there is missing data, depending on how the data was created it could still be missing.\n",
    "# Whilst df.info() seems to indicate all is well there is an oddity with our 'genre' column.\n",
    "# for data like our genre column we can use value counts to count the number of instances of different genres\n",
    "df['genre'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like whoever generated the data decided to put `***OOPS!***` in as a value if the genre wasn't available.\n",
    "Unfortunately as that is a string like all the other genres, Pandas doesn't know any different.\n",
    "In order for Pandas to understand that it is missing data, we need to replace it with a special object called a NaN.\n",
    "\n",
    "We can get one of these NaN's using Pandas `pd.np.nan` and using the `.replace()` method replace all the `***OOPS!***` with `pd.np.nan`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['genre'] = df['genre'].replace(to_replace='***OOPS!***', value=pd.np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets save this dataset now it's been cleaned\n",
    "\n",
    "df.to_csv('cleaned_spotify_top_songs.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Exercises*\n",
    "## *Data Cleaning*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load in the file 'titanic_to_clean.csv' using .read_csv and assign to the variable titanic. Examine the info().\n",
    "\n",
    "titanic = pd.read_csv('titanic_to_clean.csv',)\n",
    "titanic.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1b. Our first issue is the missing data for the fare column. \n",
    "# Use .dropna() providing the 'Fare' column as the subset. Assign the result to\n",
    "# `titanic` to overwrite it.\n",
    "\n",
    "titanic = titanic.dropna(subset=['Fare'])\n",
    "\n",
    "\n",
    "\n",
    "assert titanic.shape[0] == 853"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2a Let's check the 'Survived' column to see what it looks like.\n",
    "# Count the number of each value in 'Survived' to get an overall picture of the values used.\n",
    "titanic['Survived'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2b. We should only have two categories 0 and 1 to indicate if the individual survived. \n",
    "# Replace any of the values in 'Survived' that are neither a 0 nor 1, with a nan object.\n",
    "\n",
    "titanic['Survived'] = titanic['Survived'].replace(3, pd.np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2c. How many non null values are there in the 'Survived' column now?\n",
    "# Assign your answer to `survived_non_null`\n",
    "\n",
    "survived_non_null = 831\n",
    "\n",
    "assert survived_non_null == titanic.Survived.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2d. Drop the rows that have now have null values in the 'Survived' column\n",
    "\n",
    "titanic.dropna(subset=['Survived'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2e. Finally it would make more sense for us to make the 'Survived' column a Boolean column (True/False)\n",
    "# Pandas will work out that 0 is False whilst 1 is True if we convert it to boolean. Do this now.\n",
    "titanic['Survived'] = titanic['Survived'].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3a. Check for any duplicate records. What column do you think would be best to use for the subset?\n",
    "\n",
    "duplicate_filter = titanic.duplicated(subset=['Name'])\n",
    "titanic[duplicate_filter]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "titanic = titanic.drop_duplicates(subset=['Name'])\n",
    "titanic.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3b. Finally drop the duplicate records, check how many records you have in your cleaned dataset, \n",
    "# and assign the value to `final_countdown`.\n",
    "\n",
    "final_countdown = 815\n",
    "\n",
    "assert final_countdown == titanic.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Filtering our Data\n",
    "\n",
    "Filtering Data allows you to select portions of your dataset based on particular conditions, such as all the tracks of a particular genre, or all tracks under a particular duration, or selecting tracks with a particular word in their name.\n",
    "\n",
    "Filtering uses a particular syntax...\n",
    "\n",
    "`df[filter_rule]`\n",
    "\n",
    "We already used this when we showed all the records that had duplicates in our data...\n",
    "\n",
    "    dupe_filter = df.duplicated(subset=['track_id'])\n",
    "    df[dupe_filter]\n",
    "\n",
    "The rule that we use within the brackets can be relatively simple or complex depending on the kinds of queries we have.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload our cleaned data\n",
    "\n",
    "df = pd.read_csv('cleaned_spotify_top_songs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see what a filter is doing first...\n",
    "\n",
    "df['popularity'] > 90\n",
    "\n",
    "# We can see that the conditional statement creates a new series that has evaluated \n",
    "# whether the statement is True or False for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we place this series inside the square brackets of a call to our df variable \n",
    "# it acts as a filter, showing only those rows that have a value of True.\n",
    "\n",
    "pop_filter = df['popularity'] > 90 # same statement as above assigned to a variable.\n",
    "\n",
    "high_pop = df[pop_filter]\n",
    "high_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can filter any way that we can create a conditional statement and we don't necessarily need to assign the filter rule to another variable first...\n",
    "\n",
    "df[df['explicit'] == False].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can combine conditional statements with...\n",
    "# & (and)\n",
    "# | (or) - note this is a vertical 'pipe' not an I\n",
    "\n",
    "# Perhaps we want all explicit tracks with a popularity above 90\n",
    "\n",
    "explicit_over_90 = (df['explicit'] == True) & (df['popularity'] > 90)\n",
    "\n",
    "df[explicit_over_90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use the | OR operator to get the very highest and very lowest in popularity\n",
    "\n",
    "high_low = (df['popularity'] >90) | (df['popularity'] < 10)\n",
    "df[high_low]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating New Data Combinations\n",
    "\n",
    "Pandas allows you to quickly make new data series in your DataFrame, either by providing the values yourself, or by combining your existing data. Unlike filtering to create particular views on the data, these operations add the new data to the DataFrame stored in memory. If you want to keep the columns permanently you'll need to save the data which is covered under 'Exporting'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to create our rule and to understand this we need to understand 'Broadcasting'.\n",
    "\n",
    "# Pandas relies on 'broadcasting', which is the ability to apply an operation to\n",
    "# all values in a series or entire dataframe at the same time. Rarely is it necessary to loop over the\n",
    "# rows to make changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple example would be just adding a new column with a single value...\n",
    "\n",
    "df['source'] = 'Spotify'\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# However we normally would want different values for different rows\n",
    "\n",
    "# Say we wanted to change our 'duration' column from miliseconds to seconds to make it more meaningful to us.\n",
    "# The conversion from miliseconds to seconds is to divide by 1000.\n",
    "\n",
    "# We can do this in one line and make this a new column...\n",
    "\n",
    "df['duration_s'] = df['duration_ms'] //1000\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perhaps we wanted to make a new value by dividing a song's danceability by its speechiness, perhaps believing that\n",
    "# the most danceable songs are non-vocal.\n",
    "\n",
    "# We can add this to the Dataframe by assigning it a column name. \n",
    "\n",
    "df['pure_danciness'] = df['danceability'] / df['speechiness']\n",
    "df.sort_values('pure_danciness', ascending=False).head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning it to a new variable creates a seperate independent series that isn't in the dataframe.\n",
    "# Useful if you don't need the series in the DataFrame.\n",
    "\n",
    "new_score = df['danceability'] / df['speechiness']\n",
    "print(type(new_score))\n",
    "new_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcasting also works with string operations....\n",
    "\n",
    "combi_artist_genre = df['artists'] + ' - ' + df['genre']\n",
    "combi_artist_genre.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualising\n",
    "Pandas has a range of built in quick visualisation options, accessed through the `.plot()` method. They are built on top of another library called `matplotlib`. This is a very large library in Python which we won't cover much here, but it is key to understand the Pandas uses matplotlib, because you can tweak Pandas visuals using matplotlib commands.\n",
    "\n",
    "However `Seaborn` is a visualisation library that has much more flexibility and is easier to use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll import maptplotlib's python library here to use it to tweak our plots\n",
    "\n",
    "import matplotlib.pyplot as plt # this is the conventional way to import matplotlib\n",
    "import seaborn as sns # and this is the conventional way to import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.read_csv('titanic.csv')\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets investigate ages for the titanic to demonstrate basic Pandas plotting\n",
    "\n",
    "\n",
    "\n",
    "titanic['Age'].plot(kind='hist', # There are many kinds of plot we can use built into Pandas\n",
    "                    title='Distribution of Ages of Passengers on the Titanic', #Allows us to set the title\n",
    "                    figsize=(6,4)) # we can also adjust the size (h,v)\n",
    "\n",
    "# we can also relabel the x-axis but we need to use matplotlib to do this\n",
    "plt.xlabel('Age')\n",
    "# and to cleanly display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind='scatter',y='popularity',x='speechiness')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seaborn Examples\n",
    "Seaborn allows us to easily split the data up by categories when visualising. Pandas can be used for this but it is much more complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=titanic, x='Age', y='Fare')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter plot with hue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=titanic, x='Age', y='Fare', hue='Survived')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Line Plot with hue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=titanic, x='Age',y='Fare', hue='Survived')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box plot with hue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxenplot(data=titanic, x='Sex',y='Age',hue='Survived')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(titanic['Age'],kde=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Swarm Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.swarmplot(data=titanic, x='Pclass', y='Fare', hue='Survived')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(data=titanic, x='Age', y='Fare')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=titanic, hue='Survived',vars=['Fare','Age', 'Pclass'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Grouping Data\n",
    "\n",
    "`.groupby()` is a convenient method of spliting up the data, applying an operation and then combining the results of the seperate operations.\n",
    "\n",
    "Let's pose a question - Are there differences between explicit and non-explicit songs? With `.groupby()` and `.describe()` we can examine this quite quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by calling `.groupby()` on our dataframe, and providing the column we want to group on,\n",
    "# we create a `groupby` object.\n",
    "\n",
    "groups = df.groupby('explicit')\n",
    "groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll aggregate the results by taking the .mean() of all the numerical columns\n",
    "# before we can view the data we need to say how we're going to 'aggregate' or summarise the data together.\n",
    "\n",
    "# here we take the mean or the average of the values\n",
    "groups.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we would prefer the columns and the rows to be swapped, we can .transpose()\n",
    "\n",
    "groups.mean().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups.mean().transpose().loc['popularity'].plot(kind='bar', title='Popularity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use Groupby to group on multiple variables\n",
    "\n",
    "titanic.groupby(['Pclass','Sex']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.agg is a method that lets us specify exactly what we want each column to do\n",
    "# we give it a dictionary where the key is the column name and the value is the aggregation approach.\n",
    "# pandas has built in functions so we just have to provide the names as strings.\n",
    "\n",
    "aggregation_rules = {'Age':'mean','Survived':'sum','Fare':'mean', 'Name':'count'}\n",
    "\n",
    "grouped_titanic = titanic.groupby(['Pclass','Sex']).agg(aggregation_rules)\n",
    "grouped_titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the result of a groupby is another Dataframe, \n",
    "# so we can carry on using it like normal\n",
    "type(grouped_titanic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets rename the columns to make them more reflective of the new values\n",
    "rename_rules = {'Age':'average_age',\n",
    "               'Survived':'n_survivors',\n",
    "               'Fare': 'average_fare',\n",
    "                'Name':'n_passengers'}\n",
    "\n",
    "grouped_titanic = grouped_titanic.rename(columns=rename_rules)\n",
    "grouped_titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets make a new column to determine the percentage of survivors per group\n",
    "\n",
    "grouped_titanic['pct_survivors'] = (grouped_titanic['n_survivors'] / grouped_titanic['n_passengers']) *100\n",
    "grouped_titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple way to plot this would be to use pandas built in bar chart\n",
    "grouped_titanic['pct_survivors'].plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_titanic_to_plot = grouped_titanic.reset_index() #resetting removes the 'multi-index'\n",
    "grouped_titanic_to_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=grouped_titanic_to_plot, x='Pclass', y='pct_survivors', hue='Sex', dodge=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='export'></a>\n",
    "## 6 - Exporting Data\n",
    "You may want to export your data, or a reshaped version of it, for use in other applications. Pandas can export to a range of data formats including CSV, Excel, Stata, HTML, JSON and others.\n",
    "\n",
    "See: https://pandas.pydata.org/pandas-docs/stable/api.html#id12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Our grouped titanic table makes a good candidate\n",
    "grouped_titanic_to_plot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "grouped_titanic_to_plot.to_excel('our_export.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Exercises*\n",
    "## *Grouping and Visualising*\n",
    "\n",
    "In this exercise we are going to explore the differences between the different playlist types in our Spotify dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.  First load your cleaned_spotify_top_songs.csv as df\n",
    "\n",
    "df = pd.read_csv('cleaned_spotify_top_songs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Using a Seaborn Pairplot (Histogram), visualise the relationships between release year\n",
    "# and popularity and colour by playlist_type, for all songs released after 2000.\n",
    "# Tip. You'll need to filter the df using standard pandas filtering, either beforehand or when you pass it to the pairplot\n",
    "\n",
    "\n",
    "sns.pairplot(data=df[df['release_year'] >=2000], vars=['release_year','popularity'], hue='playlist_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Now group the dataframe by 'playlist_type' and aggregate the values so that it returns the average (or mean) values for all columns\n",
    "# Do this in a single line.\n",
    "\n",
    "# Which playlist, on average, features the most popular songs?\n",
    "df.groupby('playlist_type').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 create a new table by first grouping on release_year and then use the .agg method to aggregate just the 'explicit' column using mean\n",
    "# assign the new series to the variable avg_explicit_year\n",
    "# tip, you'll need to reset the index to make the next step easier\n",
    "\n",
    "avg_explicit_year = df.groupby('release_year').agg({'explicit':'mean'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Now using a seaborn lineplot, examine whether songs have become more explicit over time.\n",
    "sns.lineplot(data=avg_explicit_year, x='release_year', y='explicit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Using broadcasting create a new boolean column where each row is True if \n",
    "# the song has a popularity greater than or equal to 90.\n",
    "# name the column 'top_song'\n",
    "\n",
    "df['top_song'] = df['popularity'] >= 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Use a boxplot to explore the relationship between three dimensions of the data, top_song, danceability, and explicit.\n",
    "# Tip: x and y will provide you two dimensions, how might you add a third?\n",
    "\n",
    "sns.boxplot(data=df, x='top_song', y='danceability', hue='explicit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now  group the df using 'release_year' and 'playlist_type' and aggregate using the mean\n",
    "# assign the result to df_year_avg. Don't forget to reset your index\n",
    "\n",
    "df_year_avg = df.groupby(['release_year','playlist_type']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally answer these questions?\n",
    "\n",
    "# What was the average popularity for non-explicit songs in 2019? # this can be answered using one line of code\n",
    "# How does this compare all songs in our sample in or after 2010? - Demonstrate this using a visualisation. # this may need two lines\n",
    "\n",
    "df.groupby(['release_year','explicit']).mean().loc[2019]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot = df.groupby(['release_year','explicit']).mean().reset_index()\n",
    "sns.lineplot(data=to_plot[to_plot['release_year'] >= 2000], x='release_year', y='popularity', hue='explicit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
