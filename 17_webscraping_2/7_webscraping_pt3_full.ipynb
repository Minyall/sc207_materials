{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WebScraping\n",
    "## 6. Joining Thread Data and Text\n",
    "We're now ready to connect together the work put in gathering thread data from last week, and the text extraction from this week.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse\n",
    "import pandas as pd\n",
    "from random import randint\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thread functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_info_extractor(row): # We'll feed it the isolated html for a row and let it pull it apart.\n",
    "    author = row['data-author']\n",
    "    \n",
    "    id_item = row['class'][-1]\n",
    "    thread_id = int(id_item.split('-')[-1])\n",
    "    \n",
    "    title_div = row.find('div', class_='structItem-title')\n",
    "    title = title_div.a.text.strip() # remember to .strip() off the useless spaces on the ends.\n",
    "    \n",
    "    date = row.find('time')['datetime']\n",
    "    \n",
    "    views = row.find('dl',class_='pairs pairs--justified structItem-minor').dd.text\n",
    "\n",
    "    relative_url = title_div.a['href']\n",
    "    full_url = urllib.parse.urljoin('http://uberpeople.net',relative_url)\n",
    "    \n",
    "    data_package = {'id': thread_id,\n",
    "                  'author': author,\n",
    "                  'title': title,\n",
    "                  'date': date,\n",
    "                  'views': views,\n",
    "                  'url': full_url}\n",
    "    \n",
    "    return data_package\n",
    "\n",
    "def page_info_extractor(response):\n",
    "    soup = BeautifulSoup(response.text,'lxml')\n",
    "    threads_container = soup.find('div', class_=\"structItemContainer\")\n",
    "    threads = threads_container.find_all('div',class_='structItem--thread')\n",
    "    \n",
    "    page_data = []\n",
    "    for row in threads:\n",
    "        result = row_info_extractor(row)\n",
    "        page_data.append(result)\n",
    "    return page_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_extractor(post):\n",
    "    post_content = post.find('article', class_='message-body')\n",
    "    quotes = post_content.find_all('blockquote', class_='bbCodeBlock--quote') \n",
    "    \n",
    "    if quotes is not None: \n",
    "        for quote in quotes:\n",
    "            quote.decompose()\n",
    "    return post_content.text.strip()\n",
    "\n",
    "def page_posts_extractor(response):\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    post_container = soup.find('div', class_='p-body-content')\n",
    "    posts = post_container.find_all('article', class_='message')\n",
    "    texts = []\n",
    "    for post in posts:\n",
    "        extracted = text_extractor(post)\n",
    "        texts.append(extracted)\n",
    "    return texts\n",
    "\n",
    "def get_next_url(response):\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    next_button = soup.find('a', class_='pageNav-jump--next')\n",
    "    if next_button == None:\n",
    "        result = None\n",
    "    else: \n",
    "        result = next_button['href']\n",
    "    return result\n",
    "\n",
    "def thread_post_extractor(url):\n",
    "    \n",
    "    thread_text_data = []\n",
    "    original_url = url\n",
    "    condition = True\n",
    "\n",
    "    while condition:\n",
    "        response = requests.get(url) # use the url variable currently in memory\n",
    "        print(response.url)\n",
    "\n",
    "        post_texts = page_posts_extractor(response)\n",
    "        thread_text_data.extend(post_texts)\n",
    "\n",
    "        next_url = get_next_url(response)\n",
    "\n",
    "        if next_url is not None: # if there is a next url...\n",
    "            url = urllib.parse.urljoin(original_url,next_url) # overwrite the url variable with the url from the next button\n",
    "            # return to the beginning of the loop with the new url in memory\n",
    "\n",
    "        else: # however if there is no next button...\n",
    "            condition = False #set condition to False\n",
    "    thread_text = '\\n\\n****\\n\\n'.join(thread_text_data)\n",
    "    return thread_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DF Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXED MISSING VALUES!!\n",
    "def view_fixer(view_string):\n",
    "    view_string = view_string.replace('K','000')\n",
    "    view_string = view_string.replace('â€“', '0')\n",
    "    view_integer = int(view_string)\n",
    "    return view_integer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading in our Thread Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a. If we don't have much time!\n",
    "Load in our Dataframe we created last week, and sample three random rows using `.sample()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('my_uber_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ...now skip to part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a. If we have time - Let's recap from the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_page = 3\n",
    "data = []\n",
    "for page_no in range(1, max_page+1):\n",
    "    print(f'Now retrieving page {page_no}')\n",
    "    \n",
    "    url = f'https://uberpeople.net/forums/Tips/page-{page_no}'\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    page_data = page_info_extractor(response)\n",
    "    \n",
    "    data.extend(page_data)\n",
    "    \n",
    "    wait_time = randint(2,8) # randomly select an integer between 2 and 8\n",
    "    print(f'Waiting {wait_time} seconds...')\n",
    "    \n",
    "    sleep(wait_time)\n",
    "print('Finished!')\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['views'] = df['views'].apply(view_fixer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it for later!\n",
    "\n",
    "df.to_pickle('my_uber_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Gathering Post Text and Linking it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets take the first row of our dataframe to work with\n",
    "\n",
    "first_row_index = 0\n",
    "first_row = df.loc[first_row_index]\n",
    "first_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic functionality of our script will be...\n",
    "\n",
    "- Take a row from our df of threads\n",
    "- Get the url from that row\n",
    "- Feed that url to our `thread_post_extractor()` function to get the whole text from the thread.\n",
    "- Assign that set of text as the value in a 'text' column, at that particular row position in our dataframe\n",
    "- Repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To demonstrate on one row\n",
    "\n",
    "# we get the url\n",
    "thread_url = first_row['url']\n",
    "\n",
    "# we get the text\n",
    "thread_text = thread_post_extractor(thread_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we assign thread_text to the text column at that row position.\n",
    "# we don't have a text column yet but assigning a value to it will create the column\n",
    "# we want to assign this text back to our original dataframe\n",
    "\n",
    "df.loc[first_row_index,'text'] = thread_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas .iterrows()\n",
    "`iterrows` meaning 'iterate over rows'.\n",
    "\n",
    "Although generally it is faster to use broadcasting rather than loop over dataframe rows, sometimes it can be useful. We do this with `.iterrows()` which for every loop returns two things\n",
    "- the value of the index i.e. the row number \n",
    "- the rest of the row data\n",
    "\n",
    "So rather than having...\n",
    "\n",
    "```\n",
    "for item in our_iterable:\n",
    "    do something with item.\n",
    "```\n",
    "\n",
    "We have\n",
    "\n",
    "```\n",
    "for index_value, rest_of_row_data in our_iterable:\n",
    "    do something with our index_value\n",
    "    do something else with rest_of_row_data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to demonstrate lets create a random sample of 3 rows\n",
    "\n",
    "sample = df.sample(3)\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for row_index, row_data in sample.iterrows():\n",
    "    print(f\"This is the row index: {row_index}\")\n",
    "    print(f\"This is the url: {row_data['url']}\")    \n",
    "    print(f\"This is the title: {row_data['title']}\")\n",
    "    print(f\"This is the author: {row_data['author']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so we can use this iteration to automate our text collection row by row\n",
    "\n",
    "\n",
    "\n",
    "for row_number, row_data in sample.iterrows():\n",
    "    thread_url = row_data['url']\n",
    "    \n",
    "    # remember we want to assign the text back to the original dataframe, not the sample view.\n",
    "    \n",
    "    df.loc[row_number,'text'] = thread_post_extractor(thread_url)\n",
    "    wait_time = randint(1,3)\n",
    "    print(f'Waiting {wait_time} seconds...')\n",
    "    sleep(wait_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can now check how many rows of our dataframe have text data\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see which rows have text using .notna()  ... The reverse of .isna()\n",
    "\n",
    "has_text_filter = df['text'].notna()\n",
    "df[has_text_filter]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and we can see which rows still need text by using .isna()\n",
    "\n",
    "without_text_filter = df['text'].isna()\n",
    "df[without_text_filter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Fully Automated Luxury Webscraping\n",
    "\n",
    "We can use these filters, alongside saving and loading to disk, to build ourselves a semi-automated controlled system. First, above, you would gather your threads data to give yourself a list of threads which then need filling in with text. We chose to gather 3 pages worth, which gave us a total of 60 threads.\n",
    "\n",
    "Our semi-automated system should go like this....\n",
    "\n",
    "1. Load our threads dataframe from disk\n",
    "2. Define how many rows we're going to gather text for in this session\n",
    "3.\n",
    "  - `If` our dataframe doesn't yet have a column called 'text' then just grab the top however many rows we're collecting as a sample.\n",
    "  - `Else` we want to filter out rows that already have text, and then take the first however many rows we're collecting as a sample.\n",
    " \n",
    "4. We iterate over the sample rows collecting the text, and then assigning it to the 'text' column  at that row position in our original dataframe (not the sample).\n",
    "5. When we are done we save the orignal dataframe back to disk, ready to repeat at our leaisure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in your data \n",
    "df = pd.read_pickle('my_uber_df.pkl') # we load in any saved data\n",
    "\n",
    "# how many rows will we do this session?\n",
    "chunk_size = 3\n",
    "\n",
    "# if we've already started collecting text we want to only sample from rows that don't have text\n",
    "# (otherwise we'll gather than same text over and over)\n",
    "\n",
    "if 'text' in df: # this says if the column 'text' is in the dataframe...\n",
    "    filtered = df[df.text.isna()] # select only rows where the text is missing\n",
    "    sample = filtered.head(chunk_size) #our sample is set as the first rows of the filtered dataframe, as determined by our chunk size\n",
    "\n",
    "else: #if text isn't a column in the df...\n",
    "    sample = df.head(chunk_size) # ...forget the filtering and just grab the first rows.\n",
    "    \n",
    "\n",
    "try:\n",
    "    for row_number, row_data in sample.iterrows():\n",
    "        thread_url = row_data['url']\n",
    "        \n",
    "        # despite the filtering and sampling we still update the original df with our text data\n",
    "        df.loc[row_number,'text'] = thread_post_extractor(thread_url) \n",
    "        wait_time = randint(1,3)\n",
    "        print(f'Waiting {wait_time} seconds...')\n",
    "        sleep(wait_time)\n",
    "finally:\n",
    "    df.to_pickle('my_uber_df.pkl')\n",
    "print('**DONE!!**')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_text_filled_rows = len(df[df['text'].notna()])\n",
    "print(f'We have collected text for {num_text_filled_rows} of {len(df)} rows in this DataFrame')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the above again to see how it adds to the data rather than repeats itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting\n",
    "If you want to use the text information in a different software package such as NVivo or MaxQDA then you just need to export as a CSV or an excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('my_uber.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clear timezone data so you can export to excel for nvivo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = df['date'].apply(lambda x: x.replace(tzinfo=None)) #excel doesn't like timezones!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('my_uber.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:teaching]",
   "language": "python",
   "name": "conda-env-teaching-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
