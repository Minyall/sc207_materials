{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WebScraping\n",
    "## Retrieving Text Content\n",
    "To get started lets's take a url of a thread.\n",
    "\n",
    "https://uberpeople.net/threads/fight-for-equity-in-uber-not-to-become-an-employee.30393/  \n",
    "\n",
    "Our main priority here is to retrieve the text from each post within the thread.\n",
    "\n",
    "Things you might want to consider before you begin.\n",
    "- Do I want to retrieve just the entire thread text or be able to seperate the text into individual posts?\n",
    "- Some posts in the thread have quotes, am I keeping those or removing them?\n",
    "- Do I want to also track user information and date information at a per post level?\n",
    "\n",
    "For this session we will just focus on retrieving the text from the entire thread, and we'll remove quotes to demonstrate how to exclude content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "with open('user_agent.txt','r') as f:\n",
    "    agents = f.readlines()\n",
    "    agents = [x.strip() for x in agents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Retrieving (and cleaning) a Single Post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- If we inspect the page we can see posts are contained within a divison with the class `'block-body js-replyNewMessageContainer'`\n",
    "- Within that division are a series of `article` elements, which seem to correspond with the posts.\n",
    "- There are also divisions which are not articles, which appear to be adverts.\n",
    "- If we narrow down to the division that contains the posts, and then find all `article` elements with the class `message` we should avoid the adverts..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should be familiar by now\n",
    "\n",
    "test_url = 'https://uberpeople.net/threads/fight-for-equity-in-uber-not-to-become-an-employee.30393/'\n",
    "response = requests.get(test_url, headers={'user-agent':choice(agents)})\n",
    "soup = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We narrow down to the division containing the article elements\n",
    "post_container =  'block-body js-replyNewMessageContainer'\n",
    "\n",
    "# Then we find all article elements which will return us a list of posts.\n",
    "posts = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check the first post\n",
    "posts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To access just the text the most nested element that contains the text is another\n",
    "# article tag (nested within the original article tag) with the class 'message-body'\n",
    "\n",
    "posts[0].find()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks like this one could be as simple as simply asking for the text content of the container, \n",
    "# and cleaning it up with .strip()\n",
    "\n",
    "print(posts[0].find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# however if we want to remove quotes we need to remove them from our content...\n",
    "# Depending on your project you may want to keep quotes, \n",
    "# however for now we'll remove them as it is a good opportunity to demonstrate removing erroneous material\n",
    "\n",
    "posts[6].find('article', {'class':'message-body'}).text.strip() # post 6 has a quote\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quotes and the associated informaation like who said it are contained in a blockquote element nested inside our article element.\n",
    "# We can use Beautifulsoup's 'decompose' method to remove it.\n",
    "\n",
    "posts[6].find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the item again\n",
    "posts[6].find('article', {'class':'message-body'}).text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What happens if we try and decompose on a post without a quote?\n",
    "\n",
    "posts[0].find('blockquote', {'class':'bbCodeBlock--quote'}).decompose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We get an error because we're trying to run decompose on a `None` object because our `.find` method returns a `None` if it can't find what we're asking for.  We can use this to our advantage.\n",
    "\n",
    "- We can use the presence or absence of a `None` as a filter that allows us to control whether we attempt to decompose the quote or not.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to demonstrate here we just use the filter to show us only posts that contain a quote.\n",
    "\n",
    "for post in posts:\n",
    "    post_content = post.find('article', {'class':'message-body'})\n",
    "    post_quote = post_content.find('blockquote', {'class':'bbCodeBlock--quote'})\n",
    "    # print if a quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use this logic to clean posts if they need to be cleaned, or leave them if they don't.\n",
    "\n",
    "for post in posts:\n",
    "    print('**//**') # just a visual seperator to help us read seperate posts\n",
    "    post_content = post.find('article', {'class':'message-body'})\n",
    "    post_quote = post_content.find('blockquote', {'class':'bbCodeBlock--quote'})\n",
    "    # decompose if a quote\n",
    "    \n",
    "    print(post_content.text.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_extractor(post):\n",
    "    post_content = post.find('article', {'class':'message-body'})\n",
    "    post_quote = post_content.find('blockquote', {'class':'bbCodeBlock--quote'})\n",
    "    if post_quote is not None: \n",
    "        post_quote.decompose()\n",
    "    return post_content.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_post = posts[0]\n",
    "\n",
    "result = text_extractor(test_post)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving a page of posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posts_extractor(response):\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    post_container = soup.find('div', {'class':'block-body js-replyNewMessageContainer'})\n",
    "    posts = post_container.find_all('article', {'class':'message'})\n",
    "    \n",
    "    #extract posts on the page\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_url = 'https://uberpeople.net/threads/fight-for-equity-in-uber-not-to-become-an-employee.30393/'\n",
    "response = requests.get(test_url, headers={'user-agent':choice(agents)})\n",
    "\n",
    "posts_extractor(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving multiple pages of posts\n",
    "Often in forums or other sites, content will be paginated, meaning to get the full content we need to visit multiple urls and join the data together.\n",
    "To do this we also need to know what the next page url is. Luckily, if a user can click a button to go to the next page, that means the url is exposed for our scraper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_page_url = 'https://uberpeople.net/threads/will-my-no-shave-november-beard-screw-up-ubers-facial-recognition-software.219274/'\n",
    "multi_page_url = 'https://uberpeople.net/threads/worst-rider-experience-now-what.216997/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting single or multi-page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_response = requests.get(single_page_url, headers={'user-agent':choice(agents)})\n",
    "multi_response = requests.get(multi_page_url, headers={'user-agent':choice(agents)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single response returns nothing (None)\n",
    "BeautifulSoup(single_response.text, 'lxml').find('a', {'class':'pageNav-jump--next'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi_response returns an element containing the relative url of the next page\n",
    "BeautifulSoup(multi_response.text, 'lxml').find('a', {'class':'pageNav-jump--next'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can access this using ['href']\n",
    "rel_url = BeautifulSoup(multi_response.text, 'lxml').find('a', {'class':'pageNav-jump--next'}) # access the href attribute\n",
    "rel_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and intelligently rebuild the url by providing the source url, and the relative url to urllib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_page(response):\n",
    "    button = BeautifulSoup(response.text, 'lxml').find('a', {'class':'pageNav-jump--next'})\n",
    "    # return a useable url if next page exists, else return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_page(multi_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_page(single_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_page_post_extractor(url):\n",
    "    thread_text_data = []\n",
    "\n",
    "    while True:\n",
    "        response = requests.get(url, headers={'user-agent':choice(agents)}) # use the url variable currently in memory\n",
    "        print(response.url)\n",
    "\n",
    "        # extract posts from the page\n",
    "        \n",
    "        #check for next url else end\n",
    "    thread_text = \n",
    "    return thread_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(multi_page_post_extractor(multi_page_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:teaching]",
   "language": "python",
   "name": "conda-env-teaching-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
