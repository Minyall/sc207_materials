{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WebScraping\n",
    "## Putting it all together\n",
    "We're now ready to connect together the work put in gathering thread data, and the text extraction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse\n",
    "import pandas as pd\n",
    "from random import randint\n",
    "from time import sleep\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "with open('user_agent.txt','r') as f:\n",
    "    agents = f.readlines()\n",
    "    agents = [x.strip() for x in agents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "#### Thread Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_info_extractor(row): # We'll feed it the isolated html for a row and let it pull it apart.\n",
    "    author = row['data-author']\n",
    "    \n",
    "    id_item = row['class'][-1]\n",
    "    thread_id = int(id_item.split('-')[-1])\n",
    "    \n",
    "    title_div = row.find('div', class_='structItem-title')\n",
    "    title = title_div.a.text.strip() # remember to .strip() off the useless spaces on the ends.\n",
    "    \n",
    "    date_format = '%Y-%m-%dT%H:%M:%S%z'\n",
    "    date_string = row.find('time')['datetime']\n",
    "    date = datetime.strptime(date_string, date_format)\n",
    "    \n",
    "    relative_url = title_div.a['href']\n",
    "    full_url = urllib.parse.urljoin('http://uberpeople.net',relative_url)\n",
    "\n",
    "    data_package = {'author':author,\n",
    "                   'title':title,\n",
    "                   'thread_id':thread_id,\n",
    "                   'date':date,\n",
    "                   'url':full_url}\n",
    "    \n",
    "    return data_package\n",
    "\n",
    "def page_info_extractor(response):\n",
    "    soup = BeautifulSoup(response.text,'lxml')\n",
    "    threads_container = soup.find('div', class_=\"structItemContainer-group js-threadList\")\n",
    "    threads = threads_container.find_all('div', {'class':'structItem--thread', 'data-author':True} )\n",
    "    \n",
    "    page_data = []\n",
    "    for row in threads:\n",
    "        result = row_info_extractor(row)\n",
    "        page_data.append(result)\n",
    "    return page_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_extractor(post):\n",
    "    post_content = post.find('article', {'class':'message-body'})\n",
    "    post_quote = post_content.find('blockquote', {'class':'bbCodeBlock--quote'})\n",
    "    if post_quote is not None: \n",
    "        post_quote.decompose()\n",
    "    return post_content.text.strip()\n",
    "\n",
    "def posts_extractor(response):\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    post_container = soup.find('div', {'class':'block-body js-replyNewMessageContainer'})\n",
    "    posts = post_container.find_all('article', {'class':'message'})\n",
    "    texts = []\n",
    "    for post in posts:\n",
    "        extracted = text_extractor(post)\n",
    "        texts.append(extracted)\n",
    "    return texts\n",
    "\n",
    "def next_page(response):\n",
    "    button = BeautifulSoup(response.text, 'lxml').find('a', {'class':'pageNav-jump--next'})\n",
    "    if button is not None:\n",
    "        return urllib.parse.urljoin(response.url,button['href'])\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def multi_page_post_extractor(url):\n",
    "    thread_text_data = []\n",
    "\n",
    "    while True:\n",
    "        response = requests.get(url, headers={'user-agent':choice(agents)}) # use the url variable currently in memory\n",
    "        print(response.url)\n",
    "\n",
    "        post_texts = posts_extractor(response)\n",
    "        thread_text_data.extend(post_texts)\n",
    "        \n",
    "        url = next_page(response)\n",
    "        if url is None:\n",
    "            break\n",
    "    thread_text = '\\n\\n'.join(thread_text_data)\n",
    "    return thread_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 1: Gathering Thread Urls\n",
    "First we establish our list of threads. By establishing this before text extraction (which can be request heavy due to multiple pages) we can keep track of our data collection more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_page = 2\n",
    "data = []\n",
    "for page_no in range(1, max_page+1):\n",
    "    print(f'Now retrieving page {page_no}')\n",
    "    \n",
    "    url = f'https://uberpeople.net/forums/Tips/page-{page_no}'\n",
    "    \n",
    "    response = requests.get(url, headers={'user-agent':choice(agents)})\n",
    "    page_data = page_info_extractor(response)\n",
    "    \n",
    "    data.extend(page_data)\n",
    "    \n",
    "    wait_time = randint(2,8) # randomly select an integer between 2 and 8\n",
    "    print(f'Waiting {wait_time} seconds...')\n",
    "    \n",
    "    sleep(wait_time)\n",
    "print('Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df.to_csv('my_scraping.csv', index=False) # index false so it doesn't record the index as its own column in the csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2: Gathering Thread Text\n",
    "In the cell below we do a number of things designed to allow you to gather data in chunks with simply by running the cell.\n",
    "1. Load in the data, check if there is a `text` column, if not, make one and fill it with nans! (Should only run once)\n",
    "2. Set how many rows we'll collect in this run, and then select that many rows that don't have text, from the dataframe\n",
    "3. the try/finally block means that it will `try` to do whatever is inside the block. `finally`, whether there is an error or not, it will save the updated `df` to disk to avoid data loss.\n",
    "4. We iterate over the rows in our sample using `.iterrows()` which spits out an identifying `row_number` and the `row_data` itself.\n",
    "5. As the sample was made from the original df, they have the same index numbers for each row. We take the `sample`'s `row_number`, find that same row in the original `df` using `.loc` and then target the `text` column. We then assign the result of our `multi_page_post_extractor` to that cell in the `df`.\n",
    "6. We then wait a random amount of seconds before the loop starts again on the next row of the sample.\n",
    "7. `finally` (see point 3) once the loop is completed we save the updated `df` to disk. This means when the cell is run again, it will load the updated df, skip rows that already have text, and only collect for rows with missing text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "df = pd.read_csv('my_scraping.csv')\n",
    "if not 'text' in df:\n",
    "    from numpy import nan\n",
    "    df['text'] = nan\n",
    "\n",
    "#2\n",
    "chunk_size = 3\n",
    "\n",
    "sample = df[df['text'].isna()].head(chunk_size) # our sample is set as the first rows where 'text' is empty\n",
    "\n",
    "#3\n",
    "try:\n",
    "    #4\n",
    "    for row_number, row_data in sample.iterrows():\n",
    "        thread_url = row_data['url']\n",
    "        \n",
    "        #5\n",
    "        df.loc[row_number,'text'] = multi_page_post_extractor(thread_url)\n",
    "        \n",
    "        #6\n",
    "        wait_time = randint(1,3)\n",
    "        print(f'Waiting {wait_time} seconds...')\n",
    "        sleep(wait_time)\n",
    "#7        \n",
    "finally:\n",
    "    df.to_csv('my_scraping.csv', index=False)\n",
    "print('**DONE!!**')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check to see how many rows are complete in our df using the helpful readouts below.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_text_filled_rows = len(df[~df['text'].isna()])\n",
    "print(f'We have collected text for {num_text_filled_rows} of {len(df)} rows in this DataFrame')\n",
    "print(f'This accounts for {(num_text_filled_rows / len(df))*100}% of the rows.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the above again to see how it adds to the data rather than repeats itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:teaching]",
   "language": "python",
   "name": "conda-env-teaching-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
