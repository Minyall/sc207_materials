{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF you have downloaded the 'example_twitter_data.pkl' but \n",
    "# don't have a file called 'example_twitter_data_unpacked.pkl', run this cell\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def extract_original_tweets(df):\n",
    "    rt_filter = ~df.retweeted_status.isna()\n",
    "    retweet_df = pd.DataFrame( df[rt_filter]['retweeted_status'].tolist() )\n",
    "    df = df.append(retweet_df).drop_duplicates('id').reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "temp = pd.read_pickle('example_twitter_data.pkl')\n",
    "\n",
    "temp = extract_original_tweets(temp)\n",
    "df_dicts = temp.to_dict(orient='records')\n",
    "temp = pd.json_normalize(df_dicts)\n",
    "temp.to_pickle('example_twitter_data_unpacked.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SC207 - Session 7\n",
    "# APIs - Exploring and Summarising Twitter Data\n",
    "<img src=\"https://github.com/Minyall/sc207_materials/blob/master/images/tweepy.jpg?raw=true\" align=\"right\" width=\"300\">\n",
    "\n",
    "\n",
    "What kinds of exploratory analysis can we run on social media data? This session covers various examples of the kinds of insights that can be gathered through the analysis of social media data, and how to present those results.\n",
    "\n",
    "[Tweepy Documentation](http://docs.tweepy.org/en/stable/)\n",
    "\n",
    "## Section b) Exploring your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_url(row):\n",
    "    return f\"https://twitter.com/{row['user.screen_name']}/status/{row['id']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('example_twitter_data_unpacked.pkl')\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out retweets, so only original tweets are shown \n",
    "# - any retweets in our data would have had their original tweets extracted in the last session\n",
    "\n",
    "original_tweets_filter = \n",
    "df = df[original_tweets_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorations: Favourite Tweets\n",
    "A simple one to begin with. Which tweets got the most 'favourites' or hearts?\n",
    "Let's look at the top 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = ['id','user.screen_name','user.followers_count','favorite_count','retweet_count','full_text']\n",
    "\n",
    "top_favs = \n",
    "\n",
    "for index_number, row in top_favs.iterrows():\n",
    "    \n",
    "    print('*'*10)\n",
    "    print(\"INDEX:\", index_number)\n",
    "    print(\"USER:\", row['user.screen_name'])\n",
    "    print(\"FAV:\", row['favorite_count'])\n",
    "    print(\"RT:\", row['retweet_count'])\n",
    "    print(row['full_text'])\n",
    "    print(tweet_url(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you prefer, you can always export subsets of your data like this one, to a csv file to easily view in Excel or similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_favs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_favs.to_csv('top_favs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Skewed World of Twitter Interactivity\n",
    "Whilst there are approximately 6,000 tweets posted per second, the vast majority of them recieve little attention. Tweet interaction tends to skew heavily such that the majority have 0 retweets/favorites/replies.\n",
    "\n",
    "If we look above we can see a pretty swift drop in the number of interactions across the top tweets.\n",
    "\n",
    "This can make exploring our data from social media difficult, particularly around these metrics, as often graphs will skew heavily around 0 with a few points then reaching 100,000 + interactions.\n",
    "\n",
    "Because of the scale of the figures we're looking at, Pandas may use *scientific notation*, which is used to express very big numbers in a condensed format.\n",
    "\n",
    "<img src=\"https://github.com/Minyall/sc207_materials/blob/master/images/scientific_notation.png?raw=true\" align=\"left\" width=\"300\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data's overall distribution on our subset of columns using describe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use seaborn lmplot to see retweet count vs favourite count\n",
    "\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we add some extra percentiles to `describe` we can see the values needed to enter...\n",
    " - 0.9: the top 10%\n",
    " - 0.99: the top 1%\n",
    " - 0.999: the top 0.1%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[subset].describe(percentiles=[0.9,0.99,0.999])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the numbers above we can compare whether favorites or retweets are more evenly distributed in this topic. We can visualise this difference using a `boxen` plot, a type of box plot that breaks the box up so that the width indicates the number of tweets that fall into that value range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We melt our data so that our data is in a shape that seaborn can understand for the plot we want\n",
    "cat_plot_data = \n",
    "cat_plot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a seaborn cat plot to examine the distribution of favourite and retweet counts\n",
    "\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the heavy skew there isn't even a clear trend when it comes to followers_count vs retweet count - which we might have expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a seaborn lmplot to examine user followers count vs retewet count\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check this to see which are the top tweets for retweet count, and which are top for follower count. It is not always the case that the highest follower counts mean the greatest interaction. Often it is accounts with pre-existing social capital that have high follower counts. This doesn't necessarily always result in high engagement with their content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see who has high retweet count by sorting and examining the top 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see who has high followrs_count count by sorting and examining the top 10\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #Hashtags\n",
    "Examining the hashtags of your data can give you a sense of the discourses around a particular topic, and inform you of connectivity to other issues. The first step is to get the hashtags out of their nested data structure.\n",
    "\n",
    "For each entry in `entities.hashtags` we see a list, which if it is not empty, contains a set of dictionaries, and one value in each dictionary, the `text` value, is what we actually want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = ['id','entities.hashtags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[subset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each tweet contains a list, because each tweet could have one or more hashtags associated with it. The first step is to `.explode` the column, such that each hashtag gets its own row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_data = \n",
    "hashtag_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We drop any rows without hashtags at all so that our data is just tweets with hashtags associated\n",
    "not_empty = \n",
    "hashtag_data = \n",
    "hashtag_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we examine one row... remember iloc indexes purely by row and \n",
    "# column position so first row, whatever its actual index number, is iloc[0]\n",
    "example_row = hashtag_data.iloc[0]\n",
    "example_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is a dictionary, so we can get the 'text' by just using a key\n",
    "example_row['entities.hashtags']['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately what we want is to be able to do that to each row and then save the result in a new column. Enter pandas `.apply`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we create a function that does the job we want\n",
    "\n",
    "def extract_entity(entity_dict, entity_key):\n",
    "    return \n",
    "\n",
    "extract_entity(example_row['entities.hashtags'], 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we apply it to the column containing the data. We'll do it without assigning first just to check it works...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_data['tag'] = \n",
    "hashtag_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can ask how many times each tag is used...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the value count, with a reset index and top 10 to plot_tag_data\n",
    "plot_tag_data = \n",
    "plot_tag_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a seaborn barplot for the plot_tag_data\n",
    "plot = \n",
    "\n",
    "\n",
    "plot.set_xticklabels(plot.get_xticklabels(), \n",
    "                          rotation=90, \n",
    "                          horizontalalignment='right')\n",
    "plt.title(\"Top 20 Hashtags\")\n",
    "plt.xlabel('Tag')\n",
    "plt.ylabel('Freqency')\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, all the above can be condensed down into a few lines, and/or a small function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We made this one earlier\n",
    "def extract_entity(entity_dict, entity_key):\n",
    "    return entity_dict[entity_key]\n",
    "\n",
    "\n",
    "def extract_entities(df, entity_column, entity_key, new_column_name):\n",
    "     \n",
    "    entity_data = df.explode(entity_column).copy()\n",
    "    \n",
    "    not_empty = ~entity_data[entity_column].isna()\n",
    "    entity_data = entity_data[not_empty]\n",
    "    \n",
    "    entity_data[new_column_name] = entity_data[entity_column].apply(extract_entity, entity_key=entity_key)\n",
    "    return entity_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_data = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = ['id','entities.hashtags','tag']\n",
    "hashtag_data[subset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the way we've designed our function, we can apply it to any entities column..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['entities.user_mentions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_mentions = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = ['id','entities.user_mentions','mentioned']\n",
    "user_mentions[subset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = user_mentions['mentioned'].value_counts().head(20).reset_index()\n",
    "\n",
    "plot = sns.barplot(x='index', y='mentioned', data=plot_data)\n",
    "plot.set_xticklabels(plot.get_xticklabels(), \n",
    "                          rotation=90, \n",
    "                          horizontalalignment='right')\n",
    "plt.title(\"Top 20 Users Mentioned\")\n",
    "plt.xlabel('User')\n",
    "plt.ylabel('Freqency')\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding text\n",
    "Sometimes it is useful to filter, or find Tweets based on their text content. For example in this debate, some mentioned John Bercow, as another case where bullying was mentioned. Let's create a variable that allows us to split our data based on whether Bercow is mentioned or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bercow_filter = df['full_text'].str.contains('bercow', case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bercow_mentioned'] = bercow_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset=['id','bercow_mentioned']\n",
    "df[subset].groupby('bercow_mentioned').count().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series of Tweets\n",
    "Sometimes it is really useful to get a sense of the time distribution of tweets. We can use Time series information to...\n",
    "\n",
    "- See trends such as peak times for particular topics\n",
    "- Detect potential co-ordinated disinformation campaigns by examining...\n",
    "  - the account creation date of all the accounts pushing a particular hashtag. Were a significant proportion of the accounts created in a small window of time?\n",
    "  - the rate at which accounts are tweeting. Some accounts might tweets hundreds of times per hour - upwards of 50 is considered highly unusual.\n",
    "\n",
    "To ensure Pandas understands that the information in a column is a date, we convert it into date format..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform to date objects\n",
    "df['created_at'] = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By changing to a list of datetime objects pandas can now tell us more useful information, such as the earliest and latest date in the dataset\n",
    "df['created_at']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also filter it, such as asking for dates only after a certain point\n",
    "\n",
    "date_filter = \"2020-11-20\"\n",
    "recent_tweets = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then want to group our data into periods of time. There is no point grouping our data just on the 'created_at' column, because every time stamp will be slightly different by a second or two. Grouping by time needs a special object called a `Grouper`.\n",
    "\n",
    "First we create a grouper. We provide it two arguments\n",
    "- The `key` which is the column you want to group by\n",
    "- The `freq` which specifies the time period you want to group by for example 'd' for day, or 'h' for hour, or 'min' for minute.\n",
    "- You can see all the options for freq [here in the Pandas documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_grouper = \n",
    "plot_data = \n",
    "plot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a seaborn relplot to plot the time series data, x=created_at, y='id'\n",
    "plot = \n",
    "\n",
    "plt.title(\"Tweet Frequency by Hour\")\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Freqency')\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also group by other values, such as the presence of the name 'bercow'\n",
    "time_grouper = pd.Grouper(key='created_at', freq='h')\n",
    "plot_data = recent_tweets.groupby(['bercow_mentioned',time_grouper]).count()['id'].reset_index()\n",
    "plot_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = sns.relplot(x='created_at', y='id', hue='bercow_mentioned', kind='line', data=plot_data,ci=None)\n",
    "\n",
    "plt.title(\"Tweet Frequency by Hour\")\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Freqency')\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A slightly more complex one putting together entities and time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_data = extract_entities(recent_tweets, entity_column='entities.hashtags',entity_key='text', new_column_name='tag')\n",
    "\n",
    "subset=['id','created_at','tag']\n",
    "tag_time_data = hashtag_data[subset]\n",
    "tag_time_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_five_tags = tag_time_data['tag'].value_counts().head(5).reset_index()['index'].tolist()\n",
    "top_five_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_filter = tag_time_data['tag'].isin(top_five_tags)\n",
    "top_data = tag_time_data[top_filter]\n",
    "top_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_grouper = pd.Grouper(key='created_at', freq='h')\n",
    "plot_data = top_data.groupby(['tag',time_grouper]).count()['id'].reset_index()\n",
    "plot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = sns.relplot(x='created_at', y='id', hue='tag', kind='line', data=plot_data,ci=None)\n",
    "\n",
    "plt.title(\"Tweet Frequency by Hour\")\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Freqency')\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:teaching]",
   "language": "python",
   "name": "conda-env-teaching-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
