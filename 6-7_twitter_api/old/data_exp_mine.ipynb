{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SC207 - Session 4\n",
    "# Addendum: Exploring our Data\n",
    "<img src=\"https://github.com/Minyall/sc207_materials/blob/master/images/tweepy.jpg?raw=true\" align=\"right\" width=\"300\">\n",
    "\n",
    "It's down to you!\n",
    "- Tweets give us a lot of different kinds of data.\n",
    "- Some of these headings might be self-explanatory, others less so.\n",
    "- Twitter provides us with explanations of these different values in their [Tweet Object Dictionary](https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object)\n",
    "\n",
    "Explore the dataset using what you learned in the Pandas session. Interesting questions you might ask...\n",
    "- How many tweets are original, how many are retweets?\n",
    "- Who is the most active user in the sample - have we collected lots of tweets from one person?\n",
    "- Which tweets got the most retweets / favourites?\n",
    "- When were these tweets posted, and when were the most active times?\n",
    "- What hashtags do the tweets use and which are most popular (this may be a little more challenging - but possible with what we've learned)?\n",
    "\n",
    "We'll set you up with a couple of things...\n",
    "- The relevant imports you might need\n",
    "- The data to examine\n",
    "- A couple of quick examples \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Just in case you want to easily come back and start at section 4\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can either explore the data we generated together, or you can pull your own focused on a different search query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Load your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option A: I want to use the data we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('brexit_tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are finished here - proceed to step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option B: I want to use different data\n",
    "We can compress all the api work and the data management into just a few steps..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need our Tweepy import\n",
    "import tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need our function to extract the tweets from the results list\n",
    "def extract_original_tweets(list_of_tweets):\n",
    "    json_results = []\n",
    "    \n",
    "    for tweet in list_of_tweets:\n",
    "        json_results.append(tweet._json)\n",
    "        if 'retweeted_status' in tweet._json:\n",
    "            original_tweet = tweet._json['retweeted_status']\n",
    "            json_results.append(original_tweet)\n",
    "            \n",
    "    return json_results\n",
    "\n",
    "def datetime_to_twitter_id(day, month, year):\n",
    "    def utc2snowflake(stamp):\n",
    "        # from https://github.com/client9/snowflake2time\n",
    "        return (int(round(stamp * 1000)) - 1288834974657) << 22\n",
    "\n",
    "    stamp = datetime(year, month, day).replace(tzinfo=timezone.utc).timestamp()\n",
    "    return utc2snowflake(stamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set up our api access make sure you fill in your key and secret details\n",
    "\n",
    "CONSUMER_KEY = 'OrynKkk7gvFsu1AnJwCFDEBPn'\n",
    "CONSUMER_SECRET = 'QFM6BzYkmvKSzJcuMIBV8PCcvhaK2hngr50X7X2NPro7DyilN5'\n",
    "auth = tweepy.AppAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 510\n",
      "Rate limit reached. Sleeping for: 639\n"
     ]
    }
   ],
   "source": [
    "# run our query and store the results\n",
    "# remember to provide a query to the q= argument\n",
    "\n",
    "results = []\n",
    "\n",
    "for item in tweepy.Cursor(api.search, q='brexit', count=100, tweet_mode='extended').items(100000):\n",
    "    results.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_results = extract_original_tweets(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make into a dataframe\n",
    "df = pd.io.json.json_normalize(extracted_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the duplicate rows\n",
    "df = df.drop_duplicates('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the columns with no data in at all\n",
    "df = df.dropna(axis='columns', how='all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = ['created_at',\n",
    "          'id',\n",
    "          'full_text',\n",
    "          'user.screen_name',\n",
    "          'user.id',\n",
    "          'user.statuses_count',\n",
    "          'user.followers_count',\n",
    "          'retweeted_status.user.screen_name',\n",
    "          'retweeted_status.user.id',\n",
    "         'entities.user_mentions',\n",
    "         'entities.hashtags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[subset].to_pickle('large_brexit_tweets.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('large_brexit_tweets.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('large_brexit_tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1 - Most Favorited tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = ['full_text','favorite_count'] # not strictly necessary, but subset lets us control what columsn to show\n",
    "\n",
    "df[subset].sort_values(by='favorite_count', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2 - Number of retweets collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember we created a 'retweeted_status_id' column. It will either contain the id of an original tweet\n",
    "# that was retweeted, or it will be empty\n",
    "df['retweeted_status_id'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# easiest to work with is a simple column is true if the tweet is a retweet.\n",
    "df['is_retweet'] = df['retweeted_status_id'].isna()\n",
    "df['is_retweet'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can do this\n",
    "\n",
    "df['is_retweet'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3 - Popularity of Tweets, split by retweet status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=df, y='favorite_count', x='is_retweet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4 - Time Series of Tweets\n",
    "Sometimes it is really useful to get a sense of the time distribution of tweets. We can use Time series information to...\n",
    "\n",
    "- See trends such as peak times for particular topics\n",
    "- Detect potential co-ordinated disinformation campaigns by examining...\n",
    "  - the account creation date of all the accounts pushing a particular hashtag. Were a significant proportion of the accounts created in a small window of time?\n",
    "  - the rate at which accounts are tweeting. Some accounts might tweets hundreds of times per hour - upwards of 50 is considered highly unusual.\n",
    "\n",
    "To ensure Pandas understands that the information in a column is a date, we convert it into date format..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['created_at'] = pd.to_datetime(df['created_at']) #easy!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then want to group our data into periods of time. There is no point grouping our data just on the 'created_at' column, because every time stamp will be slightly different by a second or two. Grouping by time needs a special object called a `Grouper`.\n",
    "\n",
    "First we create a grouper. We provide it two arguments\n",
    "- The `key` which is the column you want to group by\n",
    "- The `freq` which specifies the time period you want to group by for example 'd' for day, or 'h' for hour.\n",
    "- You can see all the options for freq [here in the Pandas documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_grouper = pd.Grouper(key='created_at', freq='d')\n",
    "df.groupby(time_grouper).count()['id'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depending on the result it might be better if we specify to only look at tweets posted this month\n",
    "\n",
    "last_7_days_filter = df['created_at'] > '01 October 2019'\n",
    "last_7_days = df[last_7_days_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try again\n",
    "\n",
    "last_7_days.groupby(time_grouper).count()['id'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try your own explorations below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:community_mapper]",
   "language": "python",
   "name": "conda-env-community_mapper-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
