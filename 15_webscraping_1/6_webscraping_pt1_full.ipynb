{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WebScraping\n",
    "## 1. Extracting one Row of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reviewing the source\n",
    "\n",
    "- First we take a look at the page of HTML we want to capture. In a seperate window open up http://www.uberpeople.net/forums/Tips.\n",
    "\n",
    "- Explore the page as it is rendered in the browser, and the underlying code by right clicking on the page and using 'View Page Source'.\n",
    "\n",
    "- For now, we're just going to pull this entire page into memory and then we'll work out how to extract the parts we want.\n",
    "\n",
    "Python packages we are using...\n",
    "- Requests: Retrieve data from the internet - [Documentation](http://docs.python-requests.org/en/master/)\n",
    "- BeautifulSoup - Allows us to navigate through HTML content easily to isolate data we want to extract. - [Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('http://uberpeople.net/forums/Tips/') # Yes it is that simple - thanks Requests!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  If we look at our html object we get a simple response code. 200 is a success, 404 for example would be a failure.\n",
    "#  For a full list of Http response codes see https://httpstatuses.com\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We can look at the content of the retrieved package. \n",
    "# Click the bar to the left of the text to expand or contract its screen usage.\n",
    "response.text[:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting your source\n",
    "- Ok that big block of mess isn't that helpful...\n",
    "- We need to find a systematic way of combing through the entire HTML, and picking out what we need.\n",
    "- Make sure the page is open in Chrome or Firefox and then right click on the first title and choose 'Inspect (Element)' to see the underlying code.\n",
    "\n",
    "We can see that each row is in its own division. All these divisions sit inside a parent division with the class `\"structItemContainer\"`. Knowing this will let us drill down into each row, and later iterate over each row and perform the same actions. To do this we will use **Beautiful Soup**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the BeautifulSoup object and make sure we give it the html.text content, and define the parser.\n",
    "soup = BeautifulSoup(response.text,'lxml') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we look at our soup it is a little more structured, but lets keep refining....\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first focus in on the section of page we want - the element containing all the thread entries\n",
    "threads_container = soup.find('div', class_=\"structItemContainer\")\n",
    "print(threads_container.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`threads_container` is essentially a variable containing the element 'div' with the class \"structItemContainer\", and all the content of that element, which itself includes other elements.\n",
    "\n",
    "Inside the `threads_container` is a set of elements that contain the individual thread rows.\n",
    "like we used `.find()` on the soup object we made, we can use `.find()` on any variables we create using it.\n",
    "This allows us to drill down from the very top of the page structure down to individual elements.\n",
    "\n",
    "If we look in our browser we can see that every indivdual row is a 'div' element with the class `structItem structItem--thread js-inlineModContainer js-threadListItem-{some sort of id number}`. \n",
    "\n",
    "This is actually a list of classes seperated by a space. We can choose the one that seems common across all thread rows but is not too generic that is could pick up other items, or too specific that it only exists in one single row - we'll see why in a second.\n",
    "\n",
    "- `structItem` - Seems fairly generic\n",
    "- `structItem--thread` This seems like it might be more indicative of a thread row\n",
    "- `js-inlineModContainer` Seems generic again\n",
    "- `js-threadListItem-{some sort of id number}` Seems too specific with the id number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can access the first thread row by using .find() on the threads_container element\n",
    "\n",
    "threads_container.find('div', class_=\"structItem--thread\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we identified a class that is common to all thread rows we can isolate all of them at once.\n",
    "# if we take our threads_container we can use 'find_all' to return a list of all \n",
    "# child elements that match our criteria rather than just the first one as .find() does.\n",
    "\n",
    "threads = threads_container.find_all('div',class_='structItem--thread')\n",
    "threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can check this has worked and not given us any more than  the rows by counting the number of rows on the page \n",
    "# (20) and checking against the length (len) of the list here...\n",
    "len(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# If we take the first item of the list we can \n",
    "# test out extracting different parts from the row before we apply it to every item in the list\n",
    "first_item = threads[0]\n",
    "\n",
    "print(first_item.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting row items\n",
    "The items we want from this row are...\n",
    "- Author\n",
    "- Thread-id > useful for ensuring no duplicates and for quickly locating threads later.\n",
    "- Title\n",
    "- Date\n",
    "- Views\n",
    "- URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the top level of our `first_item` looks like.\n",
    "\n",
    "`<div class=\"structItem structItem--thread js-inlineModContainer js-threadListItem-360311\" data-author=\"WNYuber\">`\n",
    "\n",
    "\n",
    "The Author and the Thread-id are attributes of the element tag itself. We can extract these fairly easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can retrieve the content of a tag attrribute as if the tag were a dictionary.\n",
    "\n",
    "author = first_item['data-author']\n",
    "print(author)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thread_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unique IDs are not necesarily present in all websites, but this site happens to use them.\n",
    "\n",
    "It's not necessarily clear straight away from the code exactly what counts as the id.\n",
    "Making these decisions often requires you to look around the site and\n",
    "get a feel for its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this case we can see the same number being used in the top level row division, and in the \n",
    "# url for the thread content. We could extract this from either part of the HTML but here we'll take it\n",
    "# from the row data, where the id is in the 'class' \n",
    "\n",
    "first_item['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class has multiple elements which beautifulsoup returns as a list,\n",
    "# we need the last item in the list\n",
    "id_item = first_item['class'][-1]\n",
    "id_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the last item is a string and we just need everything after the '-'\n",
    "# we split up the string on the '-'...\n",
    "id_item.split('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the last item...\n",
    "id_item.split('-')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and convert it into an integer rather than keep the string\n",
    "thread_id = int(id_item.split('-')[-1])\n",
    "thread_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the remainder of items we need to step into the sub-divisions of the element, the div's inside our div. The row is made up of multiple subsections containing the information we want so we will need to step from our top level division into the various subsections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# starting with our first_item we use .find to search within its sub-elements to find the\n",
    "# element containing the title.\n",
    "\n",
    "title_div = first_item.find('div', class_='structItem-title')\n",
    "title_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inside the title division is a url division which is always tagged with the <a> tag. \n",
    "# we can access this either with .find('a') or the convenience method of simply .a which\n",
    "# selects the first child element that is an <a> tag.\n",
    "\n",
    "title_div.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .text allows us to just get the plain text <a> between the tags </a>\n",
    "title_div.a.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# it is a good precautinary measure when gathering text from sites to .strip() them \n",
    "# of whitespace to ensure you aren't collecting unnecessary material.\n",
    "\n",
    "title = title_div.a.text.strip()\n",
    "title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we inspect the date we can see it sits within a <time> tag within our row division\n",
    "first_item.find('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time is represented in a lot of different ways here, and Dates and times \n",
    "# are special types of object because they do not behave like normal numbers, \n",
    "# nor are they useful only as strings. The best thing to do is to save the string\n",
    "# called 'datetime' as this can be easily converted later.\n",
    "\n",
    "date = first_item.find('time')['datetime']\n",
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "views = first_item.find('dl',class_='pairs pairs--justified structItem-minor').dd.text\n",
    "views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we check the site we can see that some numbers might be problematic as they can be constructed of both numbers and letters, e.g. 2K. As we don't necessarily know all the permutations of how the numbers will be represented the simplest approach is to gather the data as it is, and handle transforming it later.\n",
    "\n",
    "It's also worth noting that unless we can find the data elsewhere, it may be the case that the site does not provide precise data after replies or views reaches 1,000, and then only displays the value in increments of 1,000. This could be a limitation if analysis relied on these figures later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get to each thread, the user would click the title of the thread,\n",
    "# meaning the url for the thread must be in the title division somewhere\n",
    "title_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# yes it is in the href attribute of the child <a>\n",
    "title_div.a['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But this is not a whole url, it's relative to the domain 'http://uberpeople.net'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So let's put it all together\n",
    "relative_url = title_div.a['href']\n",
    "url = 'http://uberpeople.net' + relative_url\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# However the safe way to do it, because URLS can go wonky sometimes is to use part of the standard library.\n",
    "url = urllib.parse.urljoin('http://uberpeople.net', relative_url)\n",
    "# This has some verification features to make sure the URL makes sense.\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make things easier for this next section let's just review in brief what we did above...\n",
    "\n",
    "# FIRST we download the html material, transform it into soup, and then drill down to\n",
    "# the element that contains the threads, then we find all elements that look like a thread\n",
    "# and we select the first one as our example test case.\n",
    "\n",
    "response = requests.get('http://uberpeople.net/forums/Tips/') # Yes it is that simple - thanks Requests!\n",
    "soup = BeautifulSoup(response.text,'lxml') # we have to make sure we give it the html.text content, and define the parser.\n",
    "threads_container = soup.find('div', class_=\"structItemContainer\")\n",
    "threads = threads_container.find_all('div',class_='structItem--thread')\n",
    "first_item = threads[0]\n",
    "\n",
    "# NEXT we took our test case first_item and we extracted all the pieces from it.\n",
    "\n",
    "#author\n",
    "author = first_item['data-author']\n",
    "\n",
    "#thread_id\n",
    "id_item = first_item['class'][-1]\n",
    "thread_id = int(id_item.split('-')[-1])\n",
    "\n",
    "#title\n",
    "title_div = first_item.find('div', class_='structItem-title') #remember that we will need title_div for the url too. \n",
    "title = title_div.a.text\n",
    "\n",
    "#date\n",
    "date = first_item.find('time')['datetime']\n",
    "\n",
    "#views\n",
    "views = first_item.find('dl',class_='pairs pairs--justified structItem-minor').dd.text\n",
    "\n",
    "#url\n",
    "relative_url = title_div.a['href'] # here we are using title_div again.\n",
    "url = urllib.parse.urljoin('http://uberpeople.net', relative_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we store this first as a dictionary it will allow us to label our data for Pandas later\n",
    "thread_data_dict = {'id': thread_id,\n",
    "                  'author': author,\n",
    "                  'title': title,\n",
    "                  'date': date,\n",
    "                  'views': views,\n",
    "                  'url': url}\n",
    "thread_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACTIVITY: Build a Function\n",
    "We're going to do this operation a lot, so it would be a good idea to turn it into a function instead. A key rule in programming is not to repeat code, but to write it once and refer to it repeatedly if necessary.\n",
    "\n",
    "Your task is to fill in the function below so that when fed a row item (like our `first_item` variable from earlier) from our webpage, it can extract out the relevant data, and returns it as a dictionary. Parts of the function have been completed for you but you need to complete it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_info_extractor(row): # We'll feed it the isolated html for a row and let it pull it apart.\n",
    "    \n",
    "    #author\n",
    "    author = row['data-author']\n",
    "    \n",
    "    #id\n",
    "    id_item = row['class'][-1]\n",
    "    thread_id = int(id_item.split('-')[-1])\n",
    "    \n",
    "    #title\n",
    "    title_div = row.find('div', class_='structItem-title')\n",
    "    title = title_div.a.text.strip() # remember to .strip() off the useless spaces on the ends.\n",
    "    \n",
    "    #date\n",
    "    date = row.find('time')['datetime']\n",
    "    \n",
    "    #views\n",
    "    views = row.find('dl',class_='pairs pairs--justified structItem-minor').dd.text\n",
    "\n",
    "    \n",
    "    #url\n",
    "    relative_url = title_div.a['href']\n",
    "    # remember the url is only relative so it needs to be made full using urlib.parse.urljoin\n",
    "    full_url = urllib.parse.urljoin('http://uberpeople.net',relative_url)\n",
    "    \n",
    "    # And now we spit out our final product - in this case we'll go for a list for Pandas use later.\n",
    "    data_package = {'id': thread_id,\n",
    "                  'author': author,\n",
    "                  'title': title,\n",
    "                  'date': date,\n",
    "                  'views': views,\n",
    "                  'url': full_url}\n",
    "    \n",
    "    return data_package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's try it out\n",
    "\n",
    "soup = BeautifulSoup(response.text,'lxml')\n",
    "threads_container = soup.find('div', class_=\"structItemContainer-group js-threadList\")\n",
    "threads = threads_container.find_all('div',class_='structItem--thread')\n",
    "\n",
    "first_item = threads[0]\n",
    "\n",
    "\n",
    "row_info_extractor(first_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:teaching]",
   "language": "python",
   "name": "conda-env-teaching-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
