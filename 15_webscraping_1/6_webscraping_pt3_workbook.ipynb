{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WebScraping\n",
    "## 3. Extracting all rows on multiple pages\n",
    "Now we can ask requests to retrieve a page, feed it to our function, and recieve a structured list of relevant information.\n",
    "\n",
    "Now let's repeat this across multiple pages on the forum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiating here to ensure everyone has the same functions\n",
    "def row_info_extractor(row): # We'll feed it the isolated html for a row and let it pull it apart.\n",
    "    author = row['data-author']\n",
    "    \n",
    "    id_item = row['class'][-1]\n",
    "    thread_id = int(id_item.split('-')[-1])\n",
    "    \n",
    "    title_div = row.find('div', class_='structItem-title')\n",
    "    title = title_div.a.text.strip() # remember to .strip() off the useless spaces on the ends.\n",
    "    \n",
    "    date = row.find('time')['datetime']\n",
    "    \n",
    "    views = row.find('dl',class_='pairs pairs--justified structItem-minor').dd.text\n",
    "\n",
    "    relative_url = title_div.a['href']\n",
    "    full_url = urllib.parse.urljoin('http://uberpeople.net',relative_url)\n",
    "    \n",
    "    data_package = {'id': thread_id,\n",
    "                  'author': author,\n",
    "                  'title': title,\n",
    "                  'date': date,\n",
    "                  'views': views,\n",
    "                  'url': full_url}\n",
    "    \n",
    "    return data_package\n",
    "\n",
    "def page_info_extractor(response):\n",
    "    soup = BeautifulSoup(response.text,'lxml')\n",
    "    threads_container = soup.find('div', class_=\"structItemContainer\")\n",
    "    threads = threads_container.find_all('div',class_='structItem--thread')\n",
    "    \n",
    "    page_data = []\n",
    "    for row in threads:\n",
    "        result = row_info_extractor(row)\n",
    "        page_data.append(result)\n",
    "    return page_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the page structure\n",
    "- Look at page 2 of the threads.\n",
    "- Note the url https://uberpeople.net/forums/Tips/page-2\n",
    "- https://uberpeople.net/forums/Tips/page-1 takes us back to our original first page\n",
    "- Does this link work https://uberpeople.net/forums/Tips/page-300 ?\n",
    "- We can also see various points where the page provides us information on how many pages there are in total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visiting multiple pages automatically\n",
    "We could, as we have above, manually create a new response object for each page, but that's not what programming is all about! How then do we automate the process.\n",
    "\n",
    "- We already know that we can predict the url for the any page of threads because it has the same structure `https://uberpeople.net/forums/Tips/page-{pick a number}`\n",
    "- This means we just need to increment that number by 1 every time we want to move to a new page.\n",
    "- Let's begin by working out how to generate urls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the built in range function is an iterator that outputs numbers based on certain criteria\n",
    "\n",
    "# up to BUT NOT INCLUDING 5, note that by default range starts at 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can set a start number \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how do we use this for our job?\n",
    "\n",
    "max_page = 3\n",
    "\n",
    " #we do +1 so it actually outputs up to AND INCLUDING the number we set as our maximum.\n",
    "f'https://uberpeople.net/forums/Tips/page-' # f-strings allows us to easily insert values into strings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So lets break down the steps\n",
    "\n",
    "# We set out maximum number of pages so we don't lose control!\n",
    "max_page = 3\n",
    "\n",
    "# We create an empty list to contain all the results from every page\n",
    "\n",
    "\n",
    "\n",
    "# We create our range generator that spits out numbers between 1 and our maximum number of pages\n",
    "\n",
    "    # build the url\n",
    "    url = 'https://uberpeople.net/forums/Tips/page-'\n",
    "    \n",
    "    # retrieve the page\n",
    "    response = \n",
    "    page_data = \n",
    "    \n",
    "    # we EXTEND the final data list with our results and the loop starts from the beginning to collect the next set\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the first item of the list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the length of the list to see how many rows we retrieved\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethical Scraping\n",
    "Web Scraping uses the resources of the websites that we draw data from. Scripted scrapers can access these resources much faster than the site would expect a user to 'browse' the site. Some sites will even block connections from computers that they believe are displaying unusual browsing activity.\n",
    "\n",
    "Therefore from an ethical and practical perspective it is important not to simply run the script at full speed, but to artificially slow it down a little.\n",
    "\n",
    "We can also use this opportunity to provide ourselves with a little more insight into what is going on in the script as it runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from random import randint\n",
    "\n",
    "max_page = 3\n",
    "\n",
    "\n",
    "\n",
    "print('Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# our completed data loads into a dataframe nice and easily...\n",
    "df =\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an opportunity for us to also clean up our date column and our views column\n",
    "\n",
    "# We can quickly transform the date column from a set of strings into date objects...\n",
    "\n",
    "df['date'] = \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function 'view_fixer' that will transform a string such as '2K' into the integer 2000\n",
    "\n",
    "def view_fixer(view_string):\n",
    "    \n",
    "    return view_integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test our function\n",
    "\n",
    "print(view_fixer(\"2K\"))\n",
    "print(view_fixer(\"500\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we cann apply this function to every row of the view column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#... and simply overwrite the views column with the result\n",
    "df['views'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally we save our dataframe for the next stage\n",
    "#Pickle files save things EXACTLY as you have them here. \n",
    "# Saving as a CSV converts data into different types, for example all our datetime objects will become strings,\n",
    "# We use pickle to preserve the dataframe as it is. Be warned however, pickling\n",
    "# is very reliant on having the exact same version of pandas so it's best to only use pickle to store data\n",
    "# between stages on the same computer\n",
    "\n",
    "'my_uber_df.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "community_mapper",
   "language": "python",
   "name": "community_mapper"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
