{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis - Finding Themes and Patterns\n",
    "## Pt 2 - Topic Models\n",
    "\n",
    "What if you have texts that don't come with handy labels to distinguish between them. What if you want to discover the themes of discussion across text, and you have so many documents you can't hope to read them all. One approach is to use *Unsupervised Machine Learning*, a type of data analysis that is essentially the computer user saying \"look at all this data, and tell me what patterns you see\".\n",
    "\n",
    "In this notebook we'll be using a particular type of unsupervised learning called *Topic Modelling*. Topic modelling looks particularly at the words and phrases used in texts and works out, based on how often words appear in different texts, what themes there might be across a collection of documents.\n",
    "\n",
    "Some limitations to keep in mind...\n",
    "- Topic modelling doesn't consider the ordering of words, just the existence or absence of words\n",
    "- Topic modelling doesn't understand the meaning of words, just the existence or absence of words.\n",
    "- Topic modelling doesn't impliclty know how many topics are in a collection of texts, you have to tell it (but there are ways around this).\n",
    "- Topic modelling can produce junk topics.\n",
    "- There is no objective way to determine if your topic modelling is 'good', it relies a lot of qualitative assessment and knowledge of the documents themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import gensim\n",
    "\n",
    "# For later plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS\n",
    "def train_phraser(texts, stopwords):\n",
    "    sentences = [\n",
    "        [token.lemma_.lower() for token in sentence if token.lemma_.lower().isalpha()]\n",
    "        for doc in texts \n",
    "        for sentence in doc.sents]\n",
    "    \n",
    "    bigram_phraser = gensim.models.Phrases(sentences, common_terms=stopwords)\n",
    "    return bigram_phraser\n",
    "\n",
    "\n",
    "def filter_text(spacy_doc, phraser, stopwords):\n",
    "    transformed_doc = []\n",
    "    for sentence in spacy_doc.sents:\n",
    "        sentence_tokens = [token.lemma_.lower() for token in sentence if token.lemma_.lower().isalpha()]\n",
    "        transformed = phraser[sentence_tokens]\n",
    "        transformed_doc.extend(transformed)\n",
    "    tokens = [token for token in transformed_doc if token.lower() not in stopwords]\n",
    "    return tokens\n",
    "\n",
    "def dummy_function(doc):\n",
    "    return doc\n",
    "\n",
    "def top_terms(df, group_name=None, top_n=5):\n",
    "    if group_name is not None:\n",
    "        df = df.drop(columns=[group_name])\n",
    "        \n",
    "    return df.sum().sort_values(ascending=False).head(top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup \n",
    "Run all the cells from here to \"SETUP END!\" to load, and preprocess our data. You can do this quickly by clicking to the left of this cell to select it, holding shift and then clicking to select the cell \"SETUP END!\". This should highlight all cells in between, then just click the run button. Whilst you're waiting for it to run have a read of the intro to the next section.\n",
    "\n",
    "### 1. Loading our Sample Data\n",
    "See part 1 for full details on this process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "newsgroups_to_fetch = ['alt.atheism', 'talk.religion.misc','comp.graphics', 'sci.space']\n",
    "\n",
    "news_set = fetch_20newsgroups(subset='all', \n",
    "                              categories=newsgroups_to_fetch,\n",
    "                              remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'text':news_set['data'], 'category_num':news_set['target']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'alt.atheism', 1: 'comp.graphics', 2: 'sci.space', 3: 'talk.religion.misc'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_lookup = {position: item for position, item in enumerate(news_set['target_names'])}\n",
    "category_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category_label'] = df['category_num'].apply(lambda category_number: category_lookup[category_number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category_num</th>\n",
       "      <th>category_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My point is that you set up your views as the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nBy '8 grey level images' you mean 8 items of...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FIRST ANNUAL PHIGS USER GROUP CONFERENCE\\n\\n  ...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I responded to Jim's other articles today, but...</td>\n",
       "      <td>3</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\nWell, I am placing a file at my ftp today th...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  category_num  \\\n",
       "0  My point is that you set up your views as the ...             0   \n",
       "1  \\nBy '8 grey level images' you mean 8 items of...             1   \n",
       "2  FIRST ANNUAL PHIGS USER GROUP CONFERENCE\\n\\n  ...             1   \n",
       "3  I responded to Jim's other articles today, but...             3   \n",
       "4  \\nWell, I am placing a file at my ftp today th...             1   \n",
       "\n",
       "       category_label  \n",
       "0         alt.atheism  \n",
       "1       comp.graphics  \n",
       "2       comp.graphics  \n",
       "3  talk.religion.misc  \n",
       "4       comp.graphics  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preparing our Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_list = nlp.Defaults.stop_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.1 s, sys: 1.57 s, total: 30.6 s\n",
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "%time df['text_nlp'] = list(nlp.pipe(df['text'],n_process=7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "phraser = train_phraser(df['text_nlp'], stopwords=stop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_tokens'] = df['text_nlp'].apply(filter_text, stopwords=stop_list, phraser=phraser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [point, set, view, way, believe, eveil, world,...\n",
       "1       [grey, level, image, mean, item, image, work, ...\n",
       "2       [annual, phigs, user, group, conference, annua...\n",
       "3       [respond, jim, article, today, neglect, respon...\n",
       "4       [place, file, ftp, today, contain, polygonal, ...\n",
       "                              ...                        \n",
       "3382    [work, program, display, wireframe, model, use...\n",
       "3383    [russian, ill, fate, phobos, mission, year_ago...\n",
       "3384    [oh, gee, billion, dollar, cover, cost, feasab...\n",
       "3385    [look, software, run, brand, new, know, site, ...\n",
       "3386    [month, look, job, computer_graphic, software,...\n",
       "Name: cleaned_tokens, Length: 3387, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleaned_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['annual', 'phigs', 'user', 'group', 'conference', 'annual', 'phigs', 'user', 'group', 'conference', 'hold', 'march', 'orlando', 'florida', 'conference', 'organize', 'laer', 'design', 'research_center', 'co', 'operation', 'ieee', 'graph', 'attendee', 'come', 'country', 'span', 'tinent', 'good', 'cross_section', 'phigs', 'community', 'represent', 'conference', 'participant', 'include', 'phigs', 'user', 'workstation', 'vendor', 'party', 'phigs', 'implementor', 'dard', 'committee', 'member', 'researcher', 'industry', 'academia', 'opening', 'speaker', 'richard', 'puk', 'challenge', 'phigs', 'user', 'charge', 'phigs', 'participate', 'phigs', 'standardization', 'activity', 'communicate', 'need', 'phigs', 'implementor', 'close', 'speaker', 'andries', 'van_dam', 'describe', 'vision', 'future', 'graphic', 'standard', 'phigs', 'technical', 'paper', 'session', 'conference', 'cover', 'follow', 'topic', 'phigs', 'x', 'application', 'toolkits', 'application', 'issues', 'texture_mapping', 'nurbs', 'phigs', 'extension', 'object_orient', 'libraries', 'frameworks', 'panel', 'session', 'phigs', 'pex', 'phigs', 'non', 'retain', 'data', 'real_world', 'cad', 'application', 'use', 'phigs', 'portability', 'issue', 'generate', 'enthusiastic', 'discussion', 'form', 'good', 'forum', 'exchange', 'idea', 'need', 'experience', 'conference', 'include', 'day', 'tutorial', 'topic', 'e', 'mathematic', 'graphic', 'object_orient', 'tool', 'base', 'phigs', 'year', 'conference', 'plan', 'march', 'phig', 'conference', 'phigs', 'vendor', 'describe', 'demonstrate', 'phigs', 'product', 'run', 'type', 'computer', 'pc', 'mainframe', 'megatek', 'corporation', 'demonstrate', 'phigs', 'extension', 'include', 'conditional', 'traversal', 'composite', 'logical', 'input_device', 'texture', 'translucency', 'template', 'graphics', 'software', 'launch', 'pro', 'realistic', 'option', 'pro', 'design', 'add', 'advanced', 'rendering', 'exist', 'api', 'feature', 'like', 'ray_trace', 'material', 'anti_aliasing', 'texture_mapping', 'radiosity', 'support', 'plan', 'example', 'tgs', 'continue', 'add', 'newly', 'emerge', 'graphic', 'feature', 'product', 'support', 'immediate', 'mode', 'extension', 'phigs', 'support', 'sun', 'xgl', 'hp', 'starbase', 'sgi_gl', 'opengl', 'nt', 'release', 'summer', 'tgs', 'demonstrate', 'late_version', 'figraph', 'powerful', 'charting', 'system', 'base', 'figt', 'orient', 'utility', 'library', 'phigs', 'pex', 'developer', 'g', 'gallium', 'software', 'demonstrate', 'new_version', 'gphigs', 'silicon_graphics', 'workstation', 'schedule', 'summer', 'sion', 'gphigs', 'company', 'library', 'tions', 'include', 'advanced', 'phigs', 'debugger', 'allow', 'phigs', 'developer', 'display', 'browse', 'phigs', 'structure', 'phigs', 'internal', 'state', 'g', 'describe', 'non', 'duplicated', 'data', 'store', 'store', 'pointer', 'application', 'datum', 'gphigs', 'css', 'efficient', 'use', 'memory', 'addition', 'g', 'describe', 'application', 'gse', 'allow', 'application', 'callback', 'function', 'gphigs', 'traversal', 'gphigs', 'phigure', 'g', 'data', 'izer', 'application', 'development', 'toolkit', 'currently_available', 'major', 'workstation', 'support', 'gl', 'x_windows', 'pex', 'starbase', 'wise', 'software', 'present', 'slide', 'z', 'phigs', 'ms_windows', 'arena', 'phigs', 'base', 'modeller', 'render', 'z', 'phigs', 'implement', 'primitive', 'addition', 'z', 'phigs', 'build', 'advanced', 'rendering', 'feature', 'like', 'texture_mapping', 'shadow', 'tion', 'area', 'quick', 'update', 'ray_tracing', 'demo_disk', 'z', 'phigs', 'arena', 'available', 'request', 'atc', 'exhibit', 'grafpak', 'phigs', 'featured', 'phigs', 'tation', 'base', 'dec_phigs', 'grafpak', 'phigs', 'available', 'workstation', 'platform', 'c', 'fortran', 'ada', 'binding', 'porates', 'pex', 'support', 'booth', 'sponsor', 'advanced', 'technology', 'center', 'digital', 'equipment', 'corporation', 'demonstrate', 'dec_phigs', 'run', 'dec', 'axp', 'pxg', 'atcs', 'grafpak', 'phigs', 'port', 'dec_phigs', 'dec_phigs', 'contain', 'phigs', 'phigs', 'plus', 'feature', 'support', 'pex', 'protocol', 'dec', 'phigs', 'contain', 'gm', 'eds', 'phig', 'extension', 'include', 'post', 'view', 'proprietary', 'extension', 'support', 'immediate', 'mode', 'render', 'use', 'phigs', 'environment', 'axp', 'dec', 'dec_phigs', 'trademark', 'digital', 'equipment', 'poration', 'grafpak', 'phigs', 'atc', 'trademark', 'advanced', 'nology', 'center', 'pex', 'trademark', 'massachusetts', 'tute', 'technology', 'ibm', 'exhibit', 'feature', 'gto', 'accelerator', 'attach', 'ibm', 'workstation', 'run', 'graphigs', 'pex', 'hewlett', 'packard', 'shographic', 'demonstrate', 'conference', 'hewlett', 'packard', 'machine', 'couple', 'display', 'shographic', 'pex', 'terminal', 'hp', 'showcase', 'late', 'phigs', 'product', 'ment', 'phigs', 'user', 'group', 'phigs', 'users', 'group', 'form', 'aid', 'development', 'phigs', 'application', 'provide', 'user', 'feedback', 'phigs', 'implementor', 'phigs', 'standard', 'body', 'information', 'phigs', 'users', 'group', 'send_e', 'mail', 'write', 'sankar', 'jayaram', 'virginia', 'polytechnic', 'institute', 'randolph', 'hall', 'blacksburg', 'virginia', 'fax', 'vendor', 'contact', 'megatek', 'corporation', 'tel', 'fax', 'template', 'graphics', 'software', 'tel', 'fax', 'wise', 'software', 'gmbh', 'tel', 'fax', 'g', 'north', 'american', 'sales', 'tel', 'fax', 'advanced', 'technology', 'center', 'tel', 'fax', 'digital', 'equipment', 'corporation', 'tel', 'international', 'business', 'machines', 'corporation', 'tel', 'hewlett', 'packard', 'company', 'tel', 'copies', 'conference', 'proceedings', 'copy', 'conference_proceeding', 'obtain', 'e', 'mary', 'johnson', 'johnson', 'mary', 'design', 'manufacturing', 'institute', 'rensselaer', 'polytechnic', 'institute', 'eighth', 'street', 'building', 'cii', 'room', 'troy', 'ny', 'tel', 'fax', 'email']\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[2, 'cleaned_tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETUP END!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In our data so far we have known which documents fall into different categories because we have labels for them. What do we do however, if we do not have helpful labels, or if we want to ask the computer whether it thinks that a corpus of documents might be divided up thematically somehow?\n",
    "\n",
    "Here we are going to use various techniques that come under the heading of \"Unsupervised Machine Learning\". Supervised machine learning is when we know the categories that our data might fall into, and we can give the computer examples of data along with their labels, and the computer works out how to best predict, just given the data, what the label is likely to be.\n",
    "\n",
    "Unsupervised machine learning essentially turns the relationship on its head, where we have the data, and we say to the computer, \"you tell me how we should divide these up\". In text, we use unsupervised machine learning to try and distinguish between documents about different topics, or to extract particular themes that might run across topics, without necessarily knowing these distinctions or themes in advance. \n",
    "\n",
    "<img src=\"https://github.com/Minyall/sc207_materials/blob/master/images/unsupervised.png?raw=true\" width=\"700\">\n",
    "\n",
    "In my research I tend to use these techniques as a first step to informing further qualitative analysis, discovering themes and then diving in to understand why those themes are prominent and what they actually mean.\n",
    "\n",
    "[Image source](https://www.edureka.co/blog/introduction-to-machine-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning: Topic Modelling\n",
    "<img src=\"https://github.com/Minyall/sc207_materials/blob/master/images/topic_modelling.png?raw=true\" align=\"right\" width=\"400\">\n",
    "\n",
    "Topic modelling is a well established standard in text analysis, and one of the most often used techniques in social science research. Topic modelling takes a set of documents and looks at what words tend to co-occur in the same documents.\n",
    "\n",
    "At the start of the process, the \"topics\" are like empty bubbles, they are there in the data but we don't yet know what they are. Based on the co-occurence of words and the seperation of those co-occurences into different documents the algorithm starts to work out...\n",
    "\n",
    "- Which documents are more similar to one another and which dissimilar.\n",
    "- Which words tend to co-occur more, and which less.\n",
    "\n",
    "Based on these observations the algorithm creates two matrices.\n",
    "\n",
    "- Term to Topic matrix - a matrix of scores that indicate how strongly each **term** is affiliated with each topic\n",
    "- Document to Topic matrix - a matrix of scores that indicate how strongly each **document** is affiliated with each topic.\n",
    "\n",
    "Topic modelling is what we call a 'soft clustering' method, as items can have a strong affiliation with more than one topic or category. Hard clustering requires an item be associated with only one category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation (LDA) Topic Modelling\n",
    "*Other Topic Modelling Algorithms are available*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec = CountVectorizer(min_df=0.01, max_df=0.999, preprocessor=dummy_function, analyzer=dummy_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_matrix = count_vec.fit_transform(df['cleaned_tokens'])\n",
    "count_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "lda_model = LatentDirichletAllocation(n_components=4) # components meaning topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick look at the two matrices\n",
    "We'll come back to these more later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_topic_matrix = lda_model.fit_transform(count_matrix)\n",
    "\n",
    "print(document_topic_matrix.shape)\n",
    "document_topic_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_term_matrix = lda_model.components_\n",
    "\n",
    "print(topic_term_matrix.shape)\n",
    "topic_term_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring your Topic Models \n",
    "#### Quick and Easy: LDAVis\n",
    "\n",
    "LDAVis is a visualisation tool specifically designed to allow quick interrogation of topic models. It offers an easy interactive interface to your models to allow you to quickly explore your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to import and then also enable notebook mode so we can see the visual inside Jupyter\n",
    "\n",
    "import pyLDAvis\n",
    "from pyLDAvis.sklearn import prepare as ldavis_prepare\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LDAvis has a special method for models built with scikit learn, \n",
    "# it takes the lda model we used to make the model, \n",
    "# the original count vectorizer matrix \n",
    "# and the vectorizer that made the matrix\n",
    "\n",
    "# we initialise it here\n",
    "visualiser = ldavis_prepare(lda_model, count_matrix, count_vec, sort_topics=False) # ignore the deprecation warning - it is hopefully going to be fixed in an upcoming version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpreting LDAvis\n",
    "Run the cell below to open the visual.\n",
    "\n",
    "On the left of the screen is the seperate topics. \n",
    "- They are positioned closer, or further from each other, depending on how much the topics overlap. For example we can see that some topics overlap a lot, whilst others are similar, but do not necessarily overlap, indicating there is some distinction between them. \n",
    "- The size of the bubbles indicates how significant those topics are within the overall corpus.\n",
    "- The numbers refer to the topic number but the numbers begin at 1, rather than 0 (helpfully).\n",
    "\n",
    "On the right is the term information for the topics.\n",
    "- If no topic is selected it gives the overall top terms for the corpus\n",
    "- If a topic is selected it shows you the top terms for that topic, including an estimate of how frequent that term is in that topic (red) compared to its overall frequency (blue).\n",
    "- Adjusting the slider at the top right allows you to tweak the measures to show terms more relevant to the topic itself.\n",
    "- Slide all the way to the left to see terms that are highly specific to the topic but to the point that they might be too niche to be meaningful.\n",
    "- Slide all the way to the right for terms that are broader but may be too generic as to not really distinguish the topics.\n",
    "- A good rule of thumb is to set the slider around 0.6 for a balanced output.\n",
    "\n",
    "What is interesting here is that whilst we manually chose 10 topics, knowing that really there are only 3 or 4 seperate categories, we still have different varieties of topic within each topical area. This indicates the variation of discussions going on within different discussion groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(visualiser,'count_my_topic_model_vis.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing the number of Topics\n",
    "\n",
    "We chose 4 topics because we knew there were four seperate newsgroups. The result implies that really there are only three topics, as one topic seems to be junk and there is not a clear distinction between the atheism and religion discussions.\n",
    "\n",
    "However, what if those different discussion groups have a variety of topics **within** them? This is the true value of unsupervised learning for social science, to highlight themes across textual datasets that would either\n",
    "- take significant academic labour to unearth (months of qualitative close reading) or \n",
    "- would  be hidden due to the sheer scale of the datasets being interrogated.\n",
    "\n",
    "Nothing is stopping us from just changing the number of topics when we initialise the LDA model, however how do we know how many topics to choose, or whether that is a valid decision?\n",
    "\n",
    "### Gensim\n",
    "\n",
    "Unfortunately there is no single library that covers all our needs for text analysis. Whilst Scikit learn has some methods for measuring how good topic models are, Gensim's `CoherenceModel` is far superior and it is worth using the library just for this feature. \n",
    "\n",
    "It should be noted that everything that we have done in the Text Analysis classes could have been done solely in Gensim, but it is a less accessible library for learners than scikit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim relies on custom built dictionaries and a special object that\n",
    "# they refer to as a corpus but is quite different from how we understand a corpus\n",
    "\n",
    "gs_dict = gensim.corpora.Dictionary(df['cleaned_tokens'])\n",
    "gs_corpus = [gs_dict.doc2bow(text) for text in df['cleaned_tokens']]\n",
    "# in order to get the coherence score we need to create a gensim version of an LDA model using their own functions\n",
    "\n",
    "gs_lda = gensim.models.LdaMulticore(num_topics=4,\n",
    "                                    corpus=gs_corpus,\n",
    "                                    id2word=gs_dict)\n",
    "\n",
    "\n",
    "# which we can then finally feed to their CoherenceModel to get a score\n",
    "\n",
    "coherence_model = gensim.models.CoherenceModel(model=gs_lda,\n",
    "                                               texts=df['cleaned_tokens'],\n",
    "                                              dictionary=gs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_model.get_coherence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the number of Topics: Testing Models\n",
    "\n",
    "In general the best way to find the right model, is to create lots of them, and get the coherence scores for each, then we can use the scores to help us make a decision about what models might be worth taking further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's build a coherence_scorer function that can take care of the whole process for us.\n",
    "\n",
    "def coherence_scorer(tokens, min_topics, max_topics, step=1):\n",
    "    gs_dict = gensim.corpora.Dictionary(tokens)\n",
    "    gs_corpus = [gs_dict.doc2bow(text) for text in tokens]\n",
    "    \n",
    "    topic_range = list(range(min_topics, max_topics, step))\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for topic_n in topic_range:\n",
    "        gs_lda = gensim.models.LdaMulticore(num_topics=topic_n,\n",
    "                                        corpus=gs_corpus,\n",
    "                                        id2word=gs_dict)\n",
    "        coherence_model = gensim.models.CoherenceModel(model=gs_lda,\n",
    "                                                   texts=tokens,\n",
    "                                                  dictionary=gs_dict)\n",
    "        score = coherence_model.get_coherence()\n",
    "        results.append(score)\n",
    "    \n",
    "    results_df = pd.DataFrame({'score':results}, index=topic_range)\n",
    "    results_df.index.name = 'topic_n'\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = coherence_scorer(df['cleaned_tokens'], min_topics=2, max_topics=30, step=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values('score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the number of Topics: Using AVERAGE Coherence\n",
    "Because LDA is a probabilistic algorithm, there is a degree of randomness to the results. \n",
    "This means the coherence of each model may be slightly different each time. Helpful!\n",
    "\n",
    "To combat this lets tweak our coherence scorer so that it tries out each number of topics multiple times, and then gives us the mean score. This will unfortunately multiply the amount of time our testing takes to run, but it will give us more confidence in the choice of model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we add a new keyword argument, runs, and set it to 1 so by default our function will only try each number of topics once. \n",
    "\n",
    "def avg_coherence_scorer(tokens, min_topics, max_topics, step=1, runs=1):\n",
    "    gs_dict = gensim.corpora.Dictionary(tokens)\n",
    "    gs_corpus = [gs_dict.doc2bow(text) for text in tokens]\n",
    "    \n",
    "    topic_range = list(range(min_topics, max_topics, step))\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for topic_n in topic_range:\n",
    "        \n",
    "        # here we create an extra list to store the scores of multiple runs of the same topic number\n",
    "        subset_results = [] \n",
    "        \n",
    "        #Inside our first loop, we create another loop that runs as many times as we have set the variable 'runs'.\n",
    "        # We're using range to control howmany times we loop, but we set the value of the range output (the number) as  _ to indicate\n",
    "        # that it doesn't matter to the code.\n",
    "        \n",
    "        for _ in range(runs): \n",
    "            gs_lda = gensim.models.LdaMulticore(num_topics=topic_n,\n",
    "                                            corpus=gs_corpus,\n",
    "                                            id2word=gs_dict)\n",
    "            coherence_model = gensim.models.CoherenceModel(model=gs_lda,\n",
    "                                                       texts=tokens,\n",
    "                                                      dictionary=gs_dict)\n",
    "            score = coherence_model.get_coherence()\n",
    "            # we run our modelliong and scoring as normal, but append the score to the subset list instead\n",
    "            subset_results.append(score)\n",
    "        #once the loop has ended we use pd.np.mean to get the mean value of all the values in the list\n",
    "        mean_score = pd.np.mean(subset_results)\n",
    "        # and append THAT score to our main results list\n",
    "        results.append(mean_score)\n",
    "    \n",
    "    results_df = pd.DataFrame({'score':results}, index=topic_range)\n",
    "    results_df.index.name = 'topic_n'\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = avg_coherence_scorer(df['cleaned_tokens'], min_topics=2, max_topics=30, step=2, runs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values('score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the number of Topics: Interpreting the scores\n",
    "<img src=\"https://github.com/Minyall/sc207_materials/blob/master/images/plot.png?raw=true\" align=\"right\" width=\"500\">\n",
    "\n",
    "It is not simply the case that the highest score is best. Some things to consider...\n",
    "\n",
    "#### Influences on the score\n",
    "- How much data you have.\n",
    "- How well preprocessed your text was - there are additional steps we could have taken such as finding common phrases.\n",
    "- There are other parameters to tweak beyond the number of topics. Details on these are beyond the scope of this module.\n",
    "\n",
    "#### How good is my score?\n",
    "- In general you're looking for a score between 0.3 and 0.7. Anything below is too low, anything above is so high it is probably an error!\n",
    "- Ideally the score should be around 0.5+ but with a corpus this small (yes this is small) it is unlikely to improve.\n",
    "\n",
    "#### Which model should I choose?\n",
    "\n",
    "- In general it is best to directly examine the results of different models.\n",
    "- The coherence score can act as a guide telling you which models score highly.\n",
    "- However it is not just the highest scoring model that is best. Often we look for the 'elbow' in the data, the model that causes a sudden jump in coherence.\n",
    "- Models with more topics and higher scores aren't necessarily better. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_model = LatentDirichletAllocation(n_components=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_model.fit(count_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualiser = ldavis_prepare(chosen_model, count_matrix, count_vec, sort_topics=False)\n",
    "# ignore the deprecation warning - it is hopefully going to be fixed in an upcoming version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualiser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Model Information\n",
    "Whilst the visualiser is useful, you may have other questions about your data which are only answerable by getting access to the results directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top words per topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can access the topic to term matrix using the .components_ attribute of the model, and\n",
    "# mix it with the feature names of the vectorizer to make ourselves a topic to term score sheet.\n",
    "topic_term_df = pd.DataFrame(chosen_model.components_, columns=count_vec.get_feature_names())\n",
    "topic_term_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we group by the index of this dataframe, i.e group into individual topics we can use our trusty top terms function\n",
    "# these should match the visualiser when relevance slider was set to 1.0\n",
    "\n",
    "top_words = topic_term_df.groupby(topic_term_df.index).apply(top_terms, top_n=20)\n",
    "\n",
    "pd.set_option('display.max_rows',200)\n",
    "top_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordclouds\n",
    "Whilst lists of words and scores are great for computers, humans have a tough time interpreting them. Here we will use a function to create wordclouds from our `top_words` list. Creating a these wordclouds is a little tricky so the function has been premade for you.\n",
    "- Required: `top_words` ---The list of top words as we created above (must be in this same format to work).\n",
    "- Option: `save` ---set save=True to save the wordclouds to disk for inclusion in your reports.\n",
    "- Option: `width` and `height` --- specify the size of each cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wordclouds(top_words, save=False, width=1000, height=500):\n",
    "    from wordcloud import WordCloud\n",
    "    import seaborn as sns\n",
    "    n_topics = top_words.index.get_level_values(0).max() +1\n",
    "    palette = sns.color_palette(\"husl\", n_topics)\n",
    "    \n",
    "    groups = top_words.reset_index().groupby('level_0')\n",
    "    \n",
    "    for topic,data in groups:\n",
    "        word_scores = data.set_index('level_1')[0].to_dict()\n",
    "        wc = WordCloud(background_color='white',color_func=lambda *args, **kwargs: palette.as_hex()[topic],width=width, height=height).generate_from_frequencies(word_scores)\n",
    "        a = plt.gca()\n",
    "        a.axis('off')\n",
    "        plt.imshow(wc, interpolation='bilinear')\n",
    "        plt.title(f\"Topic:{topic}\",loc='left')\n",
    "        if save:\n",
    "            plt.savefig(f\"topic_{topic}.png\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_wordclouds(top_words,save=True, width=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top Topic per document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The document to topic matrix is created by transforming the count matrix created by our vectoriser using our trained LDA model.\n",
    "document_topic_df = chosen_model.transform(count_matrix)\n",
    "document_topic_df = pd.DataFrame(document_topic_df)\n",
    "document_topic_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.idxmax()` method works out the highest value for a row or for a column, and then returns the index value of that highest row.\n",
    "Because in Pandas both rows have index names, and technically column names are indexes too, this means we can ask `.idxmax()` to return us the names of the `columns` with the highest value for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_topic_df.idxmax(axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['top_topic'] = document_topic_df.idxmax(axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can create a crosstab table to count how many documents of each cateogry end up assigned to each topic. This helps us to get a sense of how well the model is working. Is there a lot of overlap between topics or do they broadly seperate nicely into distinctive groups?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_comparison = pd.crosstab(index=df['category_label'],\n",
    "                         columns=df['top_topic'],\n",
    "                         values=df['text'],\n",
    "                         aggfunc='count')\n",
    "count_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the normalize keyword argument we can get the numbers to reflect \n",
    "# proportions rather than raw counts which is useful if you have variation in the number of samples for each category\n",
    "pct_comparison = pd.crosstab(index=df['category_label'],\n",
    "                         columns=df['top_topic'],\n",
    "                         values=df['text'], \n",
    "                         aggfunc='count',\n",
    "                         normalize='index')\n",
    "pct_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Seaborn we can make a heatmap of these numbers to get a quicker sense of where the larger numbers fall.\n",
    "\n",
    "To see the range of available colour palettes have a look at the [Python Graph Gallery](https://python-graph-gallery.com/python-colors/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# we create a figure of a particular size first, (width,height) in inches\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "#seaborn is based on matplotlib so it will pick up the new figure automatically and use it to create the rest\n",
    "\n",
    "\n",
    "sns.heatmap(count_comparison, cmap='PuBu', linewidths=1, annot=True, fmt='d') \n",
    "# cmap is colour map see more colour options at the link above.\n",
    "# linewidths is the size of the lines between boxes\n",
    "# annot switches on annotations of individual boxes\n",
    "# fmt=d indicates that the annotation should be formatted as a simple number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "\n",
    "sns.heatmap(pct_comparison, cmap='Greens', linewidths=1, annot=True, fmt='.1%') # cmap is colour map, for a full list\n",
    "# fmt= .1% indicates the numbers should be formatted as percentages and rounded to one decimal place. \n",
    "# Notice that chosing this formatting has automatically changed our 0.159... numbers into proper percentages (15.9%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Affiliation Strengths\n",
    "However, because topic models are soft clustering algorithms, it recognises that individual documents may express a variety of topics to different degrees. We can visualise this by grouping by the category label for each document, and then taking the average topic score for each topic. This is easy with a groupby."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, because topic models are soft clustering algorithms, it recognises that individual documents may express a variety of topics to different degrees.\n",
    "\n",
    "We can visualise this by grouping by the category label for each document, and then taking the average topic score for each topic. This is easy with a `.groupby`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_topic_df['category_label'] = df['category_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_topic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_topic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_proportions = document_topic_df.groupby('category_label').mean()\n",
    "topic_proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "\n",
    "sns.heatmap(topic_proportions, cmap='Purples', linewidths=1, annot=True, fmt='.1%') # cmap is colour map, for a full list\n",
    "# fmt= .1% indicates the numbers should be formatted as percentages and rounded to one decimal place. \n",
    "# Notice that chosing this formatting has automatically changed our 0.159... numbers into proper percentages (15.9%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(pct_comparison, cmap='Greens', linewidths=1, annot=True, fmt='.1%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/Minyall/sc207_materials/blob/master/images/hm.png?raw=true\" align=\"right\" width=\"400\">\n",
    "Compare this with our previous heatmap that showed the percentage of documents that most strongly affiliated with each topic per category. The two heatmaps show quite  different things.\n",
    "\n",
    "- The green heatmap shows the proportion of documents, per category, that most strongly affiliate with each topic. We  gave each document a top topic, regardless of how strong the affiliation with that topic was, just so long as it was the strongest.\n",
    "\n",
    "- The purple heatmap shows the mean affiliation strength, this is a more nuanced score because it is showing us the average strength of affiliation to each topic, across all documents. This affiliation strength is based on the kinds of words used by the document. \n",
    "\n",
    "The way we would interpret and talk about this kind of heatmap (using the orange heatmap to the right as an example)\n",
    " - The content of documents from alt.atheism most strongly expressed topic 6, however these documents also expressed themes identified by topics 3 and 0.\n",
    " - The content of documents from sci.space was also strongly affiliated with topic 0. This indicates some intersection between the discussions on alt.atheism and sci.space.\n",
    "\n",
    "We would then need to look at the top terms for topic 0 to discern what the topic was about, and perhaps look at documents that were most strongly affiliated with one topic, but also had a high score in another topic (How? Suggestion: filter the df by top topic first, then sort by topic score on whichever other topic you were interested in)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "The above demonstrates the effectiveness of topic modelling for finding themes within large amounts of text. At no point did we tell the topic modelling algorithm which documents came from which newsgroup, we just provided it the preprocessed text and asked it to work it out.\n",
    "\n",
    "Interestingly, our testing processes indicated that there were more topics than the number of newsgroups we had, indicating it had discovered distinct themes of discussion within newgroups. If used on more homogenous text, provided there are enough samples of the text, topic modelling can be used to tease out different themes.\n",
    "\n",
    "Whilst it is a common approach in computational social science, as LDA relies on word frequencies, it is not as nuanced as other approaches to pulling out themes that consider word significance (like TFIDF does), or even techniques that can consider the semantic similarity of texts, i.e. two documents may use different words but be  similar in that they are talking about similar things. As a result LDA works best on very large datasets, and the more homogenous the texts, the more samples it needs to distinguish differences between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:teaching]",
   "language": "python",
   "name": "conda-env-teaching-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
