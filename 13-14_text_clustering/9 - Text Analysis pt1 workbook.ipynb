{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis - Clustering\n",
    "## Pt 1 - Significant Terms\n",
    "\n",
    "If you have thousands, or hundreds of thousands of documents, how do you get an overall picture of what they are about? Techniques to find *significant* terms in large amounts of text are a useufl way to summarise large amounts of text effectively either summarising an entire collection of documents, or finding the terms that best describe a subset of those documents.\n",
    "\n",
    "In this workbook we'll be looking at discovering significant terms through a process called *Vectorization*, and we'll be looking at two approaches.\n",
    "\n",
    "- Count Vectorization\n",
    "- TFIDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Loading our Sample Data\n",
    "\n",
    "To demonstrate these techniques it is useful to have a set of documents with clear differences, so we can test to see how well the words we discover both express the overall collection of texts, and the groups of text seperately.\n",
    "\n",
    "We will be using a dataset known as the \"20 Newsgroups\" Dataset. The [website about the dataset](http://qwone.com/~jason/20Newsgroups/) has more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "news_set = fetch_20newsgroups(subset='all', \n",
    "                              categories=['alt.atheism', \n",
    "                                          'talk.religion.misc',\n",
    "                                            'comp.graphics', \n",
    "                                          'sci.space'],\n",
    "                              remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is delivered as a dictionary, with different keys referring to different components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# the list of texts itself here we look art just the first by using [0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # a list of the different categories of document, based on the newsgroup they came from.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and a list that assigns each of the documents to a particular category number, which maps back to the order of the list of names above\n",
    "\n",
    "# i.e. Target number 0 refers to alt.atheism because that is the label at position 0 in the list.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of texts should match the number of target labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(news_set['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(news_set['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by putting our text and category labels into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To provide a string label we essentially want to take the `category_num` and look up the correct label.\n",
    "\n",
    "The best data structure to look up information is a dictionary. In our case we want one that looks like this...\n",
    "```\n",
    "{0: 'alt.atheism',\n",
    " 1: 'comp.graphics',\n",
    " 2: 'sci.space',\n",
    " 3: 'talk.religion.misc'}\n",
    " ```\n",
    "Now you could just copy that into a variable and be done with it, but what happens if you need a dictionary with 300 category labels? Better to use code that will create the dictionary for you no matter how long it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_set['target_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The items are in the correct order in the list and we already know that the target numbers relate to the position of the label in `news_set['target_names']`. So what we need to do is... \n",
    "\n",
    "- create a dictionary where the key is the position of a label and the value is the label itself.\n",
    "- To do this we need to be able to somehow automatically get the position of an item in a list.\n",
    "- For this we can use the built-in `enumerate` function.\n",
    "- When you loop over an iterable like a list, which has been wrapped in the `enumerate` function, every loop will produce two values. As well as producing the value of the item in the list, it will also produce a number that, (unless you pass in extra arguments to enumerate) will return the index position of the item as well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a for loop to print the position and item in news_set['target_names']\n",
    "for position, item in enumerate(news_set['target_names']):\n",
    "    print(position, item)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all we need to do is translate this loop into a dictionary.\n",
    "\n",
    "# method A - the for loop\n",
    "\n",
    "category_lookup = {}\n",
    "\n",
    "\n",
    "\n",
    "category_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method B the Dictionary comprehension - like a list comprehnsion, but as a dictionary!\n",
    "\n",
    "# note the curled braces rather than square brackets, \n",
    "# ...and the seperation of the position and the item using a : to denote the key:value pair.\n",
    "\n",
    "category_lookup = \n",
    "category_lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this to look up our string labels to go with our category numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_lookup[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this for every row we can use apply and a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_label(category_number):\n",
    "    return category_lookup[category_number]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply to col 'category_num'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However this is not BEST practice because we now have a function hanging around that we probably will only ever use once. Instead we ideally just want the function to exist for one job, after which we can forget it.\n",
    "\n",
    "We can do this with a `lambda` function, which essentially creates a new function on the fly, rather than defining a function seperately using `def`. Lambda's are good for creating simple functions that you only need for one particular job. The general structure of a lambda is...\n",
    "\n",
    "    lambda value being passed in : value to return after doing something with the passed in value \n",
    "\n",
    "\n",
    "Here we start the function by declaring `lambda` and then we give a name to the value about to be passed to it, which is the target `category_number` in each row of our dataframe.\n",
    "\n",
    "The first part of the `lambda` ends with `:` like we have ended the line after declaring the start of our `lookup_label` function. \n",
    "\n",
    "The code after the `:` declares what will be returned. In this case we look up a string label using our `category_lookup` dictionary and the `category_number` and return that label value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category_label'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the Text\n",
    "As before we will use SpaCy for quick pre-processing of the text to tokenize and clean it.\n",
    "\n",
    "However we're going to make a couple of changes to our text processor..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = 'en_core_web_md'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_list = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df['text_nlp'] = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrasing\n",
    "<img src=\"https://github.com/Minyall/sc207_materials/blob/master/images/Archer-phrasing.jpg?raw=true\" align=\"right\" width=\"300\">\n",
    "\n",
    "- Operates on the assumption that if words often co-occur together in a corpus, they should be considered as a single 'phrase', rather than as individual words.\n",
    "- Phrasing improves the accuracy of various analyses as it recognises that words may be transformed by their context.\n",
    "- For example: \n",
    "    - In one document we have the phrase \"human rights\", in the other, \"human biology\". \n",
    "    - **Without phrasing** these may be considered similar as they both use the word \"human\".\n",
    "    - However **with phrasing** these would be transformed into two seperate tokens, human_rights and human_biology, and therefore be more likely to be distinguished as different.\n",
    "\n",
    "### The phrasing process\n",
    "\n",
    "Currently our system of text processing first preprocesses all the documents using Spacy. Then we use Pandas apply to operate on all those documents individually, lemmatising and cleaning out unwanted material.\n",
    "\n",
    "For the phraser to know what words often co-occur, it needs to see the entire corpus at once so we need to train the phraser on the entire corpus, before we then use it on a row by row basis on individual documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Functions\n",
    "\n",
    "`train_phraser` has three stages. \n",
    "- First we create a list of tokenized sentences. \n",
    "- We then feed that list of sentences to a Gensim `Phrases` model. This model looks at which token co-occur, how often and [makes a judgement](https://arxiv.org/abs/1310.4546) about whether co-occurence is common enough to consider it a 'phrase'.\n",
    "- We `return` our trained phraser to use later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_phraser(texts, stopwords):\n",
    "    \n",
    "    \n",
    "    \n",
    "    return bigram_phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phraser = train_phraser(df['text_nlp'], stopwords=stop_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our new filter function. It has a couple of new features.\n",
    "- Now we pass in the set of stopwords rather than rely on the function finding them in the global scope.\n",
    "- We have a process to handle the phrase detection stage\n",
    "\n",
    "Our new `filter_text` function...\n",
    "- Takes a SpaCy doc\n",
    "- Iterates over the doc sentences\n",
    "- For each sentence it breaks the sentence up into individual tokens, lemmatises and lowers and filters out any non-alphabetical characters\n",
    "- It then transforms those remaining tokens using the trained phraser\n",
    "- ... and adds those tokens to a list using extend so the result is a flat list of tokens for the whole document.\n",
    "- It then filters for stopwords before returning the list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_text(spacy_doc, phraser, stopwords):\n",
    "    transformed_doc = []\n",
    "    \n",
    "    # transform using the phraser\n",
    "    \n",
    "    # filter out stopwords\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why sentences?\n",
    "We break down the text into sentences for phrase detection because of the following issue.\n",
    "\n",
    "Consider this text...\n",
    "\n",
    "```\n",
    "... and so recognising that he was only human. Rights based discussions can only....\n",
    "````\n",
    "\n",
    "Here we have a division between two sentences around the full stop (period). If we feed the full text to our function, before it applies the phraser it will lower all the text, and remove non-alphabetical tokens meaning this section of the document would now look like...\n",
    "```\n",
    ".... and so recognising that he was only human rights based discussions can only..\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "The phraser, having perhaps seen the phrase 'human rights' a lot, would presume this another instance of the phrase being used. By feeding the phraser individual sentences, we maintain the boundaries in the text, and don't get false positives on phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick block of code for us to examine the phraser output\n",
    "phrases = []\n",
    "for i,row in df.iterrows():\n",
    "    text = row['text_nlp']\n",
    "    filtered = filter_text(text, phraser=phraser, stopwords=stop_list)\n",
    "    test = [token for token in filtered if token.count('_') >0]\n",
    "    phrases.extend(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "phrase_counts = Counter(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_counts.most_common(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the filter_text to out 'text_nlp' col\n",
    "df['cleaned_tokens'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc 2 has a lot of phrases we can see\n",
    "\n",
    "print(df.loc[2, 'cleaned_tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing\n",
    "For much of this process we will use tools from a library called [SciKit-Learn](https://scikit-learn.org/stable/). This is a very thorough data science and Machine Learning library in Python with a LOT of different features. Today we'll just use a few of their functions for text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turning Documents into numbers\n",
    "<img src=\"https://github.com/Minyall/sc207_materials/blob/master/images/alicequotes.png?raw=true\" align=\"right\" width=\"400\">\n",
    "\n",
    "Vectorizers are designed to turn a set of documents into a grid of values to be treated as data for other analysis techniques that rely on numerical rather than textual data. This is a key part of many text analysis processes, and how you vectorize makes a big difference to how your data will be treated.\n",
    "\n",
    "We will look at two vectorizers today.\n",
    "\n",
    "- Count Vectorizer: Numbers that represent simple frequency counts of words\n",
    "- TFIDF Vectorizer: Numbers that represent the 'significance' of a word based on a formula (more later).\n",
    "\n",
    "The result of vectorizing a list of documents is a spreadsheet with a row representing each document, and a column representing each unique word used across the entire corpus of texts. In each cell is a value representing the relationship between that document and that word. For example, for a count vectorizer it will be a frequency count.\n",
    "\n",
    "This means that these arrays tend to have many values of 0, because a word that occurs across maybe two or three documents, may not occur in any of the other hundreds of documents in the corpus.\n",
    "\n",
    "<img src=\"https://github.com/Minyall/sc207_materials/blob/master/images/docterm.png?raw=true\" align=\"left\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = ['This is my first sentence',\n",
    "          'This is the second',\n",
    "          'I enjoy peas in my sentence, peas peas peas!',\n",
    "          'This is my first sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we make a basic count vectorizer without filters\n",
    "test_vec = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and transform to create a matrix\n",
    "test_matrix = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and we'll view as a dataframe for ease\n",
    "test_df = \n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpreting the matrix\n",
    "We can see that each row corresponds to each document, and that each column corresponds to a unique word. The values correspond to the frequency of that word, in each document. For example \"Peas\" only occurs in the document at position 2, and it occurs 4 times. The word \"Sentence\" occurs once in all documents except the document at position 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dummy function\n",
    "\n",
    "Scikit Vectorizers are designed to do the majority of heavy lifting for you. Above we were able to feed it unprocessed text and it did the job of  tokenizing for us. However they do not necessarily filter, lemmatise and pre-process in the ways that might be necessary for the kinds of text you are using. The way to get around this is to specify a custom tokenizer, and custom preprocessor for the vectorizer to use. \n",
    "\n",
    "Our `dummy_function` just pretends to do something and then returns what was fed into it. This allows us to feed the vectorizer a list of pre-tokenized pre-prepared documents. The downside is that this knocks out a couple of features of the vectorizer, such as discovering ngrams, but this can either be handled with extra preprocessing or you can allow the vectorizer to handle everything, but there may be some trade-offs, such as no lemmatisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_function(doc):\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The vectorizer\n",
    "above we define our vectorizer and name it `count_vec`. The arguments we have passed to it are...\n",
    "\n",
    "- min_df: Minimum document frequency. The proportion of documents a token must occur in to be included. Filters out very low frequency words, which is also good for spelling mistakes. Here we set it to 0.01 or 1% which is approximately 33 documents out of 3,387. \n",
    "- max_df: Maximum document frequency. The proportion of documents a token can occur in before it is excluded. Filters out very high frequency words. If a word occurs in every single document, it does little for us if we want to distinguish the differences between documents. Here we set it to 0.999 or 99.9% which is approximately 3,383 documents out of 3,387. \n",
    "- tokenizer: Use to pass in a custom tokenizer function - as described in \"The dummy function\" above.\n",
    "- preprocessor: Use to pass in a custom preprocessor function - as described in \"The dummy function\" above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec  = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting and Transforming\n",
    "The next step we take our `count_vec` vectorizer and use the `.fit_transform()` method. We feed the method our list of tokenized documents. `.fit_transform()` then goes through two stages.\n",
    "\n",
    "- fit: Examines the documents, learns the vocabulary of the entire corpus, filters out words based on our `max_df` and `min_df` arguments and works out how to score those words. For a Count vectorizer this is simple, +1 every time a word occurs in a single document. TFIDF is a little more involved as we'll see. After fitting we can see the vocabulary the vectorizer has retained using `.get_feature_names()`.\n",
    "\n",
    "- transform: Takes the list of documents given, and creates a document/term matrix based on what it learned in the fitting stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_matrix = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the feature names, we'll limit to the first 20 items in the list\n",
    "count_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Sparse Matrix\n",
    "\n",
    "A sparse matrix is a space efficient way to store data that has a lot of 0's in it. Rather than remembering every 0 it simply remembers the non-zero numbers and where they are, and assumes the rest to be 0. Whilst they are space efficient, not all python functions can understand a sparse matrix, so often we have to transform them into a normal matrix as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now view as an array\n",
    "count_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we put this into a dataframe, we can set the column names to be the associated word using out .get_feature_names() method.\n",
    "count_df = \n",
    "count_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top Terms\n",
    "In this array, the values relate to how many times the relevant word was used in a document. That means that if we were to add up all the values in each column (think top to bottom), and then sorted those values, we could see which word occurred most across all documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we take our count_df, sum up across the rows using the axis argument, \n",
    "# and then sort the results into descending order (largest number first) and then take the head, the top 5.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not particularly informative because...\n",
    "- It is simple word frequencies, more frequent words are going to be generally quite dull words, even with our filters we used at the point of vectorizing. \n",
    "- This is across ALL documents, so more generic frequent words are going to rise to the top.\n",
    "\n",
    "However this is still a good approach to getting top word lists so we'll create a function to do it quickly, and to use in our next stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_terms(df, top_n=5):\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_terms(count_df, top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouped Top Terms\n",
    "Using the top words can be a great way to get a sense of what a set of documents is about. Our current dataset is a mix of discussions around computing, graphics, religion and space. This is generally expressed in our overall top words above, but what about the top words per group. If you have a way to slice up your documents into groups, the top terms can be a great indicator of what the each group is about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets label our rows in our count_df by concatenating it with the 'category_label' column from our original dataframe\n",
    "count_df_labelled = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df_labelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember in pandas if we groupby first, and then .apply a function we are essentially...\n",
    "\n",
    "# splitting the dataframe into four different dataframes (one for each category)\n",
    "# applying the function to each dataframe seperately - so summing together all the values only for rows labelled as alt.atheism, etc.\n",
    "# combining the results of each application back together into a single object.\n",
    "\n",
    "count_results = \n",
    "count_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Groupby result\n",
    "The produced object will be a pandas series with two indexes. \n",
    "\n",
    "- The first index is the names of the groups that we split the dataframe into\n",
    "- The second index is the names of the columns with the highest scores. i.e the most frequent words\n",
    "- Finally comes the actual series value, which is the frequency for each word.\n",
    "\n",
    "Note for example that the word \"use\" occurs multiple times with different scores, this is the different frequency of the word \"use\" within each group. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can access an individual gorup in the multi-index series by just using the name of the group as a key\n",
    "count_results['sci.space']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and we can even access individual items by indexing twice\n",
    "count_results['sci.space']['earth']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF Vectorising\n",
    "\n",
    "Term Frequency Inverse Document Frequency (TFIDF) is an approach to measuring word frequency that can be thought of as giving higher scores to words of greater \"significance\". \n",
    "\n",
    "TFIDF is not a simple word frequency, instead it assigns a word a score based on...\n",
    "\n",
    "- The frequency of that word in a document\n",
    "- How many other words are in that document\n",
    "- How many documents are in the overall corpus\n",
    "- How many of those documents that word appears in.\n",
    "\n",
    "#### The forumla for those interested\n",
    "- TFIDF = term freqency * inverse document frequency\n",
    "- term frequency = Frequency of occurences of a term within a single document, sometimes divided by the number of terms in the document.\n",
    "- inverse document frequency = number of documents within the entire corpus / number of documents the term occurs in.\n",
    "\n",
    "Remember our test example from earlier?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = ['This is my first sentence',\n",
    "          'This is the second',\n",
    "          'I enjoy peas in my sentence, peas peas peas!',\n",
    "          'This is my first sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use the TFIDF vectorizer from scikit learn, we can transform these numbers based on the formula above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tfidf = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tfidf_matrix = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine as a DF with named columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/Minyall/sc207_materials/blob/master/images/peas.jpg?raw=true\" align=\"right\" width=\"300\">\n",
    "We can see the weighting in these figures that have a range of 0-1.\n",
    "\n",
    "- 'Peas' has a high weighting in doc 2 because it is frequent in doc 2, but infrequent elsewhere.\n",
    "- 'Sentence' has the same weighting in docs 0 and 3, but lower in 2 despite occuring once in all three, because it is competing against more terms.\n",
    "- 'Second' has an above average score because it is only competing against a few other words, and it doesn't occur anywhere else in the corpus.\n",
    "\n",
    "TFIDF highlights \"significant\" words for two reasons...\n",
    "\n",
    "- It gives higher scores to words that occur frequently within a single document, relative to the amount of other words in a document. \n",
    "    - In a document with only 10 words, and 8 of them are \"Peas\", you would imagine peas to be a word that indicates what that document is about.\n",
    "    - In a document where \"Peas\" occurs 8 times, but there are 10,000 other words, then suddenly Peas doesn't look so significant.\n",
    "\n",
    "\n",
    "- It drags down the scores of words if they exist in many of the documents in the corpus. This gives a sense of context to the significance of words. \n",
    "- If you have a corpus about growing Peas, and every document mentions them, well then no matter how many times the word occurs in an individual document, it is probably not very indicative of what that particular Pea focussed document is about, in the broader context of Pea focussed documents.\n",
    "\n",
    "Peas photo by <a href=\"//commons.wikimedia.org/wiki/User:Atomicbre\" title=\"User:Atomicbre\">Bill Ebbesen</a> - <span class=\"int-own-work\" lang=\"en\">Own work</span>, <a href=\"https://creativecommons.org/licenses/by-sa/3.0\" title=\"Creative Commons Attribution-Share Alike 3.0\">CC BY-SA 3.0</a>, <a href=\"https://commons.wikimedia.org/w/index.php?curid=15727721\">Link</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying TFIDF to our News Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "tfidf_vectorizer = \n",
    "tfidf_matrix = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a df with named columns\n",
    "tfidf_scores = \n",
    "tfidf_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from seeing that some form of weighting is occuring (values are less than 1 so we know they're not just frequency counts) the above is pretty uninterpretible  as is.\n",
    "Let's take a look at top terms..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# across the whole corpus\n",
    "\n",
    "top_terms(tfidf_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again not great but let's try aross groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we label our rows with categories\n",
    "tfidf_scores_labelled = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_results = \n",
    "tfidf_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare TFIDF to simple frequency counts like so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_results['sci.space']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_results['sci.space']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even examine the articles and their associated significant words and get a sense ourselves of how well they fit the original document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = 4\n",
    "print(tfidf_scores.loc[article].sort_values(ascending=False).head(10))\n",
    "print(df.loc[article,'text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Pre-processing text and doing simple word frequency vectorisation or more complex TFIDF word vectorisation allows us to distinguish groups of documents from one another by using their words. Words, particularly the usage of words, either in the frequency of use, or through a more nuanced scoring of word use, can indicate to the computer the similarity or dissimilarity of documents which can be used to find themes/patterns across a corpus of texts.\n",
    "\n",
    "We can use these techniques to allow us to find groups of documents, or patterns across documents, even if we don't have any labels telling us which documents are different.\n",
    "\n",
    "On to part 2!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:teaching]",
   "language": "python",
   "name": "conda-env-teaching-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
