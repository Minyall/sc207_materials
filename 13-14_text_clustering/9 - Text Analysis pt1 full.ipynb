{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis - Clustering\n",
    "## Pt 1 - Significant Terms\n",
    "\n",
    "If you have thousands, or hundreds of thousands of documents, how do you get an overall picture of what they are about? Techniques to find *significant* terms in large amounts of text are a useufl way to summarise large amounts of text effectively either summarising an entire collection of documents, or finding the terms that best describe a subset of those documents.\n",
    "\n",
    "In this workbook we'll be looking at discovering significant terms through a process called *Vectorization*, and we'll be looking at two approaches.\n",
    "\n",
    "- Count Vectorization\n",
    "- TFIDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Loading our Sample Data\n",
    "\n",
    "To demonstrate these techniques it is useful to have a set of documents with clear differences, so we can test to see how well the words we discover both express the overall collection of texts, and the groups of text seperately.\n",
    "\n",
    "We will be using a dataset known as the \"20 Newsgroups\" Dataset. The [website about the dataset](http://qwone.com/~jason/20Newsgroups/) has more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "news_set = fetch_20newsgroups(subset='all', \n",
    "                              categories=['alt.atheism', \n",
    "                                          'talk.religion.misc',\n",
    "                                            'comp.graphics', \n",
    "                                          'sci.space'],\n",
    "                              remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is delivered as a dictionary, with different keys referring to different components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"My point is that you set up your views as the only way to believe.  Saying \\nthat all eveil in this world is caused by atheism is ridiculous and \\ncounterproductive to dialogue in this newsgroups.  I see in your posts a \\nspirit of condemnation of the atheists in this newsgroup bacause they don'\\nt believe exactly as you do.  If you're here to try to convert the atheists \\nhere, you're failing miserably.  Who wants to be in position of constantly \\ndefending themselves agaist insulting attacks, like you seem to like to do?!\\nI'm sorry you're so blind that you didn't get the messgae in the quote, \\neveryone else has seemed to.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_set['data'][0] # the list of texts itself here we look art just the first by using [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # a list of the different categories of document, based on the newsgroup they came from.\n",
    "news_set['target_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 2, 1, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and a list that assigns each of the documents to a particular category number, which maps back to the order of the list of names above\n",
    "\n",
    "# i.e. Target number 0 refers to alt.atheism because that is the label at position 0 in the list.\n",
    "news_set['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of texts should match the number of target labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3387"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news_set['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3387"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news_set['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by putting our text and category labels into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My point is that you set up your views as the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nBy '8 grey level images' you mean 8 items of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FIRST ANNUAL PHIGS USER GROUP CONFERENCE\\n\\n  ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I responded to Jim's other articles today, but...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\nWell, I am placing a file at my ftp today th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3382</th>\n",
       "      <td>I am working on a program to display 3d wirefr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3383</th>\n",
       "      <td>\\n  Did the Russian spacecraft(s) on the ill-f...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3384</th>\n",
       "      <td>\\n\\nOh gee, a billion dollars!  That'd be just...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3385</th>\n",
       "      <td>I am looking for software to run on my brand n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3386</th>\n",
       "      <td>Within the next several months I'll be looking...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3387 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  category_num\n",
       "0     My point is that you set up your views as the ...             0\n",
       "1     \\nBy '8 grey level images' you mean 8 items of...             1\n",
       "2     FIRST ANNUAL PHIGS USER GROUP CONFERENCE\\n\\n  ...             1\n",
       "3     I responded to Jim's other articles today, but...             3\n",
       "4     \\nWell, I am placing a file at my ftp today th...             1\n",
       "...                                                 ...           ...\n",
       "3382  I am working on a program to display 3d wirefr...             1\n",
       "3383  \\n  Did the Russian spacecraft(s) on the ill-f...             2\n",
       "3384  \\n\\nOh gee, a billion dollars!  That'd be just...             2\n",
       "3385  I am looking for software to run on my brand n...             1\n",
       "3386  Within the next several months I'll be looking...             1\n",
       "\n",
       "[3387 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'text':news_set['data'], 'category_num':news_set['target']})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To provide a string label we essentially want to take the `category_num` and look up the correct label.\n",
    "\n",
    "The best data structure to look up information is a dictionary. In our case we want one that looks like this...\n",
    "```\n",
    "{0: 'alt.atheism',\n",
    " 1: 'comp.graphics',\n",
    " 2: 'sci.space',\n",
    " 3: 'talk.religion.misc'}\n",
    " ```\n",
    "Now you could just copy that into a variable and be done with it, but what happens if you need a dictionary with 300 category labels? Better to use code that will create the dictionary for you no matter how long it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_set['target_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The items are in the correct order in the list and we already know that the target numbers relate to the position of the label in `news_set['target_names']`. So what we need to do is... \n",
    "\n",
    "- create a dictionary where the key is the position of a label and the value is the label itself.\n",
    "- To do this we need to be able to somehow automatically get the position of an item in a list.\n",
    "- For this we can use the built-in `enumerate` function.\n",
    "- When you loop over an iterable like a list, which has been wrapped in the `enumerate` function, every loop will produce two values. As well as producing the value of the item in the list, it will also produce a number that, (unless you pass in extra arguments to enumerate) will return the index position of the item as well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 alt.atheism\n",
      "1 comp.graphics\n",
      "2 sci.space\n",
      "3 talk.religion.misc\n"
     ]
    }
   ],
   "source": [
    "for position, item in enumerate(news_set['target_names']):\n",
    "    print(position, item)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'alt.atheism', 1: 'comp.graphics', 2: 'sci.space', 3: 'talk.religion.misc'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all we need to do is translate this loop into a dictionary.\n",
    "\n",
    "# method A - the for loop\n",
    "\n",
    "category_lookup = {}\n",
    "for position, item in enumerate(news_set['target_names']):\n",
    "    category_lookup[position] = item\n",
    "category_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'alt.atheism', 1: 'comp.graphics', 2: 'sci.space', 3: 'talk.religion.misc'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# method B the Dictionary comprehension - like a list comprehnsion, but as a dictionary!\n",
    "\n",
    "# note the curled braces rather than square brackets, \n",
    "# ...and the seperation of the position and the item using a : to denote the key:value pair.\n",
    "\n",
    "category_lookup = {position: item for position, item in enumerate(news_set['target_names'])}\n",
    "category_lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this to look up our string labels to go with our category numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sci.space'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_lookup[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this for every row we can use apply and a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_label(category_number):\n",
    "    return category_lookup[category_number]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              alt.atheism\n",
       "1            comp.graphics\n",
       "2            comp.graphics\n",
       "3       talk.religion.misc\n",
       "4            comp.graphics\n",
       "               ...        \n",
       "3382         comp.graphics\n",
       "3383             sci.space\n",
       "3384             sci.space\n",
       "3385         comp.graphics\n",
       "3386         comp.graphics\n",
       "Name: category_num, Length: 3387, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category_num'].apply(lookup_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However this is not BEST practice because we now have a function hanging around that we probably will only ever use once. Instead we ideally just want the function to exist for one job, after which we can forget it.\n",
    "\n",
    "We can do this with a `lambda` function, which essentially creates a new function on the fly, rather than defining a function seperately using `def`. Lambda's are good for creating simple functions that you only need for one particular job. The general structure of a lambda is...\n",
    "\n",
    "    lambda value being passed in : value to return after doing something with the passed in value \n",
    "\n",
    "\n",
    "Here we start the function by declaring `lambda` and then we give a name to the value about to be passed to it, which is the target `category_number` in each row of our dataframe.\n",
    "\n",
    "The first part of the `lambda` ends with `:` like we have ended the line after declaring the start of our `lookup_label` function. \n",
    "\n",
    "The code after the `:` declares what will be returned. In this case we look up a string label using our `category_lookup` dictionary and the `category_number` and return that label value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category_label'] = df['category_num'].apply(lambda category_number: category_lookup[category_number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category_num</th>\n",
       "      <th>category_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My point is that you set up your views as the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nBy '8 grey level images' you mean 8 items of...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FIRST ANNUAL PHIGS USER GROUP CONFERENCE\\n\\n  ...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I responded to Jim's other articles today, but...</td>\n",
       "      <td>3</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\nWell, I am placing a file at my ftp today th...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  category_num  \\\n",
       "0  My point is that you set up your views as the ...             0   \n",
       "1  \\nBy '8 grey level images' you mean 8 items of...             1   \n",
       "2  FIRST ANNUAL PHIGS USER GROUP CONFERENCE\\n\\n  ...             1   \n",
       "3  I responded to Jim's other articles today, but...             3   \n",
       "4  \\nWell, I am placing a file at my ftp today th...             1   \n",
       "\n",
       "       category_label  \n",
       "0         alt.atheism  \n",
       "1       comp.graphics  \n",
       "2       comp.graphics  \n",
       "3  talk.religion.misc  \n",
       "4       comp.graphics  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the Text\n",
    "As before we will use SpaCy for quick pre-processing of the text to tokenize and clean it.\n",
    "\n",
    "However we're going to make a couple of changes to our text processor..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_list = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.4 s, sys: 1.5 s, total: 29.9 s\n",
      "Wall time: 1min 19s\n"
     ]
    }
   ],
   "source": [
    "%time df['text_nlp'] = list(nlp.pipe(df['text'],n_process=7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrasing\n",
    "<img src=\"https://github.com/Minyall/sc207_materials/blob/master/images/Archer-phrasing.jpg?raw=true\" align=\"right\" width=\"300\">\n",
    "\n",
    "- Operates on the assumption that if words often co-occur together in a corpus, they should be considered as a single 'phrase', rather than as individual words.\n",
    "- Phrasing improves the accuracy of various analyses as it recognises that words may be transformed by their context.\n",
    "- For example: \n",
    "    - In one document we have the phrase \"human rights\", in the other, \"human biology\". \n",
    "    - **Without phrasing** these may be considered similar as they both use the word \"human\".\n",
    "    - However **with phrasing** these would be transformed into two seperate tokens, human_rights and human_biology, and therefore be more likely to be distinguished as different.\n",
    "\n",
    "### The phrasing process\n",
    "\n",
    "Currently our system of text processing first preprocesses all the documents using Spacy. Then we use Pandas apply to operate on all those documents individually, lemmatising and cleaning out unwanted material.\n",
    "\n",
    "For the phraser to know what words often co-occur, it needs to see the entire corpus at once so we need to train the phraser on the entire corpus, before we then use it on a row by row basis on individual documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Functions\n",
    "\n",
    "`train_phraser` has three stages. \n",
    "- First we create a list of tokenized sentences. \n",
    "- We then feed that list of sentences to a Gensim `Phrases` model. This model looks at which token co-occur, how often and [makes a judgement](https://arxiv.org/abs/1310.4546) about whether co-occurence is common enough to consider it a 'phrase'.\n",
    "- We `return` our trained phraser to use later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_phraser(texts, stopwords):\n",
    "    sentences = [\n",
    "        [token.lemma_.lower() for token in sentence if token.lemma_.lower().isalpha()]\n",
    "        for doc in texts \n",
    "        for sentence in doc.sents]\n",
    "    \n",
    "    bigram_phraser = gensim.models.Phrases(sentences, common_terms=stopwords)\n",
    "    return bigram_phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "phraser = train_phraser(df['text_nlp'], stopwords=stop_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our new filter function. It has a couple of new features.\n",
    "- Now we pass in the set of stopwords rather than rely on the function finding them in the global scope.\n",
    "- We have a process to handle the phrase detection stage\n",
    "\n",
    "Our new `filter_text` function...\n",
    "- Takes a SpaCy doc\n",
    "- Iterates over the doc sentences\n",
    "- For each sentence it breaks the sentence up into individual tokens, lemmatises and lowers and filters out any non-alphabetical characters\n",
    "- It then transforms those remaining tokens using the trained phraser\n",
    "- ... and adds those tokens to a list using extend so the result is a flat list of tokens for the whole document.\n",
    "- It then filters for stopwords before returning the list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_text(spacy_doc, phraser, stopwords):\n",
    "    transformed_doc = []\n",
    "    for sentence in spacy_doc.sents:\n",
    "        sentence_tokens = [token.lemma_.lower() for token in sentence if token.lemma_.lower().isalpha()]\n",
    "        transformed = phraser[sentence_tokens]\n",
    "        transformed_doc.extend(transformed)\n",
    "    tokens = [token for token in transformed_doc if token.lower() not in stopwords]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why sentences?\n",
    "We break down the text into sentences for phrase detection because of the following issue.\n",
    "\n",
    "Consider this text...\n",
    "\n",
    "```\n",
    "... and so recognising that he was only human. Rights based discussions can only....\n",
    "````\n",
    "\n",
    "Here we have a division between two sentences around the full stop (period). If we feed the full text to our function, before it applies the phraser it will lower all the text, and remove non-alphabetical tokens meaning this section of the document would now look like...\n",
    "```\n",
    ".... and so recognising that he was only human rights based discussions can only..\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "The phraser, having perhaps seen the phrase 'human rights' a lot, would presume this another instance of the phrase being used. By feeding the phraser individual sentences, we maintain the boundaries in the text, and don't get false positives on phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = []\n",
    "for i,row in df.iterrows():\n",
    "    text = row['text_nlp']\n",
    "    filtered = filter_text(text, phraser=phraser, stopwords=stop_list)\n",
    "    test = [token for token in filtered if token.count('_') >0]\n",
    "    phrases.extend(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "phrase_counts = Counter(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('e_mail', 161),\n",
       " ('image_processing', 113),\n",
       " ('solar_system', 89),\n",
       " ('space_station', 88),\n",
       " ('file_format', 83),\n",
       " ('lord_jehovah', 78),\n",
       " ('space_shuttle', 77),\n",
       " ('thank_in_advance', 76),\n",
       " ('source_code', 73),\n",
       " ('god_elohim', 72),\n",
       " ('anonymous_ftp', 71),\n",
       " ('ray_tracer', 64),\n",
       " ('look_like', 64),\n",
       " ('new_york', 63),\n",
       " ('gamma_ray', 61),\n",
       " ('jpeg_file', 61),\n",
       " ('computer_graphics', 59),\n",
       " ('year_ago', 57),\n",
       " ('image_quality', 55),\n",
       " ('jesus_christ', 54)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_counts.most_common(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_tokens'] = df['text_nlp'].apply(filter_text, stopwords=stop_list, phraser=phraser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [point, set, view, way, believe, eveil, world,...\n",
       "1       [grey, level, image, mean, item, image, work, ...\n",
       "2       [annual, phigs, user, group, conference, annua...\n",
       "3       [respond, jim, article, today, neglect, respon...\n",
       "4       [place, file, ftp, today, contain, polygonal, ...\n",
       "                              ...                        \n",
       "3382    [work, program, display, wireframe, model, use...\n",
       "3383    [russian, ill, fate, phobos, mission, year_ago...\n",
       "3384    [oh, gee, billion, dollar, cover, cost, feasab...\n",
       "3385    [look, software, run, brand, new, know, site, ...\n",
       "3386    [month, look, job, computer_graphic, software,...\n",
       "Name: cleaned_tokens, Length: 3387, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleaned_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['annual', 'phigs', 'user', 'group', 'conference', 'annual', 'phigs', 'user', 'group', 'conference', 'hold', 'march', 'orlando', 'florida', 'conference', 'organize', 'laer', 'design', 'research_center', 'co', 'operation', 'ieee', 'graph', 'attendee', 'come', 'country', 'span', 'tinent', 'good', 'cross_section', 'phigs', 'community', 'represent', 'conference', 'participant', 'include', 'phigs', 'user', 'workstation', 'vendor', 'party', 'phigs', 'implementor', 'dard', 'committee', 'member', 'researcher', 'industry', 'academia', 'opening', 'speaker', 'richard', 'puk', 'challenge', 'phigs', 'user', 'charge', 'phigs', 'participate', 'phigs', 'standardization', 'activity', 'communicate', 'need', 'phigs', 'implementor', 'close', 'speaker', 'andries', 'van_dam', 'describe', 'vision', 'future', 'graphic', 'standard', 'phigs', 'technical', 'paper', 'session', 'conference', 'cover', 'follow', 'topic', 'phigs', 'x', 'application', 'toolkits', 'application', 'issues', 'texture_mapping', 'nurbs', 'phigs', 'extension', 'object_orient', 'libraries', 'frameworks', 'panel', 'session', 'phigs', 'pex', 'phigs', 'non', 'retain', 'data', 'real_world', 'cad', 'application', 'use', 'phigs', 'portability', 'issue', 'generate', 'enthusiastic', 'discussion', 'form', 'good', 'forum', 'exchange', 'idea', 'need', 'experience', 'conference', 'include', 'day', 'tutorial', 'topic', 'e', 'mathematic', 'graphic', 'object_orient', 'tool', 'base', 'phigs', 'year', 'conference', 'plan', 'march', 'phig', 'conference', 'phigs', 'vendor', 'describe', 'demonstrate', 'phigs', 'product', 'run', 'type', 'computer', 'pc', 'mainframe', 'megatek', 'corporation', 'demonstrate', 'phigs', 'extension', 'include', 'conditional', 'traversal', 'composite', 'logical', 'input_device', 'texture', 'translucency', 'template', 'graphics', 'software', 'launch', 'pro', 'realistic', 'option', 'pro', 'design', 'add', 'advanced', 'rendering', 'exist', 'api', 'feature', 'like', 'ray_trace', 'material', 'anti_aliasing', 'texture_mapping', 'radiosity', 'support', 'plan', 'example', 'tgs', 'continue', 'add', 'newly', 'emerge', 'graphic', 'feature', 'product', 'support', 'immediate', 'mode', 'extension', 'phigs', 'support', 'sun', 'xgl', 'hp', 'starbase', 'sgi_gl', 'opengl', 'nt', 'release', 'summer', 'tgs', 'demonstrate', 'late_version', 'figraph', 'powerful', 'charting', 'system', 'base', 'figt', 'orient', 'utility', 'library', 'phigs', 'pex', 'developer', 'g', 'gallium', 'software', 'demonstrate', 'new_version', 'gphigs', 'silicon_graphics', 'workstation', 'schedule', 'summer', 'sion', 'gphigs', 'company', 'library', 'tions', 'include', 'advanced', 'phigs', 'debugger', 'allow', 'phigs', 'developer', 'display', 'browse', 'phigs', 'structure', 'phigs', 'internal', 'state', 'g', 'describe', 'non', 'duplicated', 'data', 'store', 'store', 'pointer', 'application', 'datum', 'gphigs', 'css', 'efficient', 'use', 'memory', 'addition', 'g', 'describe', 'application', 'gse', 'allow', 'application', 'callback', 'function', 'gphigs', 'traversal', 'gphigs', 'phigure', 'g', 'data', 'izer', 'application', 'development', 'toolkit', 'currently_available', 'major', 'workstation', 'support', 'gl', 'x_windows', 'pex', 'starbase', 'wise', 'software', 'present', 'slide', 'z', 'phigs', 'ms_windows', 'arena', 'phigs', 'base', 'modeller', 'render', 'z', 'phigs', 'implement', 'primitive', 'addition', 'z', 'phigs', 'build', 'advanced', 'rendering', 'feature', 'like', 'texture_mapping', 'shadow', 'tion', 'area', 'quick', 'update', 'ray_tracing', 'demo_disk', 'z', 'phigs', 'arena', 'available', 'request', 'atc', 'exhibit', 'grafpak', 'phigs', 'featured', 'phigs', 'tation', 'base', 'dec_phigs', 'grafpak', 'phigs', 'available', 'workstation', 'platform', 'c', 'fortran', 'ada', 'binding', 'porates', 'pex', 'support', 'booth', 'sponsor', 'advanced', 'technology', 'center', 'digital', 'equipment', 'corporation', 'demonstrate', 'dec_phigs', 'run', 'dec', 'axp', 'pxg', 'atcs', 'grafpak', 'phigs', 'port', 'dec_phigs', 'dec_phigs', 'contain', 'phigs', 'phigs', 'plus', 'feature', 'support', 'pex', 'protocol', 'dec', 'phigs', 'contain', 'gm', 'eds', 'phig', 'extension', 'include', 'post', 'view', 'proprietary', 'extension', 'support', 'immediate', 'mode', 'render', 'use', 'phigs', 'environment', 'axp', 'dec', 'dec_phigs', 'trademark', 'digital', 'equipment', 'poration', 'grafpak', 'phigs', 'atc', 'trademark', 'advanced', 'nology', 'center', 'pex', 'trademark', 'massachusetts', 'tute', 'technology', 'ibm', 'exhibit', 'feature', 'gto', 'accelerator', 'attach', 'ibm', 'workstation', 'run', 'graphigs', 'pex', 'hewlett', 'packard', 'shographic', 'demonstrate', 'conference', 'hewlett', 'packard', 'machine', 'couple', 'display', 'shographic', 'pex', 'terminal', 'hp', 'showcase', 'late', 'phigs', 'product', 'ment', 'phigs', 'user', 'group', 'phigs', 'users', 'group', 'form', 'aid', 'development', 'phigs', 'application', 'provide', 'user', 'feedback', 'phigs', 'implementor', 'phigs', 'standard', 'body', 'information', 'phigs', 'users', 'group', 'send_e', 'mail', 'write', 'sankar', 'jayaram', 'virginia', 'polytechnic', 'institute', 'randolph', 'hall', 'blacksburg', 'virginia', 'fax', 'vendor', 'contact', 'megatek', 'corporation', 'tel', 'fax', 'template', 'graphics', 'software', 'tel', 'fax', 'wise', 'software', 'gmbh', 'tel', 'fax', 'g', 'north', 'american', 'sales', 'tel', 'fax', 'advanced', 'technology', 'center', 'tel', 'fax', 'digital', 'equipment', 'corporation', 'tel', 'international', 'business', 'machines', 'corporation', 'tel', 'hewlett', 'packard', 'company', 'tel', 'copies', 'conference', 'proceedings', 'copy', 'conference_proceeding', 'obtain', 'e', 'mary', 'johnson', 'johnson', 'mary', 'design', 'manufacturing', 'institute', 'rensselaer', 'polytechnic', 'institute', 'eighth', 'street', 'building', 'cii', 'room', 'troy', 'ny', 'tel', 'fax', 'email']\n"
     ]
    }
   ],
   "source": [
    "#doc 2 has a lot of phrases we can see\n",
    "\n",
    "print(df.loc[2, 'cleaned_tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing\n",
    "For much of this process we will use tools from a library called [SciKit-Learn](https://scikit-learn.org/stable/). This is a very thorough data science and Machine Learning library in Python with a LOT of different features. Today we'll just use a few of their functions for text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turning Documents into numbers\n",
    "<img src=\"https://github.com/Minyall/sc207_materials/blob/master/images/alicequotes.png?raw=true\" align=\"right\" width=\"400\">\n",
    "\n",
    "Vectorizers are designed to turn a set of documents into a grid of values to be treated as data for other analysis techniques that rely on numerical rather than textual data. This is a key part of many text analysis processes, and how you vectorize makes a big difference to how your data will be treated.\n",
    "\n",
    "We will look at two vectorizers today.\n",
    "\n",
    "- Count Vectorizer: Numbers that represent simple frequency counts of words\n",
    "- TFIDF Vectorizer: Numbers that represent the 'significance' of a word based on a formula (more later).\n",
    "\n",
    "The result of vectorizing a list of documents is a spreadsheet with a row representing each document, and a column representing each unique word used across the entire corpus of texts. In each cell is a value representing the relationship between that document and that word. For example, for a count vectorizer it will be a frequency count.\n",
    "\n",
    "This means that these arrays tend to have many values of 0, because a word that occurs across maybe two or three documents, may not occur in any of the other hundreds of documents in the corpus.\n",
    "\n",
    "<img src=\"https://github.com/Minyall/sc207_materials/blob/master/images/docterm.png?raw=true\" align=\"left\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = ['This is my first sentence',\n",
    "          'This is the second',\n",
    "          'I enjoy peas in my sentence, peas peas peas!',\n",
    "          'This is my first sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we make a basic count vectorizer without filters\n",
    "test_vec = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and transform to create a matrix\n",
    "test_matrix = test_vec.fit_transform(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>enjoy</th>\n",
       "      <th>first</th>\n",
       "      <th>in</th>\n",
       "      <th>is</th>\n",
       "      <th>my</th>\n",
       "      <th>peas</th>\n",
       "      <th>second</th>\n",
       "      <th>sentence</th>\n",
       "      <th>the</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   enjoy  first  in  is  my  peas  second  sentence  the  this\n",
       "0      0      1   0   1   1     0       0         1    0     1\n",
       "1      0      0   0   1   0     0       1         0    1     1\n",
       "2      1      0   1   0   1     4       0         1    0     0\n",
       "3      0      1   0   1   1     0       0         1    0     1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and we'll view as a dataframe for ease\n",
    "test_df = pd.DataFrame(test_matrix.toarray(), columns=test_vec.get_feature_names())\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpreting the matrix\n",
    "We can see that each row corresponds to each document, and that each column corresponds to a unique word. The values correspond to the frequency of that word, in each document. For example \"Peas\" only occurs in the document at position 2, and it occurs 4 times. The word \"Sentence\" occurs once in all documents except the document at position 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dummy function\n",
    "\n",
    "Scikit Vectorizers are designed to do the majority of heavy lifting for you. Above we were able to feed it unprocessed text and it did the job of  tokenizing for us. However they do not necessarily filter, lemmatise and pre-process in the ways that might be necessary for the kinds of text you are using. The way to get around this is to specify a custom tokenizer, and custom preprocessor for the vectorizer to use. \n",
    "\n",
    "Our `dummy_function` just pretends to do something and then returns what was fed into it. This allows us to feed the vectorizer a list of pre-tokenized pre-prepared documents. The downside is that this knocks out a couple of features of the vectorizer, such as discovering ngrams, but this can either be handled with extra preprocessing or you can allow the vectorizer to handle everything, but there may be some trade-offs, such as no lemmatisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_function(doc):\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The vectorizer\n",
    "above we define our vectorizer and name it `count_vec`. The arguments we have passed to it are...\n",
    "\n",
    "- min_df: Minimum document frequency. The proportion of documents a token must occur in to be included. Filters out very low frequency words, which is also good for spelling mistakes. Here we set it to 0.01 or 1% which is approximately 33 documents out of 3,387. \n",
    "- max_df: Maximum document frequency. The proportion of documents a token can occur in before it is excluded. Filters out very high frequency words. If a word occurs in every single document, it does little for us if we want to distinguish the differences between documents. Here we set it to 0.999 or 99.9% which is approximately 3,383 documents out of 3,387. \n",
    "- tokenizer: Use to pass in a custom tokenizer function - as described in \"The dummy function\" above.\n",
    "- preprocessor: Use to pass in a custom preprocessor function - as described in \"The dummy function\" above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec  = CountVectorizer(min_df=0.01, max_df=0.999, tokenizer=dummy_function, preprocessor=dummy_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting and Transforming\n",
    "The next step we take our `count_vec` vectorizer and use the `.fit_transform()` method. We feed the method our list of tokenized documents. `.fit_transform()` then goes through two stages.\n",
    "\n",
    "- fit: Examines the documents, learns the vocabulary of the entire corpus, filters out words based on our `max_df` and `min_df` arguments and works out how to score those words. For a Count vectorizer this is simple, +1 every time a word occurs in a single document. TFIDF is a little more involved as we'll see. After fitting we can see the vocabulary the vectorizer has retained using `.get_feature_names()`.\n",
    "\n",
    "- transform: Takes the list of documents given, and creates a document/term matrix based on what it learned in the fitting stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_matrix = count_vec.fit_transform(df['cleaned_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ability',\n",
       " 'able',\n",
       " 'absolute',\n",
       " 'absolutely',\n",
       " 'accept',\n",
       " 'access',\n",
       " 'accord',\n",
       " 'account',\n",
       " 'accurate',\n",
       " 'achieve',\n",
       " 'act',\n",
       " 'action',\n",
       " 'active',\n",
       " 'activity',\n",
       " 'actual',\n",
       " 'actually',\n",
       " 'ad',\n",
       " 'add',\n",
       " 'addition',\n",
       " 'address']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we'll limit to the first 20 items in the list\n",
    "count_vec.get_feature_names()[:20] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Sparse Matrix\n",
    "\n",
    "A sparse matrix is a space efficient way to store data that has a lot of 0's in it. Rather than remembering every 0 it simply remembers the non-zero numbers and where they are, and assumes the rest to be 0. Whilst they are space efficient, not all python functions can understand a sparse matrix, so often we have to transform them into a normal matrix as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3387x1087 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 97940 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 4, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>absolute</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>accept</th>\n",
       "      <th>access</th>\n",
       "      <th>accord</th>\n",
       "      <th>account</th>\n",
       "      <th>accurate</th>\n",
       "      <th>achieve</th>\n",
       "      <th>...</th>\n",
       "      <th>worth</th>\n",
       "      <th>write</th>\n",
       "      <th>writing</th>\n",
       "      <th>wrong</th>\n",
       "      <th>x</th>\n",
       "      <th>year</th>\n",
       "      <th>year_ago</th>\n",
       "      <th>yes</th>\n",
       "      <th>z</th>\n",
       "      <th>zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3382</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3383</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3384</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3385</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3386</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3387 rows Ã— 1087 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ability  able  absolute  absolutely  accept  access  accord  account  \\\n",
       "0           0     0         0           0       0       0       0        0   \n",
       "1           0     0         0           0       0       0       0        0   \n",
       "2           0     0         0           0       0       0       0        0   \n",
       "3           0     0         0           0       0       0       0        0   \n",
       "4           0     0         0           0       0       0       0        0   \n",
       "...       ...   ...       ...         ...     ...     ...     ...      ...   \n",
       "3382        0     1         0           0       0       0       0        0   \n",
       "3383        0     0         0           0       0       0       0        0   \n",
       "3384        0     0         0           0       0       0       0        0   \n",
       "3385        0     0         0           0       0       0       0        0   \n",
       "3386        0     0         0           0       0       0       0        0   \n",
       "\n",
       "      accurate  achieve  ...  worth  write  writing  wrong  x  year  year_ago  \\\n",
       "0            0        0  ...      0      0        0      0  0     0         0   \n",
       "1            0        0  ...      0      1        0      0  0     0         0   \n",
       "2            0        0  ...      0      1        0      0  1     1         0   \n",
       "3            0        0  ...      0      0        0      0  0     0         0   \n",
       "4            0        0  ...      0      0        0      0  0     0         0   \n",
       "...        ...      ...  ...    ...    ...      ...    ... ..   ...       ...   \n",
       "3382         0        0  ...      0      1        0      0  1     0         0   \n",
       "3383         0        0  ...      0      0        0      0  0     0         1   \n",
       "3384         0        0  ...      0      0        0      0  0     0         0   \n",
       "3385         0        0  ...      0      0        0      0  0     0         0   \n",
       "3386         0        0  ...      0      0        0      0  0     0         0   \n",
       "\n",
       "      yes  z  zero  \n",
       "0       0  0     0  \n",
       "1       0  0     0  \n",
       "2       0  4     0  \n",
       "3       0  0     0  \n",
       "4       0  2     0  \n",
       "...   ... ..   ...  \n",
       "3382    0  0     0  \n",
       "3383    0  0     0  \n",
       "3384    0  0     0  \n",
       "3385    0  0     0  \n",
       "3386    0  0     0  \n",
       "\n",
       "[3387 rows x 1087 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if we put this into a dataframe, we can set the column names to be the associated word using out .get_feature_names() method.\n",
    "count_df = pd.DataFrame(count_matrix.toarray(), columns=count_vec.get_feature_names())\n",
    "count_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top Terms\n",
    "In this array, the values relate to how many times the relevant word was used in a document. That means that if we were to add up all the values in each column (think top to bottom), and then sorted those values, we could see which word occurred most across all documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "use       1916\n",
       "image     1484\n",
       "know      1386\n",
       "think     1252\n",
       "people    1232\n",
       "dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we take our count_df, sum up across the rows using the axis argument, \n",
    "# and then sort the results into descending order (largest number first) and then take the head, the top 5.\n",
    "\n",
    "count_df.sum(axis='rows').sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not particularly informative because...\n",
    "- It is simple word frequencies, more frequent words are going to be generally quite dull words, even with our filters we used at the point of vectorizing. \n",
    "- This is across ALL documents, so more generic frequent words are going to rise to the top.\n",
    "\n",
    "However this is still a good approach to getting top word lists so we'll create a function to do it quickly, and to use in our next stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_terms(df, top_n=5):\n",
    "    return df.sum().sort_values(ascending=False).head(top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "use       1916\n",
       "image     1484\n",
       "know      1386\n",
       "think     1252\n",
       "people    1232\n",
       "god       1217\n",
       "like      1131\n",
       "time      1000\n",
       "good       884\n",
       "find       863\n",
       "dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_terms(count_df, top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouped Top Terms\n",
    "Using the top words can be a great way to get a sense of what a set of documents is about. Our current dataset is a mix of discussions around computing, graphics, religion and space. This is generally expressed in our overall top words above, but what about the top words per group. If you have a way to slice up your documents into groups, the top terms can be a great indicator of what the each group is about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets label our rows in our count_df by concatenating it with the 'category_label' column from our original dataframe\n",
    "count_df_labelled = pd.concat([df['category_label'], count_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_label</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>absolute</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>accept</th>\n",
       "      <th>access</th>\n",
       "      <th>accord</th>\n",
       "      <th>account</th>\n",
       "      <th>accurate</th>\n",
       "      <th>...</th>\n",
       "      <th>worth</th>\n",
       "      <th>write</th>\n",
       "      <th>writing</th>\n",
       "      <th>wrong</th>\n",
       "      <th>x</th>\n",
       "      <th>year</th>\n",
       "      <th>year_ago</th>\n",
       "      <th>yes</th>\n",
       "      <th>z</th>\n",
       "      <th>zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3382</th>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3383</th>\n",
       "      <td>sci.space</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3384</th>\n",
       "      <td>sci.space</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3385</th>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3386</th>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3387 rows Ã— 1088 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          category_label  ability  able  absolute  absolutely  accept  access  \\\n",
       "0            alt.atheism        0     0         0           0       0       0   \n",
       "1          comp.graphics        0     0         0           0       0       0   \n",
       "2          comp.graphics        0     0         0           0       0       0   \n",
       "3     talk.religion.misc        0     0         0           0       0       0   \n",
       "4          comp.graphics        0     0         0           0       0       0   \n",
       "...                  ...      ...   ...       ...         ...     ...     ...   \n",
       "3382       comp.graphics        0     1         0           0       0       0   \n",
       "3383           sci.space        0     0         0           0       0       0   \n",
       "3384           sci.space        0     0         0           0       0       0   \n",
       "3385       comp.graphics        0     0         0           0       0       0   \n",
       "3386       comp.graphics        0     0         0           0       0       0   \n",
       "\n",
       "      accord  account  accurate  ...  worth  write  writing  wrong  x  year  \\\n",
       "0          0        0         0  ...      0      0        0      0  0     0   \n",
       "1          0        0         0  ...      0      1        0      0  0     0   \n",
       "2          0        0         0  ...      0      1        0      0  1     1   \n",
       "3          0        0         0  ...      0      0        0      0  0     0   \n",
       "4          0        0         0  ...      0      0        0      0  0     0   \n",
       "...      ...      ...       ...  ...    ...    ...      ...    ... ..   ...   \n",
       "3382       0        0         0  ...      0      1        0      0  1     0   \n",
       "3383       0        0         0  ...      0      0        0      0  0     0   \n",
       "3384       0        0         0  ...      0      0        0      0  0     0   \n",
       "3385       0        0         0  ...      0      0        0      0  0     0   \n",
       "3386       0        0         0  ...      0      0        0      0  0     0   \n",
       "\n",
       "      year_ago  yes  z  zero  \n",
       "0            0    0  0     0  \n",
       "1            0    0  0     0  \n",
       "2            0    0  4     0  \n",
       "3            0    0  0     0  \n",
       "4            0    0  2     0  \n",
       "...        ...  ... ..   ...  \n",
       "3382         0    0  0     0  \n",
       "3383         1    0  0     0  \n",
       "3384         0    0  0     0  \n",
       "3385         0    0  0     0  \n",
       "3386         0    0  0     0  \n",
       "\n",
       "[3387 rows x 1088 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_df_labelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category_label               \n",
       "alt.atheism         god           601\n",
       "                    people        495\n",
       "                    think         449\n",
       "                    know          381\n",
       "                    believe       376\n",
       "                    atheist       337\n",
       "                    religion      306\n",
       "                    use           280\n",
       "                    thing         265\n",
       "                    argument      260\n",
       "comp.graphics       image        1285\n",
       "                    use           920\n",
       "                    file          748\n",
       "                    jpeg          583\n",
       "                    program       583\n",
       "                    format        427\n",
       "                    software      398\n",
       "                    system        376\n",
       "                    color         372\n",
       "                    available     364\n",
       "sci.space           space         722\n",
       "                    use           482\n",
       "                    launch        414\n",
       "                    time          346\n",
       "                    like          344\n",
       "                    earth         336\n",
       "                    satellite     334\n",
       "                    orbit         321\n",
       "                    year          316\n",
       "                    think         314\n",
       "talk.religion.misc  god           592\n",
       "                    people        407\n",
       "                    jesus         346\n",
       "                    know          344\n",
       "                    think         307\n",
       "                    bible         237\n",
       "                    use           234\n",
       "                    believe       234\n",
       "                    christian     220\n",
       "                    come          214\n",
       "dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remember in pandas if we groupby first, and then .apply a function we are essentially...\n",
    "\n",
    "# splitting the dataframe into four different dataframes (one for each category)\n",
    "# applying the function to each dataframe seperately - so summing together all the values only for rows labelled as alt.atheism, etc.\n",
    "# combining the results of each application back together into a single object.\n",
    "\n",
    "count_results = count_df_labelled.groupby('category_label').apply(top_terms, top_n=10)\n",
    "count_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Groupby result\n",
    "The produced object will be a pandas series with two indexes. \n",
    "\n",
    "- The first index is the names of the groups that we split the dataframe into\n",
    "- The second index is the names of the columns with the highest scores. i.e the most frequent words\n",
    "- Finally comes the actual series value, which is the frequency for each word.\n",
    "\n",
    "Note for example that the word \"use\" occurs multiple times with different scores, this is the different frequency of the word \"use\" within each group. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "space        722\n",
       "use          482\n",
       "launch       414\n",
       "time         346\n",
       "like         344\n",
       "earth        336\n",
       "satellite    334\n",
       "orbit        321\n",
       "year         316\n",
       "think        314\n",
       "dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can access an individual gorup in the multi-index series by just using the name of the group as a key\n",
    "count_results['sci.space']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "336"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and we can even access individual items by indexing twice\n",
    "count_results['sci.space']['earth']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF Vectorising\n",
    "\n",
    "Term Frequency Inverse Document Frequency (TFIDF) is an approach to measuring word frequency that can be thought of as giving higher scores to words of greater \"significance\". \n",
    "\n",
    "TFIDF is not a simple word frequency, instead it assigns a word a score based on...\n",
    "\n",
    "- The frequency of that word in a document\n",
    "- How many other words are in that document\n",
    "- How many documents are in the overall corpus\n",
    "- How many of those documents that word appears in.\n",
    "\n",
    "#### The forumla for those interested\n",
    "- TFIDF = term freqency * inverse document frequency\n",
    "- term frequency = Frequency of occurences of a term within a single document, sometimes divided by the number of terms in the document.\n",
    "- inverse document frequency = number of documents within the entire corpus / number of documents the term occurs in.\n",
    "\n",
    "Remember our test example from earlier?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = ['This is my first sentence',\n",
    "          'This is the second',\n",
    "          'I enjoy peas in my sentence, peas peas peas!',\n",
    "          'This is my first sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>enjoy</th>\n",
       "      <th>first</th>\n",
       "      <th>in</th>\n",
       "      <th>is</th>\n",
       "      <th>my</th>\n",
       "      <th>peas</th>\n",
       "      <th>second</th>\n",
       "      <th>sentence</th>\n",
       "      <th>the</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   enjoy  first  in  is  my  peas  second  sentence  the  this\n",
       "0      0      1   0   1   1     0       0         1    0     1\n",
       "1      0      0   0   1   0     0       1         0    1     1\n",
       "2      1      0   1   0   1     4       0         1    0     0\n",
       "3      0      1   0   1   1     0       0         1    0     1"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use the TFIDF vectorizer from scikit learn, we can transform these numbers based on the formula above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tfidf_matrix = test_tfidf.fit_transform(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>enjoy</th>\n",
       "      <th>first</th>\n",
       "      <th>in</th>\n",
       "      <th>is</th>\n",
       "      <th>my</th>\n",
       "      <th>peas</th>\n",
       "      <th>second</th>\n",
       "      <th>sentence</th>\n",
       "      <th>the</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.525464</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.425408</td>\n",
       "      <td>0.425408</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.425408</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.425408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.380444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.596039</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.596039</td>\n",
       "      <td>0.380444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.230542</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.230542</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.147152</td>\n",
       "      <td>0.922168</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.147152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.525464</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.425408</td>\n",
       "      <td>0.425408</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.425408</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.425408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      enjoy     first        in        is        my      peas    second  \\\n",
       "0  0.000000  0.525464  0.000000  0.425408  0.425408  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.380444  0.000000  0.000000  0.596039   \n",
       "2  0.230542  0.000000  0.230542  0.000000  0.147152  0.922168  0.000000   \n",
       "3  0.000000  0.525464  0.000000  0.425408  0.425408  0.000000  0.000000   \n",
       "\n",
       "   sentence       the      this  \n",
       "0  0.425408  0.000000  0.425408  \n",
       "1  0.000000  0.596039  0.380444  \n",
       "2  0.147152  0.000000  0.000000  \n",
       "3  0.425408  0.000000  0.425408  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(test_tfidf_matrix.toarray(), columns=test_tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/Minyall/sc207_materials/blob/master/images/peas.jpg?raw=true\" align=\"right\" width=\"300\">\n",
    "We can see the weighting in these figures that have a range of 0-1.\n",
    "\n",
    "- 'Peas' has a high weighting in doc 2 because it is frequent in doc 2, but infrequent elsewhere.\n",
    "- 'Sentence' has the same weighting in docs 0 and 3, but lower in 2 despite occuring once in all three, because it is competing against more terms.\n",
    "- 'Second' has an above average score because it is only competing against a few other words, and it doesn't occur anywhere else in the corpus.\n",
    "\n",
    "TFIDF highlights \"significant\" words for two reasons...\n",
    "\n",
    "- It gives higher scores to words that occur frequently within a single document, relative to the amount of other words in a document. \n",
    "    - In a document with only 10 words, and 8 of them are \"Peas\", you would imagine peas to be a word that indicates what that document is about.\n",
    "    - In a document where \"Peas\" occurs 8 times, but there are 10,000 other words, then suddenly Peas doesn't look so significant.\n",
    "\n",
    "\n",
    "- It drags down the scores of words if they exist in many of the documents in the corpus. This gives a sense of context to the significance of words. \n",
    "- If you have a corpus about growing Peas, and every document mentions them, well then no matter how many times the word occurs in an individual document, it is probably not very indicative of what that particular Pea focussed document is about, in the broader context of Pea focussed documents.\n",
    "\n",
    "Peas photo by <a href=\"//commons.wikimedia.org/wiki/User:Atomicbre\" title=\"User:Atomicbre\">Bill Ebbesen</a> - <span class=\"int-own-work\" lang=\"en\">Own work</span>, <a href=\"https://creativecommons.org/licenses/by-sa/3.0\" title=\"Creative Commons Attribution-Share Alike 3.0\">CC BY-SA 3.0</a>, <a href=\"https://commons.wikimedia.org/w/index.php?curid=15727721\">Link</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying TFIDF to our News Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df =0.01, max_df=0.999, preprocessor=dummy_function, tokenizer=dummy_function)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['cleaned_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>absolute</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>accept</th>\n",
       "      <th>access</th>\n",
       "      <th>accord</th>\n",
       "      <th>account</th>\n",
       "      <th>accurate</th>\n",
       "      <th>achieve</th>\n",
       "      <th>...</th>\n",
       "      <th>worth</th>\n",
       "      <th>write</th>\n",
       "      <th>writing</th>\n",
       "      <th>wrong</th>\n",
       "      <th>x</th>\n",
       "      <th>year</th>\n",
       "      <th>year_ago</th>\n",
       "      <th>yes</th>\n",
       "      <th>z</th>\n",
       "      <th>zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.103762</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.024820</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.033153</td>\n",
       "      <td>0.026435</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.156742</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.319160</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3382</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.108671</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.091771</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.122585</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3383</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.350791</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3384</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3385</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3386</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3387 rows Ã— 1087 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ability      able  absolute  absolutely  accept  access  accord  \\\n",
       "0         0.0  0.000000       0.0         0.0     0.0     0.0     0.0   \n",
       "1         0.0  0.000000       0.0         0.0     0.0     0.0     0.0   \n",
       "2         0.0  0.000000       0.0         0.0     0.0     0.0     0.0   \n",
       "3         0.0  0.000000       0.0         0.0     0.0     0.0     0.0   \n",
       "4         0.0  0.000000       0.0         0.0     0.0     0.0     0.0   \n",
       "...       ...       ...       ...         ...     ...     ...     ...   \n",
       "3382      0.0  0.108671       0.0         0.0     0.0     0.0     0.0   \n",
       "3383      0.0  0.000000       0.0         0.0     0.0     0.0     0.0   \n",
       "3384      0.0  0.000000       0.0         0.0     0.0     0.0     0.0   \n",
       "3385      0.0  0.000000       0.0         0.0     0.0     0.0     0.0   \n",
       "3386      0.0  0.000000       0.0         0.0     0.0     0.0     0.0   \n",
       "\n",
       "      account  accurate  achieve  ...  worth     write  writing  wrong  \\\n",
       "0         0.0       0.0      0.0  ...    0.0  0.000000      0.0    0.0   \n",
       "1         0.0       0.0      0.0  ...    0.0  0.103762      0.0    0.0   \n",
       "2         0.0       0.0      0.0  ...    0.0  0.024820      0.0    0.0   \n",
       "3         0.0       0.0      0.0  ...    0.0  0.000000      0.0    0.0   \n",
       "4         0.0       0.0      0.0  ...    0.0  0.000000      0.0    0.0   \n",
       "...       ...       ...      ...  ...    ...       ...      ...    ...   \n",
       "3382      0.0       0.0      0.0  ...    0.0  0.091771      0.0    0.0   \n",
       "3383      0.0       0.0      0.0  ...    0.0  0.000000      0.0    0.0   \n",
       "3384      0.0       0.0      0.0  ...    0.0  0.000000      0.0    0.0   \n",
       "3385      0.0       0.0      0.0  ...    0.0  0.000000      0.0    0.0   \n",
       "3386      0.0       0.0      0.0  ...    0.0  0.000000      0.0    0.0   \n",
       "\n",
       "             x      year  year_ago  yes         z  zero  \n",
       "0     0.000000  0.000000  0.000000  0.0  0.000000   0.0  \n",
       "1     0.000000  0.000000  0.000000  0.0  0.000000   0.0  \n",
       "2     0.033153  0.026435  0.000000  0.0  0.156742   0.0  \n",
       "3     0.000000  0.000000  0.000000  0.0  0.000000   0.0  \n",
       "4     0.000000  0.000000  0.000000  0.0  0.319160   0.0  \n",
       "...        ...       ...       ...  ...       ...   ...  \n",
       "3382  0.122585  0.000000  0.000000  0.0  0.000000   0.0  \n",
       "3383  0.000000  0.000000  0.350791  0.0  0.000000   0.0  \n",
       "3384  0.000000  0.000000  0.000000  0.0  0.000000   0.0  \n",
       "3385  0.000000  0.000000  0.000000  0.0  0.000000   0.0  \n",
       "3386  0.000000  0.000000  0.000000  0.0  0.000000   0.0  \n",
       "\n",
       "[3387 rows x 1087 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_scores = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names())\n",
    "tfidf_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from seeing that some form of weighting is occuring (values are less than 1 so we know they're not just frequency counts) the above is pretty uninterpretible  as is.\n",
    "Let's take a look at top terms..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "think    95.041990\n",
       "know     94.863873\n",
       "use      92.717395\n",
       "like     78.224450\n",
       "god      77.616942\n",
       "dtype: float64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# across the whole corpus\n",
    "\n",
    "top_terms(tfidf_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again not great but let's try aross groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we label our rows with categories\n",
    "tfidf_scores_labelled = pd.concat([df['category_label'], tfidf_scores], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category_label               \n",
       "alt.atheism         god          40.127134\n",
       "                    think        32.036445\n",
       "                    people       29.700357\n",
       "                    atheist      26.767684\n",
       "                    religion     25.425237\n",
       "                    believe      25.377210\n",
       "                    know         22.136654\n",
       "                    post         21.408256\n",
       "                    claim        18.397012\n",
       "                    thing        17.902507\n",
       "comp.graphics       file         44.677253\n",
       "                    image        43.719291\n",
       "                    use          41.937761\n",
       "                    program      33.746890\n",
       "                    look         31.641670\n",
       "                    know         31.019904\n",
       "                    format       29.613754\n",
       "                    graphic      27.905464\n",
       "                    thank        26.269853\n",
       "                    need         26.033888\n",
       "sci.space           space        47.051557\n",
       "                    like         27.083155\n",
       "                    think        27.073104\n",
       "                    orbit        26.758225\n",
       "                    use          26.649382\n",
       "                    launch       26.450035\n",
       "                    know         23.324493\n",
       "                    moon         22.931924\n",
       "                    satellite    21.176212\n",
       "                    time         20.691494\n",
       "talk.religion.misc  god          35.171812\n",
       "                    jesus        23.286767\n",
       "                    people       21.050372\n",
       "                    think        19.168974\n",
       "                    christian    18.571059\n",
       "                    know         18.382823\n",
       "                    bible        16.614704\n",
       "                    believe      15.363158\n",
       "                    word         14.579395\n",
       "                    child        13.922584\n",
       "dtype: float64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_results = tfidf_scores_labelled.groupby('category_label').apply(top_terms, top_n=10)\n",
    "tfidf_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare TFIDF to simple frequency counts like so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "space        47.051557\n",
       "like         27.083155\n",
       "think        27.073104\n",
       "orbit        26.758225\n",
       "use          26.649382\n",
       "launch       26.450035\n",
       "know         23.324493\n",
       "moon         22.931924\n",
       "satellite    21.176212\n",
       "time         20.691494\n",
       "dtype: float64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_results['sci.space']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "space        722\n",
       "use          482\n",
       "launch       414\n",
       "time         346\n",
       "like         344\n",
       "earth        336\n",
       "satellite    334\n",
       "orbit        321\n",
       "year         316\n",
       "think        314\n",
       "dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_results['sci.space']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even examine the articles and their associated significant words and get a sense ourselves of how well they fit the original document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ftp            0.437019\n",
      "datum          0.371959\n",
      "file           0.335330\n",
      "polygon        0.330666\n",
      "z              0.319160\n",
      "contain        0.253432\n",
      "workstation    0.163045\n",
      "following      0.159580\n",
      "normal         0.157069\n",
      "resolution     0.155888\n",
      "Name: 4, dtype: float64\n",
      "\n",
      "Well, I am placing a file at my ftp today that contains several\n",
      "polygonal descriptions of a head, face, skull, vase, etc. The format\n",
      "of the files is a list of vertices, normals, and triangles. There are\n",
      "various resolutions and the name of the data file includes the number\n",
      "of polygons, eg. phred.1.3k.vbl contains 1300 polygons.\n",
      "\n",
      "\n",
      "In order to get the data via ftp do the following:\n",
      "\n",
      "\t1) ftp taurus.cs.nps.navy.mil\n",
      "\t2) login as anonymous, guest as the password\n",
      "\t3) cd pub/dabro\n",
      "\t4) binary\n",
      "\t5) get cyber.tar.Z\n",
      "\n",
      "Once you get the data onto your workstation:\n",
      "\n",
      "\t1) uncompress data.tar.Z\n",
      "\t2) tar xvof data.tar\n",
      "\n",
      "If you have any questions, please let me know.\n",
      "\n",
      "george dabro\n",
      "dabro@taurus.cs.nps.navy.mil\n",
      "-- \n",
      "george dabrowski\n",
      "Cyberware Labs\n"
     ]
    }
   ],
   "source": [
    "article = 4\n",
    "print(tfidf_scores.loc[article].sort_values(ascending=False).head(10))\n",
    "print(df.loc[article,'text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Pre-processing text and doing simple word frequency vectorisation or more complex TFIDF word vectorisation allows us to distinguish groups of documents from one another by using their words. Words, particularly the usage of words, either in the frequency of use, or through a more nuanced scoring of word use, can indicate to the computer the similarity or dissimilarity of documents which can be used to find themes/patterns across a corpus of texts.\n",
    "\n",
    "We can use these techniques to allow us to find groups of documents, or patterns across documents, even if we don't have any labels telling us which documents are different.\n",
    "\n",
    "On to part 2!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:teaching]",
   "language": "python",
   "name": "conda-env-teaching-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
